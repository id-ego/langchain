---
description: LangChain의 주요 구성 요소와 아키텍처에 대한 소개를 제공합니다. 다양한 패키지와 통합에 대한 설명이 포함되어 있습니다.
---

# 개념적 가이드

import ThemedImage from '@theme/ThemedImage';  
import useBaseUrl from '@docusaurus/useBaseUrl';  

이 섹션은 LangChain의 주요 부분에 대한 소개를 포함합니다.

## 아키텍처

LangChain은 프레임워크로 여러 패키지로 구성됩니다.

### `langchain-core`  
이 패키지는 다양한 구성 요소의 기본 추상화와 이를 함께 구성하는 방법을 포함합니다. LLM, 벡터 저장소, 검색기 등과 같은 핵심 구성 요소의 인터페이스가 여기에서 정의됩니다. 제3자 통합은 여기에서 정의되지 않습니다. 의존성은 의도적으로 매우 가벼운 상태로 유지됩니다.

### 파트너 패키지

긴 통합 목록은 `langchain-community`에 있지만, 인기 있는 통합을 별도의 패키지로 분리했습니다(예: `langchain-openai`, `langchain-anthropic` 등). 이는 이러한 중요한 통합에 대한 지원을 개선하기 위해 수행되었습니다.

### `langchain`  

주요 `langchain` 패키지는 애플리케이션의 인지 아키텍처를 구성하는 체인, 에이전트 및 검색 전략을 포함합니다. 이들은 제3자 통합이 아닙니다. 여기의 모든 체인, 에이전트 및 검색 전략은 특정 통합에 국한되지 않고 모든 통합에 대해 일반적입니다.

### `langchain-community`  

이 패키지는 LangChain 커뮤니티에서 유지 관리하는 제3자 통합을 포함합니다. 주요 파트너 패키지는 분리되어 있습니다(아래 참조). 이 패키지는 다양한 구성 요소(LLM, 벡터 저장소, 검색기)에 대한 모든 통합을 포함합니다. 이 패키지의 모든 의존성은 패키지를 가능한 한 가볍게 유지하기 위해 선택적입니다.

### [`langgraph`](https://langchain-ai.github.io/langgraph)  

`langgraph`는 LLM을 사용하여 단계를 그래프의 엣지와 노드로 모델링하여 강력하고 상태 유지 가능한 다중 액터 애플리케이션을 구축하는 것을 목표로 하는 `langchain`의 확장입니다.  

LangGraph는 일반적인 유형의 에이전트를 생성하기 위한 고급 인터페이스와 사용자 정의 흐름을 구성하기 위한 저급 API를 제공합니다.

### [`langserve`](/docs/langserve)  

LangChain 체인을 REST API로 배포하는 패키지입니다. 프로덕션 준비가 완료된 API를 쉽게 설정할 수 있습니다.

### [LangSmith](https://docs.smith.langchain.com)  

LLM 애플리케이션을 디버깅, 테스트, 평가 및 모니터링할 수 있는 개발자 플랫폼입니다.

<ThemedImage  
alt="LangChain 프레임워크의 계층적 조직을 설명하는 다이어그램, 여러 레이어에 걸쳐 상호 연결된 부분을 표시합니다."  
sources={{  
light: useBaseUrl('/svg/langchain_stack_062024.svg'),  
dark: useBaseUrl('/svg/langchain_stack_062024_dark.svg'),  
}}  
title="LangChain 프레임워크 개요"  
style={{ width: "100%" }}  
/>

## LangChain 표현 언어 (LCEL)  
<span data-heading-keywords="lcel"></span>  

LangChain 표현 언어, 또는 LCEL은 LangChain 구성 요소를 체인으로 연결하는 선언적 방법입니다. LCEL은 **코드 변경 없이 프로토타입을 프로덕션에 배포할 수 있도록** 설계되었습니다. 가장 간단한 "프롬프트 + LLM" 체인부터 가장 복잡한 체인까지(우리는 사람들이 프로덕션에서 100단계의 LCEL 체인을 성공적으로 실행하는 것을 보았습니다). LCEL을 사용해야 할 몇 가지 이유를 강조합니다:

**일급 스트리밍 지원**  
LCEL로 체인을 구축하면 가능한 최상의 첫 번째 토큰까지의 시간(첫 번째 출력 청크가 나올 때까지 경과한 시간)을 얻을 수 있습니다. 일부 체인의 경우, 예를 들어 LLM에서 스트리밍 출력 파서로 토큰을 직접 스트리밍하여 LLM 제공자가 원시 토큰을 출력하는 속도와 동일한 속도로 구문 분석된 점진적 출력 청크를 받을 수 있습니다.

**비동기 지원**  
LCEL로 구축된 모든 체인은 동기 API(예: 프로토타입 중 Jupyter 노트북에서)와 비동기 API(예: [LangServe](/docs/langserve/) 서버에서) 모두에서 호출할 수 있습니다. 이를 통해 프로토타입과 프로덕션에서 동일한 코드를 사용할 수 있으며, 뛰어난 성능과 동일한 서버에서 많은 동시 요청을 처리할 수 있는 능력을 제공합니다.

**최적화된 병렬 실행**  
LCEL 체인에 병렬로 실행할 수 있는 단계가 있는 경우(예: 여러 검색기에서 문서를 가져오는 경우) 우리는 자동으로 이를 수행합니다. 동기 및 비동기 인터페이스 모두에서 가장 작은 대기 시간으로 처리됩니다.

**재시도 및 대체**  
LCEL 체인의 모든 부분에 대해 재시도 및 대체를 구성할 수 있습니다. 이는 체인을 대규모로 더 신뢰할 수 있도록 만드는 훌륭한 방법입니다. 현재 우리는 재시도/대체에 대한 스트리밍 지원을 추가하는 작업을 진행 중이며, 이를 통해 대기 시간 비용 없이 추가 신뢰성을 얻을 수 있습니다.

**중간 결과 접근**  
더 복잡한 체인의 경우 최종 출력이 생성되기 전에 중간 단계의 결과에 접근하는 것이 매우 유용할 수 있습니다. 이는 최종 사용자에게 무언가가 진행되고 있음을 알리거나 체인을 디버깅하는 데 사용할 수 있습니다. 중간 결과를 스트리밍할 수 있으며, 모든 [LangServe](/docs/langserve) 서버에서 사용할 수 있습니다.

**입력 및 출력 스키마**  
입력 및 출력 스키마는 모든 LCEL 체인에 대해 체인의 구조에서 유추된 Pydantic 및 JSONSchema 스키마를 제공합니다. 이는 입력 및 출력의 유효성 검증에 사용될 수 있으며, LangServe의 필수적인 부분입니다.

[**매끄러운 LangSmith 추적**](https://docs.smith.langchain.com)  
체인이 점점 더 복잡해짐에 따라 각 단계에서 정확히 무슨 일이 일어나고 있는지를 이해하는 것이 점점 더 중요해집니다. LCEL을 사용하면 **모든** 단계가 최대 가시성과 디버깅 가능성을 위해 [LangSmith](https://docs.smith.langchain.com/)에 자동으로 기록됩니다.

LCEL은 `LLMChain` 및 `ConversationalRetrievalChain`과 같은 레거시 서브클래스 체인에 대한 동작 및 사용자 정의의 일관성을 제공하는 것을 목표로 합니다. 이러한 레거시 체인 중 많은 부분은 프롬프트와 같은 중요한 세부정보를 숨기며, 더 다양한 실행 가능한 모델이 등장함에 따라 사용자 정의가 점점 더 중요해졌습니다.

현재 이러한 레거시 체인을 사용하고 있다면 [이 가이드를 참조하여 마이그레이션 방법에 대한 안내를 받으세요](/docs/versions/migrating_chains).

LCEL로 특정 작업을 수행하는 방법에 대한 가이드는 [관련 방법 가이드를 확인하세요](/docs/how_to/#langchain-expression-language-lcel).

### 실행 가능 인터페이스  
<span data-heading-keywords="invoke,runnable"></span>  

사용자 정의 체인을 쉽게 생성할 수 있도록 ["Runnable"](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) 프로토콜을 구현했습니다. 많은 LangChain 구성 요소가 `Runnable` 프로토콜을 구현하고 있으며, 여기에는 채팅 모델, LLM, 출력 파서, 검색기, 프롬프트 템플릿 등이 포함됩니다. 실행 가능 항목과 함께 작업하기 위한 여러 유용한 기본 요소도 있으며, 아래에서 읽을 수 있습니다.

이는 표준 인터페이스로, 사용자 정의 체인을 정의하고 표준 방식으로 호출하는 것을 쉽게 만듭니다. 표준 인터페이스에는 다음이 포함됩니다:

- `stream`: 응답의 청크를 스트리밍하여 반환
- `invoke`: 입력에 대해 체인을 호출
- `batch`: 입력 목록에 대해 체인을 호출

이들은 모두 비동기성을 위해 [asyncio](https://docs.python.org/3/library/asyncio.html) `await` 구문과 함께 사용해야 하는 해당 비동기 메서드가 있습니다:

- `astream`: 응답의 청크를 비동기적으로 스트리밍하여 반환
- `ainvoke`: 입력에 대해 체인을 비동기적으로 호출
- `abatch`: 입력 목록에 대해 체인을 비동기적으로 호출
- `astream_log`: 최종 응답 외에도 발생하는 중간 단계를 스트리밍하여 반환
- `astream_events`: **베타** 체인에서 발생하는 이벤트를 스트리밍( `langchain-core` 0.1.14에서 도입됨)

**입력 유형** 및 **출력 유형**은 구성 요소에 따라 다릅니다:

| 구성 요소 | 입력 유형 | 출력 유형 |
| --- | --- | --- |
| 프롬프트 | 사전 | PromptValue |
| ChatModel | 단일 문자열, 채팅 메시지 목록 또는 PromptValue | ChatMessage |
| LLM | 단일 문자열, 채팅 메시지 목록 또는 PromptValue | 문자열 |
| OutputParser | LLM 또는 ChatModel의 출력 | 파서에 따라 다름 |
| Retriever | 단일 문자열 | 문서 목록 |
| Tool | 도구에 따라 단일 문자열 또는 사전 | 도구에 따라 다름 |

모든 실행 가능 항목은 입력 및 출력 **스키마**를 노출하여 입력 및 출력을 검사할 수 있습니다:  
- `input_schema`: Runnable의 구조에서 자동 생성된 입력 Pydantic 모델  
- `output_schema`: Runnable의 구조에서 자동 생성된 출력 Pydantic 모델  

## 구성 요소

LangChain은 LLM으로 구축하는 데 유용한 다양한 구성 요소에 대한 표준, 확장 가능한 인터페이스 및 외부 통합을 제공합니다. 일부 구성 요소는 LangChain이 구현하고, 일부 구성 요소는 제3자 통합에 의존하며, 다른 구성 요소는 혼합되어 있습니다.

### 채팅 모델  
<span data-heading-keywords="chat model,chat models"></span>  

메시지 시퀀스를 입력으로 사용하고 채팅 메시지를 출력으로 반환하는 언어 모델(일반 텍스트를 사용하는 것과는 반대). 이러한 모델은 전통적으로 더 최신 모델입니다(구형 모델은 일반적으로 `LLMs`, 아래 참조). 채팅 모델은 대화 메시지에 대해 서로 다른 역할을 할당할 수 있도록 지원하여 AI, 사용자 및 시스템 메시지와 같은 지침의 메시지를 구별하는 데 도움을 줍니다.

기본 모델은 메시지를 입력으로 받고 메시지를 출력으로 반환하지만, LangChain 래퍼는 이러한 모델이 문자열을 입력으로 받을 수 있도록 허용합니다. 즉, LLM 대신 채팅 모델을 쉽게 사용할 수 있습니다.

문자열이 입력으로 전달되면 `HumanMessage`로 변환된 후 기본 모델에 전달됩니다.

LangChain은 어떤 채팅 모델도 호스팅하지 않으며, 대신 제3자 통합에 의존합니다.

채팅 모델을 구성할 때 몇 가지 표준화된 매개변수가 있습니다:
- `model`: 모델의 이름
- `temperature`: 샘플링 온도
- `timeout`: 요청 시간 초과
- `max_tokens`: 생성할 최대 토큰 수
- `stop`: 기본 중지 시퀀스
- `max_retries`: 요청을 재시도할 최대 횟수
- `api_key`: 모델 제공자의 API 키
- `base_url`: 요청을 보낼 엔드포인트

중요한 점은:
- 표준 매개변수는 의도된 기능으로 매개변수를 노출하는 모델 제공자에게만 적용됩니다. 예를 들어, 일부 제공자는 최대 출력 토큰에 대한 구성을 노출하지 않으므로 이러한 경우 max_tokens를 지원할 수 없습니다.
- 표준 매개변수는 현재 자체 통합 패키지가 있는 통합에만 적용됩니다(예: `langchain-openai`, `langchain-anthropic` 등), `langchain-community`의 모델에는 적용되지 않습니다.

채팅 모델은 해당 통합에 특정한 다른 매개변수도 수용합니다. ChatModel에서 지원하는 모든 매개변수를 찾으려면 해당 모델의 API 참조로 이동하세요.

:::important  
일부 채팅 모델은 **도구 호출**을 위해 미세 조정되어 있으며, 이를 위한 전용 API를 제공합니다. 일반적으로 이러한 모델은 비미세 조정 모델보다 도구 호출에 더 뛰어나며, 도구 호출이 필요한 사용 사례에 권장됩니다. 자세한 내용은 [도구 호출 섹션](/docs/concepts/#functiontool-calling)을 참조하세요.  
:::

채팅 모델을 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 방법 가이드를 참조하세요](/docs/how_to/#chat-models).

#### 다중 모달성

일부 채팅 모델은 다중 모달로, 이미지, 오디오 및 비디오를 입력으로 수용합니다. 이러한 모델은 여전히 덜 일반적이며, 모델 제공자는 API를 정의하는 "최고의" 방법에 대해 표준화하지 않았습니다. 다중 모달 **출력**은 더욱 덜 일반적입니다. 따라서 우리는 다중 모달 추상화를 비교적 가볍게 유지했으며, 이 분야가 성숙해짐에 따라 다중 모달 API 및 상호 작용 패턴을 더욱 확고히 할 계획입니다.

LangChain에서 다중 모달 입력을 지원하는 대부분의 채팅 모델은 OpenAI의 콘텐츠 블록 형식으로 이러한 값을 수용합니다. 현재까지 이는 이미지 입력으로 제한됩니다. 비디오 및 기타 바이트 입력을 지원하는 Gemini와 같은 모델의 경우, API는 기본 모델별 표현도 지원합니다.

다중 모달 모델을 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 방법 가이드를 참조하세요](/docs/how_to/#multimodal).

다중 모달 모델을 가진 LangChain 모델 제공자의 전체 목록은 [이 표를 확인하세요](/docs/integrations/chat/#advanced-features).

### LLMs  
<span data-heading-keywords="llm,llms"></span>  

:::caution  
순수 텍스트 입력/출력 LLM은 일반적으로 오래되었거나 낮은 수준의 모델입니다. 많은 인기 있는 모델은 비채팅 사용 사례에서도 [채팅 완성 모델](/docs/concepts/#chat-models)로 사용하는 것이 가장 좋습니다.  
대신 [위 섹션을 참조하세요](/docs/concepts/#chat-models).  
:::

문자열을 입력으로 받아 문자열을 반환하는 언어 모델입니다. 이러한 모델은 전통적으로 오래된 모델입니다(더 최신 모델은 일반적으로 [채팅 모델](/docs/concepts/#chat-models)입니다).

기본 모델은 문자열 입력/출력 방식이지만, LangChain 래퍼는 이러한 모델이 메시지를 입력으로 받을 수 있도록 허용합니다. 이는 [채팅 모델](/docs/concepts/#chat-models)과 동일한 인터페이스를 제공합니다. 메시지가 입력으로 전달되면 기본 모델에 전달되기 전에 내부적으로 문자열로 포맷됩니다.

LangChain은 어떤 LLM도 호스팅하지 않으며, 대신 제3자 통합에 의존합니다.

LLM을 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 방법 가이드를 참조하세요](/docs/how_to/#llms).

### 메시지

일부 언어 모델은 메시지 목록을 입력으로 받아 메시지를 반환합니다. 메시지의 몇 가지 유형이 있습니다. 모든 메시지는 `role`, `content`, 및 `response_metadata` 속성을 가집니다.

`role`은 메시지를 누가 말하고 있는지를 설명합니다. LangChain은 서로 다른 역할에 대해 서로 다른 메시지 클래스를 가지고 있습니다.

`content` 속성은 메시지의 내용을 설명합니다. 이는 몇 가지 다른 것일 수 있습니다:

- 문자열(대부분의 모델은 이 유형의 콘텐츠를 처리합니다)
- 사전 목록(이는 다중 모달 입력에 사용되며, 사전은 해당 입력 유형 및 입력 위치에 대한 정보를 포함합니다)

#### HumanMessage

이는 사용자로부터의 메시지를 나타냅니다.
#### AIMessage

이것은 모델의 메시지를 나타냅니다. `content` 속성 외에도 이러한 메시지에는 다음이 포함됩니다:

**`response_metadata`**

`response_metadata` 속성은 응답에 대한 추가 메타데이터를 포함합니다. 여기의 데이터는 종종 각 모델 제공자에 따라 다릅니다. 여기에는 로그 확률 및 토큰 사용량과 같은 정보가 저장될 수 있습니다.

**`tool_calls`**

이들은 언어 모델이 도구를 호출하기로 결정한 것을 나타냅니다. 이들은 `AIMessage` 출력의 일부로 포함됩니다. `.tool_calls` 속성을 통해 접근할 수 있습니다.

이 속성은 `ToolCall`의 목록을 반환합니다. `ToolCall`은 다음 인수를 가진 사전입니다:

- `name`: 호출해야 할 도구의 이름입니다.
- `args`: 해당 도구에 대한 인수입니다.
- `id`: 해당 도구 호출의 ID입니다.

#### SystemMessage

이것은 모델에게 어떻게 행동해야 하는지를 알려주는 시스템 메시지를 나타냅니다. 모든 모델 제공자가 이를 지원하는 것은 아닙니다.

#### ToolMessage

이것은 도구 호출의 결과를 나타냅니다. `role`과 `content` 외에도 이 메시지에는 다음이 있습니다:

- 호출된 도구의 ID를 전달하는 `tool_call_id` 필드.
- 추적에 유용하지만 모델에 전송되어서는 안 되는 도구 실행의 임의 아티팩트를 전달하는 데 사용할 수 있는 `artifact` 필드.

#### (Legacy) FunctionMessage

이것은 OpenAI의 레거시 함수 호출 API에 해당하는 레거시 메시지 유형입니다. `ToolMessage`는 업데이트된 도구 호출 API에 해당하는 대신 사용해야 합니다.

이것은 함수 호출의 결과를 나타냅니다. `role`과 `content` 외에도 이 메시지에는 이 결과를 생성하기 위해 호출된 함수의 이름을 전달하는 `name` 매개변수가 있습니다.

### Prompt templates
<span data-heading-keywords="prompt,prompttemplate,chatprompttemplate"></span>

프롬프트 템플릿은 사용자 입력 및 매개변수를 언어 모델에 대한 지침으로 변환하는 데 도움이 됩니다. 이는 모델의 응답을 안내하고, 맥락을 이해하며 관련 있고 일관된 언어 기반 출력을 생성하는 데 도움이 됩니다.

프롬프트 템플릿은 각 키가 프롬프트 템플릿에서 채워야 할 변수를 나타내는 사전을 입력으로 받습니다.

프롬프트 템플릿은 PromptValue를 출력합니다. 이 PromptValue는 LLM 또는 ChatModel에 전달될 수 있으며, 문자열 또는 메시지 목록으로 변환될 수 있습니다. 이 PromptValue가 존재하는 이유는 문자열과 메시지 간의 전환을 쉽게 하기 위함입니다.

프롬프트 템플릿에는 몇 가지 유형이 있습니다:

#### String PromptTemplates

이 프롬프트 템플릿은 단일 문자열을 형식화하는 데 사용되며, 일반적으로 더 간단한 입력에 사용됩니다. 예를 들어, 프롬프트 템플릿을 구성하고 사용하는 일반적인 방법은 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")

prompt_template.invoke({"topic": "cats"})
```


#### ChatPromptTemplates

이 프롬프트 템플릿은 메시지 목록을 형식화하는 데 사용됩니다. 이러한 "템플릿"은 템플릿 자체의 목록으로 구성됩니다. 예를 들어, ChatPromptTemplate을 구성하고 사용하는 일반적인 방법은 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

prompt_template.invoke({"topic": "cats"})
```


위의 예에서 이 ChatPromptTemplate은 호출될 때 두 개의 메시지를 구성합니다. 첫 번째는 변수를 형식화할 필요가 없는 시스템 메시지입니다. 두 번째는 HumanMessage이며, 사용자가 전달한 `topic` 변수를 통해 형식화됩니다.

#### MessagesPlaceholder
<span data-heading-keywords="messagesplaceholder"></span>

이 프롬프트 템플릿은 특정 위치에 메시지 목록을 추가하는 역할을 합니다. 위의 ChatPromptTemplate에서는 두 개의 메시지를 형식화하는 방법을 보았습니다. 하지만 사용자가 특정 위치에 삽입할 메시지 목록을 전달하고 싶다면 어떻게 해야 할까요? 이것이 MessagesPlaceholder를 사용하는 방법입니다.

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}, {"imported": "MessagesPlaceholder", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html", "title": "Conceptual guide"}, {"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})
```


이것은 두 개의 메시지 목록을 생성합니다. 첫 번째는 시스템 메시지이고, 두 번째는 우리가 전달한 HumanMessage입니다. 만약 우리가 5개의 메시지를 전달했다면, 총 6개의 메시지가 생성됩니다(시스템 메시지와 전달된 5개 메시지 포함). 이는 메시지 목록을 특정 위치에 삽입할 수 있도록 하는 데 유용합니다.

`MessagesPlaceholder` 클래스를 명시적으로 사용하지 않고 동일한 작업을 수행하는 대안적인 방법은 다음과 같습니다:

```python
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("placeholder", "{msgs}") # <-- This is the changed part
])
```


프롬프트 템플릿 사용에 대한 구체적인 내용은 [관련 사용 가이드 여기](/docs/how_to/#prompt-templates)를 참조하십시오.

### Example selectors
더 나은 성능을 달성하기 위한 일반적인 프롬프트 기법 중 하나는 프롬프트의 일부로 예제를 포함하는 것입니다. 이는 언어 모델에게 어떻게 행동해야 하는지에 대한 구체적인 예를 제공합니다. 때때로 이러한 예제는 프롬프트에 하드코딩되지만, 더 고급 상황에서는 동적으로 선택하는 것이 좋습니다. 예제 선택기는 예제를 선택하고 이를 프롬프트로 형식화하는 책임이 있는 클래스입니다.

예제 선택기 사용에 대한 구체적인 내용은 [관련 사용 가이드 여기](/docs/how_to/#example-selectors)를 참조하십시오.

### Output parsers
<span data-heading-keywords="output parser"></span>

:::note

여기의 정보는 모델의 텍스트 출력을 가져와 보다 구조화된 표현으로 파싱하는 파서에 관한 것입니다. 점점 더 많은 모델이 함수(또는 도구) 호출을 지원하고 있으며, 이는 자동으로 처리됩니다. 출력 파싱보다 함수/도구 호출을 사용하는 것이 권장됩니다. 이에 대한 문서는 [여기](/docs/concepts/#function-tool-calling)를 참조하십시오.

:::

모델의 출력을 받아들이고 이를 다운스트림 작업에 더 적합한 형식으로 변환하는 역할을 합니다. LLM을 사용하여 구조화된 데이터를 생성하거나 채팅 모델 및 LLM의 출력을 정규화할 때 유용합니다.

LangChain에는 다양한 유형의 출력 파서가 있습니다. 아래는 LangChain이 지원하는 출력 파서 목록입니다. 아래 표에는 다양한 정보가 포함되어 있습니다:

**이름**: 출력 파서의 이름

**스트리밍 지원**: 출력 파서가 스트리밍을 지원하는지 여부.

**형식 지침 있음**: 출력 파서에 형식 지침이 있는지 여부. 이는 일반적으로 사용 가능하지만 (a) 원하는 스키마가 프롬프트가 아니라 다른 매개변수(예: OpenAI 함수 호출)에서 지정된 경우 또는 (b) OutputParser가 다른 OutputParser를 래핑하는 경우에는 그렇지 않습니다.

**LLM 호출**: 이 출력 파서가 자체적으로 LLM을 호출하는지 여부. 이는 일반적으로 잘못 형식화된 출력을 수정하려는 출력 파서에서만 수행됩니다.

**입력 유형**: 예상 입력 유형. 대부분의 출력 파서는 문자열과 메시지 모두에서 작동하지만 일부(예: OpenAI Functions)는 특정 kwargs가 있는 메시지가 필요합니다.

**출력 유형**: 파서가 반환하는 객체의 출력 유형.

**설명**: 이 출력 파서에 대한 우리의 설명 및 사용 시기.

| 이름            | 스트리밍 지원 | 형식 지침 있음       | LLM 호출 | 입력 유형                       | 출력 유형          | 설명                                                                                                                                                                                                                                              |
|-----------------|--------------------|-------------------------------|-----------|----------------------------------|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [JSON](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html#langchain_core.output_parsers.json.JsonOutputParser)            | ✅                  | ✅                             |           | `str` \| `Message`               | JSON 객체          | 지정된 JSON 객체를 반환합니다. Pydantic 모델을 지정할 수 있으며, 해당 모델에 대한 JSON을 반환합니다. 함수 호출을 사용하지 않는 구조화된 데이터를 얻기 위한 가장 신뢰할 수 있는 출력 파서일 것입니다.                                    |
| [XML](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html#langchain_core.output_parsers.xml.XMLOutputParser)            | ✅                  | ✅                             |           | `str` \| `Message`                 | `dict`               | XML 출력이 필요할 때 사용합니다. XML 작성을 잘하는 모델(예: Anthropic의 모델)과 함께 사용합니다.                                                                                                                            |
| [CSV](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#langchain_core.output_parsers.list.CommaSeparatedListOutputParser)           | ✅                  | ✅                             |           | `str` \| `Message`                 | `List[str]`          | 쉼표로 구분된 값의 목록을 반환합니다.                                                                                                                                                                                                                |
| [OutputFixing](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html#langchain.output_parsers.fix.OutputFixingParser)    |                    |                               | ✅         | `str` \| `Message`                 |                      | 다른 출력 파서를 래핑합니다. 해당 출력 파서에서 오류가 발생하면, 이 출력 파서는 오류 메시지와 잘못된 출력을 LLM에 전달하고 출력을 수정하도록 요청합니다.                                                                                              |
| [RetryWithError](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html#langchain.output_parsers.retry.RetryWithErrorOutputParser)  |                    |                               | ✅         | `str` \| `Message`                 |                      | 다른 출력 파서를 래핑합니다. 해당 출력 파서에서 오류가 발생하면, 이 출력 파서는 원래 입력, 잘못된 출력 및 오류 메시지를 LLM에 전달하고 이를 수정하도록 요청합니다. OutputFixingParser와 비교할 때, 이 출력 파서는 원래 지침도 전송합니다. |
| [Pydantic](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html#langchain_core.output_parsers.pydantic.PydanticOutputParser)        |                    | ✅                             |           | `str` \| `Message`                 | `pydantic.BaseModel` | 사용자 정의 Pydantic 모델을 가져와 해당 형식으로 데이터를 반환합니다.                                                                                                                                                                                     |
| [YAML](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html#langchain.output_parsers.yaml.YamlOutputParser)        |                    | ✅                             |           | `str` \| `Message`                 | `pydantic.BaseModel` | 사용자 정의 Pydantic 모델을 가져와 해당 형식으로 데이터를 반환합니다. YAML을 사용하여 인코딩합니다.                                                                                                                                                                                    |
| [PandasDataFrame](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser) |                    | ✅                             |           | `str` \| `Message`                 | `dict`               | pandas DataFrame으로 작업할 때 유용합니다.                                                                                                                                                                                                      |
| [Enum](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)            |                    | ✅                             |           | `str` \| `Message`                 | `Enum`               | 응답을 제공된 열거형 값 중 하나로 파싱합니다.                                                                                                                                                                                                    |
| [Datetime](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)        |                    | ✅                             |           | `str` \| `Message`                 | `datetime.datetime`  | 응답을 날짜 및 시간 문자열로 파싱합니다.                                                                                                                                                                                                                  |
| [Structured](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)      |                    | ✅                             |           | `str` \| `Message`                 | `Dict[str, str]`     | 구조화된 정보를 반환하는 출력 파서입니다. 이는 필드가 문자열만 허용되기 때문에 다른 출력 파서보다 덜 강력합니다. 이는 더 작은 LLM과 작업할 때 유용할 수 있습니다.                                            |

출력 파서 사용에 대한 구체적인 내용은 [관련 사용 가이드 여기](/docs/how_to/#output-parsers)를 참조하십시오.

### Chat history
대부분의 LLM 애플리케이션은 대화형 인터페이스를 가지고 있습니다. 대화의 필수 요소는 대화 중에 이전에 소개된 정보를 참조할 수 있는 것입니다. 최소한, 대화형 시스템은 과거 메시지의 일부 창에 직접 접근할 수 있어야 합니다.

`ChatHistory` 개념은 LangChain에서 임의의 체인을 래핑하는 데 사용할 수 있는 클래스를 나타냅니다. 이 `ChatHistory`는 기본 체인의 입력 및 출력을 추적하고 이를 메시지 데이터베이스에 메시지로 추가합니다. 이후 상호작용은 이러한 메시지를 로드하고 이를 입력의 일부로 체인에 전달합니다.

### Documents
<span data-heading-keywords="document,documents"></span>

LangChain의 Document 객체는 일부 데이터에 대한 정보를 포함합니다. 두 개의 속성이 있습니다:

- `page_content: str`: 이 문서의 내용입니다. 현재는 문자열만 포함됩니다.
- `metadata: dict`: 이 문서와 관련된 임의의 메타데이터입니다. 문서 ID, 파일 이름 등을 추적할 수 있습니다.

### Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

이 클래스들은 Document 객체를 로드합니다. LangChain은 Slack, Notion, Google Drive 등 다양한 데이터 소스와의 수백 가지 통합을 가지고 있습니다.

각 DocumentLoader는 고유한 특정 매개변수를 가지고 있지만, 모두 `.load` 메서드를 사용하여 동일한 방식으로 호출할 수 있습니다. 사용 사례의 예는 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "CSVLoader", "source": "langchain_community.document_loaders.csv_loader", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html", "title": "Conceptual guide"}]-->
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # <-- Integration specific parameters here
)
data = loader.load()
```


문서 로더 사용에 대한 구체적인 내용은 [관련 사용 가이드 여기](/docs/how_to/#document-loaders)를 참조하십시오.
### 텍스트 분할기

문서를 로드한 후, 애플리케이션에 더 적합하도록 변환하고 싶을 때가 많습니다. 가장 간단한 예는 긴 문서를 모델의 컨텍스트 창에 맞는 더 작은 조각으로 나누고 싶을 때입니다. LangChain에는 문서를 쉽게 분할, 결합, 필터링 및 조작할 수 있는 여러 내장 문서 변환기가 있습니다.

긴 텍스트를 다루고 싶을 때, 그 텍스트를 조각으로 나누는 것이 필요합니다. 간단하게 들리지만, 여기에는 많은 잠재적 복잡성이 있습니다. 이상적으로는 의미적으로 관련된 텍스트 조각을 함께 유지하고 싶습니다. "의미적으로 관련된" 것이 무엇을 의미하는지는 텍스트의 유형에 따라 달라질 수 있습니다. 이 노트북에서는 이를 수행하는 여러 가지 방법을 보여줍니다.

높은 수준에서 텍스트 분할기는 다음과 같이 작동합니다:

1. 텍스트를 작고 의미 있는 조각(종종 문장)으로 나눕니다.
2. 특정 크기에 도달할 때까지 이러한 작은 조각을 결합하기 시작합니다(어떤 함수로 측정됨).
3. 그 크기에 도달하면 그 조각을 별도의 텍스트 조각으로 만들고, 일부 겹침이 있는 새로운 텍스트 조각을 만들기 시작합니다(조각 간의 맥락을 유지하기 위해).

즉, 텍스트 분할기를 사용자 정의할 수 있는 두 가지 축이 있습니다:

1. 텍스트가 나누어지는 방식
2. 조각 크기가 측정되는 방식

텍스트 분할기를 사용하는 방법에 대한 구체적인 내용은 [관련 사용 안내서 여기](/docs/how_to/#text-splitters)를 참조하세요.

### 임베딩 모델
<span data-heading-key워드="embedding,embeddings"></span>

임베딩 모델은 텍스트 조각의 벡터 표현을 생성합니다. 벡터를 텍스트의 의미를 포착하는 숫자 배열로 생각할 수 있습니다. 이러한 방식으로 텍스트를 표현함으로써, 의미가 가장 유사한 다른 텍스트 조각을 검색하는 등의 수학적 작업을 수행할 수 있습니다. 이러한 자연어 검색 기능은 [컨텍스트 검색](/docs/concepts/#retrieval)의 많은 유형을 뒷받침하며, 여기서 우리는 LLM에 쿼리에 효과적으로 응답하는 데 필요한 관련 데이터를 제공합니다.

![](/img/embeddings.png)

`Embeddings` 클래스는 텍스트 임베딩 모델과 인터페이스하기 위해 설계된 클래스입니다. 다양한 임베딩 모델 제공자(OpenAI, Cohere, Hugging Face 등)와 로컬 모델이 있으며, 이 클래스는 이들 모두에 대한 표준 인터페이스를 제공하도록 설계되었습니다.

LangChain의 기본 Embeddings 클래스는 문서를 임베딩하는 메서드와 쿼리를 임베딩하는 메서드 두 가지를 제공합니다. 전자는 여러 텍스트를 입력으로 받고, 후자는 단일 텍스트를 입력으로 받습니다. 이러한 두 개의 별도 메서드를 두는 이유는 일부 임베딩 제공자가 문서(검색할 수 있는)와 쿼리(검색 쿼리 자체)에 대해 서로 다른 임베딩 방법을 가지고 있기 때문입니다.

임베딩 모델을 사용하는 방법에 대한 구체적인 내용은 [관련 사용 안내서 여기](/docs/how_to/#embedding-models)를 참조하세요.

### 벡터 저장소
<span data-heading-key워드="vector,vectorstore,vectorstores,vector store,vector stores"></span>

비구조화된 데이터를 저장하고 검색하는 가장 일반적인 방법 중 하나는 이를 임베딩하고 결과 임베딩 벡터를 저장하는 것이며, 쿼리 시 비구조화된 쿼리를 임베딩하고 임베딩된 쿼리와 '가장 유사한' 임베딩 벡터를 검색하는 것입니다. 벡터 저장소는 임베딩된 데이터를 저장하고 벡터 검색을 수행하는 역할을 합니다.

대부분의 벡터 저장소는 또한 임베딩 벡터에 대한 메타데이터를 저장할 수 있으며, 유사성 검색 전에 해당 메타데이터에 대한 필터링을 지원하여 반환된 문서에 대한 더 많은 제어를 제공합니다.

벡터 저장소는 다음과 같이 검색기 인터페이스로 변환할 수 있습니다:

```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```


벡터 저장소를 사용하는 방법에 대한 구체적인 내용은 [관련 사용 안내서 여기](/docs/how_to/#vector-stores)를 참조하세요.

### 검색기
<span data-heading-key워드="retriever,retrievers"></span>

검색기는 비구조화된 쿼리를 주면 문서를 반환하는 인터페이스입니다. 이는 벡터 저장소보다 더 일반적입니다. 검색기는 문서를 저장할 필요는 없으며, 단지 문서를 반환(또는 검색)하면 됩니다. 검색기는 벡터 저장소에서 생성할 수 있지만, [위키피디아 검색](/docs/integrations/retrievers/wikipedia/) 및 [아마존 켄드라](/docs/integrations/retrievers/amazon_kendra_retriever/)와 같이 더 넓은 범위를 포함합니다.

검색기는 문자열 쿼리를 입력으로 받아 Document의 목록을 출력으로 반환합니다.

검색기를 사용하는 방법에 대한 구체적인 내용은 [관련 사용 안내서 여기](/docs/how_to/#retrievers)를 참조하세요.

### 키-값 저장소

[문서당 여러 벡터로 인덱싱 및 검색하기](/docs/how_to/multi_vector/) 또는 [임베딩 캐싱](/docs/how_to/caching_embeddings/)와 같은 일부 기술에서는 키-값(KV) 저장소 형태가 유용합니다.

LangChain은 임의의 데이터를 저장할 수 있는 [`BaseStore`](https://api.python.langchain.com/en/latest/stores/langchain_core.stores.BaseStore.html) 인터페이스를 포함하고 있습니다. 그러나 KV 저장소가 필요한 LangChain 구성 요소는 이진 데이터를 저장하는 보다 구체적인 `BaseStore[str, bytes]` 인스턴스를 수용하며(이를 `ByteStore`라고 함), 내부적으로 특정 요구 사항에 맞게 데이터를 인코딩하고 디코딩하는 작업을 처리합니다.

즉, 사용자는 서로 다른 유형의 데이터에 대해 서로 다른 저장소를 생각할 필요 없이 하나의 저장소 유형만 고려하면 됩니다.

#### 인터페이스

모든 [`BaseStores`](https://api.python.langchain.com/en/latest/stores/langchain_core.stores.BaseStore.html)는 다음 인터페이스를 지원합니다. 이 인터페이스는 **여러** 키-값 쌍을 동시에 수정할 수 있도록 허용합니다:

- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: 여러 키의 내용을 가져오며, 키가 존재하지 않으면 `None`을 반환
- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: 여러 키의 내용을 설정
- `mdelete(key: Sequence[str]) -> None`: 여러 키를 삭제
- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: 저장소의 모든 키를 생성하며, 선택적으로 접두사로 필터링

키-값 저장소 구현에 대한 내용은 [이 섹션](/docs/integrations/stores/)을 참조하세요.

### 도구
<span data-heading-key워드="tool,tools"></span>

도구는 모델에 의해 호출되도록 설계된 유틸리티입니다: 그들의 입력은 모델에 의해 생성되도록 설계되었고, 그들의 출력은 모델에 다시 전달되도록 설계되었습니다. 모델이 코드의 일부를 제어하거나 외부 API를 호출해야 할 때 도구가 필요합니다.

도구는 다음으로 구성됩니다:

1. 도구의 이름.
2. 도구가 수행하는 작업에 대한 설명.
3. 도구의 입력을 정의하는 JSON 스키마.
4. 함수(선택적으로 함수의 비동기 변형).

도구가 모델에 바인딩되면, 이름, 설명 및 JSON 스키마가 모델에 대한 컨텍스트로 제공됩니다. 도구 목록과 지침 세트를 주어진 경우, 모델은 특정 입력으로 하나 이상의 도구를 호출하도록 요청할 수 있습니다. 일반적인 사용 예는 다음과 같을 수 있습니다:

```python
tools = [...] # Define a list of tools
llm_with_tools = llm.bind_tools(tools)
ai_msg = llm_with_tools.invoke("do xyz...")
# -> AIMessage(tool_calls=[ToolCall(...), ...], ...)
```


모델에서 반환된 `AIMessage`는 `tool_calls`와 연결될 수 있습니다. 응답 유형이 어떻게 생겼는지에 대한 자세한 내용은 [이 가이드](/docs/concepts/#aimessage)를 참조하세요.

선택한 도구가 호출되면, 결과는 모델에 다시 전달되어 모델이 수행 중인 작업을 완료할 수 있도록 합니다. 도구를 호출하고 응답을 전달하는 방법은 일반적으로 두 가지가 있습니다:

#### 인수만으로 호출

인수만으로 도구를 호출하면 원시 도구 출력을 받게 됩니다(일반적으로 문자열). 일반적으로 다음과 같이 보입니다:

```python
# You will want to previously check that the LLM returned tool calls
tool_call = ai_msg.tool_calls[0]
# ToolCall(args={...}, id=..., ...)
tool_output = tool.invoke(tool_call["args"])
tool_message = ToolMessage(
    content=tool_output,
    tool_call_id=tool_call["id"],
    name=tool_call["name"]
)
```


`content` 필드는 일반적으로 모델에 다시 전달됩니다. 원시 도구 응답을 모델에 전달하고 싶지 않지만 여전히 보관하고 싶다면, 도구 출력을 변환하되 아티팩트로 전달할 수 있습니다(자세한 내용은 [`ToolMessage.artifact` 여기](/docs/concepts/#toolmessage)를 참조하세요).

```python
... # Same code as above
response_for_llm = transform(response)
tool_message = ToolMessage(
    content=response_for_llm,
    tool_call_id=tool_call["id"],
    name=tool_call["name"],
    artifact=tool_output
)
```


#### `ToolCall`로 호출

도구를 호출하는 또 다른 방법은 모델에 의해 생성된 전체 `ToolCall`로 호출하는 것입니다. 이렇게 하면 도구가 ToolMessage를 반환합니다. 이 방법의 이점은 도구 출력을 ToolMessage로 변환하는 로직을 직접 작성할 필요가 없다는 것입니다. 일반적으로 다음과 같이 보입니다:

```python
tool_call = ai_msg.tool_calls[0]
# -> ToolCall(args={...}, id=..., ...)
tool_message = tool.invoke(tool_call)
# -> ToolMessage(
    content="tool result foobar...",
    tool_call_id=...,
    name="tool_name"
)
```


이 방법으로 도구를 호출하고 ToolMessage에 대한 [아티팩트](/docs/concepts/#toolmessage)를 포함하려면 도구가 두 가지를 반환해야 합니다. 아티팩트를 반환하는 도구 정의에 대한 자세한 내용은 [여기](/docs/how_to/tool_artifacts/)를 참조하세요.

#### 모범 사례

모델이 사용할 도구를 설계할 때 다음 사항을 염두에 두는 것이 중요합니다:

- 명시적인 [도구 호출 API](/docs/concepts/#functiontool-calling)를 가진 채팅 모델이 비세밀하게 조정된 모델보다 도구 호출을 더 잘 수행합니다.
- 도구의 이름, 설명 및 JSON 스키마가 잘 선택되면 모델이 더 잘 수행됩니다. 이는 또 다른 형태의 프롬프트 엔지니어링입니다.
- 단순하고 좁은 범위의 도구가 복잡한 도구보다 모델이 사용하기 더 쉽습니다.

#### 관련

도구를 사용하는 방법에 대한 구체적인 내용은 [도구 사용 안내서](/docs/how_to/#tools)를 참조하세요.

미리 구축된 도구를 사용하려면 [도구 통합 문서](/docs/integrations/tools/)를 참조하세요.

### 툴킷
<span data-heading-key워드="toolkit,toolkits"></span>

툴킷은 특정 작업을 위해 함께 사용되도록 설계된 도구 모음입니다. 이들은 편리한 로딩 방법을 가지고 있습니다.

모든 툴킷은 도구 목록을 반환하는 `get_tools` 메서드를 노출합니다. 따라서 다음과 같이 할 수 있습니다:

```python
# Initialize a toolkit
toolkit = ExampleTookit(...)

# Get list of tools
tools = toolkit.get_tools()
```


### 에이전트

언어 모델은 스스로 행동을 취할 수 없으며, 단지 텍스트를 출력합니다. LangChain의 주요 사용 사례는 **에이전트**를 만드는 것입니다. 에이전트는 LLM을 추론 엔진으로 사용하여 어떤 행동을 취할지, 그 행동의 입력이 무엇이어야 하는지를 결정하는 시스템입니다. 이러한 행동의 결과는 다시 에이전트에 피드백되어 추가 행동이 필요한지, 아니면 종료해도 되는지를 결정합니다.

[LangGraph](https://github.com/langchain-ai/langgraph)는 매우 제어 가능하고 사용자 정의 가능한 에이전트를 만드는 것을 목표로 하는 LangChain의 확장입니다. 에이전트 개념에 대한 보다 심층적인 개요는 해당 문서를 참조하세요.

LangChain에는 우리가 더 이상 사용하지 않으려는 레거시 에이전트 개념이 있습니다: `AgentExecutor`. AgentExecutor는 본질적으로 에이전트를 위한 런타임이었습니다. 시작하기에 좋은 장소였지만, 더 많은 사용자 정의 에이전트를 갖기 시작하면서 유연성이 부족했습니다. 이를 해결하기 위해 LangGraph를 구축하여 유연하고 제어 가능한 런타임이 되도록 했습니다.

여전히 AgentExecutor를 사용하고 있다면 걱정하지 마세요: [AgentExecutor 사용 방법](/docs/how_to/agent_executor)에 대한 가이드가 여전히 있습니다. 그러나 LangGraph로 전환하는 것이 좋습니다. 이를 지원하기 위해 [전환 가이드](/docs/how_to/migrate_agent)를 준비했습니다.

#### ReAct 에이전트
<span data-heading-key워드="react,react agent"></span>

에이전트를 구축하기 위한 인기 있는 아키텍처 중 하나는 [**ReAct**](https://arxiv.org/abs/2210.03629)입니다. ReAct는 추론과 행동을 반복적인 과정으로 결합합니다. 사실 "ReAct"라는 이름은 "Reason"과 "Act"를 의미합니다.

일반적인 흐름은 다음과 같습니다:

- 모델은 입력 및 이전 관찰에 대한 응답으로 어떤 단계를 취할지 "생각"합니다.
- 모델은 사용 가능한 도구 중에서 행동을 선택합니다(또는 사용자에게 응답하기로 선택합니다).
- 모델은 해당 도구에 대한 인수를 생성합니다.
- 에이전트 런타임(실행기)은 선택한 도구를 파싱하고 생성된 인수로 호출합니다.
- 실행기는 도구 호출의 결과를 모델에 관찰로 반환합니다.
- 이 과정은 에이전트가 응답하기로 선택할 때까지 반복됩니다.

모델 특정 기능이 필요 없는 일반적인 프롬프트 기반 구현이 있지만, 가장 신뢰할 수 있는 구현은 [도구 호출](/docs/how_to/tool_calling/)과 같은 기능을 사용하여 출력을 신뢰성 있게 형식화하고 변동성을 줄입니다.

자세한 내용은 [LangGraph 문서](https://langchain-ai.github.io/langgraph/)를 참조하거나, LangGraph로의 전환에 대한 구체적인 정보는 [이 사용 안내서](/docs/how_to/migrate_agent/)를 참조하세요.

### 콜백

LangChain은 LLM 애플리케이션의 다양한 단계에 연결할 수 있는 콜백 시스템을 제공합니다. 이는 로깅, 모니터링, 스트리밍 및 기타 작업에 유용합니다.

API 전반에 걸쳐 사용 가능한 `callbacks` 인수를 사용하여 이러한 이벤트를 구독할 수 있습니다. 이 인수는 아래에서 더 자세히 설명된 하나 이상의 메서드를 구현할 것으로 예상되는 핸들러 객체의 목록입니다.
#### 콜백 이벤트

| 이벤트            | 이벤트 트리거                               | 관련 메서드            |
|------------------|---------------------------------------------|-----------------------|
| 채팅 모델 시작   | 채팅 모델이 시작될 때                      | `on_chat_model_start` |
| LLM 시작         | LLM이 시작될 때                            | `on_llm_start`        |
| LLM 새 토큰      | LLM 또는 채팅 모델이 새 토큰을 발행할 때 | `on_llm_new_token`    |
| LLM 종료         | LLM 또는 채팅 모델이 종료될 때            | `on_llm_end`          |
| LLM 오류         | LLM 또는 채팅 모델에서 오류가 발생할 때  | `on_llm_error`        |
| 체인 시작        | 체인이 실행되기 시작할 때                 | `on_chain_start`      |
| 체인 종료        | 체인이 종료될 때                          | `on_chain_end`        |
| 체인 오류        | 체인에서 오류가 발생할 때                 | `on_chain_error`      |
| 도구 시작        | 도구가 실행되기 시작할 때                 | `on_tool_start`       |
| 도구 종료        | 도구가 종료될 때                          | `on_tool_end`         |
| 도구 오류        | 도구에서 오류가 발생할 때                 | `on_tool_error`       |
| 에이전트 액션    | 에이전트가 액션을 취할 때                 | `on_agent_action`     |
| 에이전트 종료     | 에이전트가 종료될 때                      | `on_agent_finish`     |
| 검색기 시작      | 검색기가 시작될 때                        | `on_retriever_start`  |
| 검색기 종료      | 검색기가 종료될 때                        | `on_retriever_end`    |
| 검색기 오류      | 검색기에서 오류가 발생할 때               | `on_retriever_error`  |
| 텍스트           | 임의의 텍스트가 실행될 때                 | `on_text`             |
| 재시도           | 재시도 이벤트가 실행될 때                 | `on_retry`            |

#### 콜백 핸들러

콜백 핸들러는 `sync` 또는 `async`일 수 있습니다:

* 동기 콜백 핸들러는 [BaseCallbackHandler](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) 인터페이스를 구현합니다.
* 비동기 콜백 핸들러는 [AsyncCallbackHandler](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html) 인터페이스를 구현합니다.

런타임 동안 LangChain은 이벤트가 트리거될 때 "등록된" 각 콜백 핸들러에서 적절한 메서드를 호출할 책임이 있는 적절한 콜백 관리자를 구성합니다 (예: [CallbackManager](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.manager.CallbackManager.html) 또는 [AsyncCallbackManager](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.manager.AsyncCallbackManager.html)).

#### 콜백 전달

`callbacks` 속성은 API 전반의 대부분의 객체(모델, 도구, 에이전트 등)에서 두 가지 다른 위치에서 사용할 수 있습니다:

콜백은 API 전반의 대부분의 객체(모델, 도구, 에이전트 등)에서 두 가지 다른 위치에서 사용할 수 있습니다:

- **요청 시 콜백**: 입력 데이터와 함께 요청 시 전달됩니다. 모든 표준 `Runnable` 객체에서 사용할 수 있습니다. 이러한 콜백은 정의된 객체의 모든 자식에 의해 상속됩니다. 예: `chain.invoke({"number": 25}, {"callbacks": [handler]})`.
- **생성자 콜백**: `chain = TheNameOfSomeChain(callbacks=[handler])`. 이러한 콜백은 객체의 생성자에 인수로 전달됩니다. 콜백은 정의된 객체에만 범위가 있으며, 객체의 자식에게는 **상속되지 않습니다**.

:::warning
생성자 콜백은 정의된 객체에만 범위가 있습니다. 자식 객체에 **상속되지 않습니다**.
:::

사용자 정의 체인 또는 실행 가능 객체를 생성하는 경우 요청 시 콜백을 모든 자식 객체에 전파해야 함을 기억해야 합니다.

:::important Async in Python<=3.10

다른 실행 가능 객체를 호출하고 python<=3.10에서 비동기로 실행되는 모든 `RunnableLambda`, `RunnableGenerator` 또는 `Tool`은 콜백을 자식 객체에 수동으로 전파해야 합니다. 이는 이 경우 LangChain이 자식 객체에 콜백을 자동으로 전파할 수 없기 때문입니다.

이는 사용자 정의 실행 가능 객체나 도구에서 이벤트가 발생하지 않는 일반적인 이유입니다.
:::

콜백 사용에 대한 세부정보는 [관련 사용 안내서 여기](/docs/how_to/#callbacks)를 참조하십시오.

## 기술

### 스트리밍
<span data-heading-keywords="stream,streaming"></span>

개별 LLM 호출은 종종 전통적인 리소스 요청보다 훨씬 더 오랜 시간 동안 실행됩니다. 이는 여러 추론 단계를 요구하는 더 복잡한 체인이나 에이전트를 구축할 때 더욱 심화됩니다.

다행히도, LLM은 출력을 반복적으로 생성하므로 최종 응답이 준비되기 전에 합리적인 중간 결과를 보여줄 수 있습니다. 출력이 사용 가능해지는 즉시 소비하는 것은 LLM을 사용하여 앱을 구축할 때 지연 문제를 완화하는 데 중요한 부분이 되었으며, LangChain은 스트리밍에 대한 일급 지원을 목표로 하고 있습니다.

아래에서는 LangChain의 스트리밍과 관련된 몇 가지 개념과 고려 사항에 대해 논의하겠습니다.

#### `.stream()` 및 `.astream()`

LangChain의 대부분 모듈은 인체공학적인 스트리밍 인터페이스로서 `.stream()` 메서드(및 [비동기](https://docs.python.org/3/library/asyncio.html) 환경을 위한 동등한 `.astream()` 메서드)를 포함합니다. `.stream()`은 반복자를 반환하며, 간단한 `for` 루프로 소비할 수 있습니다. 다음은 채팅 모델의 예입니다:

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Conceptual guide"}]-->
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")

for chunk in model.stream("what color is the sky?"):
    print(chunk.content, end="|", flush=True)
```


스트리밍을 기본적으로 지원하지 않는 모델(또는 다른 구성 요소)의 경우 이 반복자는 단일 청크만 생성하지만, 호출할 때 동일한 일반 패턴을 사용할 수 있습니다. `.stream()`을 사용하면 추가 구성 없이 자동으로 스트리밍 모드에서 모델을 호출합니다.

출력된 각 청크의 유형은 구성 요소의 유형에 따라 다릅니다 - 예를 들어, 채팅 모델은 [`AIMessageChunks`](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessageChunk.html)를 생성합니다. 이 메서드는 [LangChain 표현 언어](/docs/concepts/#langchain-expression-language-lcel)의 일부이므로, 출력 파서를 사용하여 서로 다른 출력의 형식 차이를 처리할 수 있습니다.

`.stream()` 사용 방법에 대한 자세한 내용은 [이 가이드](/docs/how_to/streaming/#using-stream)를 참조하십시오.

#### `.astream_events()`
<span data-heading-keywords="astream_events,stream_events,stream events"></span>

`.stream()` 메서드는 직관적이지만 체인의 최종 생성 값만 반환할 수 있습니다. 이는 단일 LLM 호출에는 괜찮지만, 여러 LLM 호출로 구성된 더 복잡한 체인을 구축할 때는 최종 출력과 함께 체인의 중간 값을 사용하고 싶을 수 있습니다 - 예를 들어, 문서에 대한 채팅 앱을 구축할 때 최종 생성과 함께 출처를 반환하는 경우입니다.

이를 위해 [콜백](/docs/concepts/#callbacks-1)을 사용하거나 체인을 구성하여 중간 값을 최종 결과로 전달하는 방법이 있지만, LangChain은 또한 콜백의 유연성과 `.stream()`의 인체공학을 결합한 `.astream_events()` 메서드를 포함합니다. 호출 시 다양한 유형의 이벤트를 생성하는 반복자를 반환하며, 이를 프로젝트의 필요에 따라 필터링하고 처리할 수 있습니다.

다음은 스트리밍된 채팅 모델 출력을 포함하는 이벤트만 인쇄하는 작은 예입니다:

```python
<!--IMPORTS:[{"imported": "StrOutputParser", "source": "langchain_core.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html", "title": "Conceptual guide"}, {"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}, {"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Conceptual guide"}]-->
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}, version="v2"):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```


대략적으로 콜백 이벤트에 대한 반복자로 생각할 수 있으며(형식은 다르지만) 거의 모든 LangChain 구성 요소에서 사용할 수 있습니다!

`.astream_events()` 사용 방법에 대한 자세한 정보는 [이 가이드](/docs/how_to/streaming/#using-stream-events)를 참조하십시오. 여기에는 사용 가능한 이벤트 목록이 포함된 표가 있습니다.

#### 콜백

LangChain에서 LLM의 출력을 스트리밍하는 가장 낮은 수준의 방법은 [콜백](/docs/concepts/#callbacks) 시스템을 통해 이루어집니다. 콜백 핸들러를 전달하여 [`on_llm_new_token`](https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html#langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.on_llm_new_token) 이벤트를 처리할 수 있습니다. 해당 구성 요소가 호출될 때, 구성 요소에 포함된 모든 [LLM](/docs/concepts/#llms) 또는 [채팅 모델](/docs/concepts/#chat-models)은 생성된 토큰으로 콜백을 호출합니다. 콜백 내에서 토큰을 다른 목적지로 파이프할 수 있습니다. 예를 들어, HTTP 응답으로 보낼 수 있습니다. 또한 [`on_llm_end`](https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html#langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.on_llm_end) 이벤트를 처리하여 필요한 정리를 수행할 수 있습니다.

콜백 사용에 대한 구체적인 내용은 [이 사용 안내서 섹션](/docs/how_to/#callbacks)을 참조하십시오.

콜백은 LangChain에서 스트리밍을 위해 처음 도입된 기술입니다. 강력하고 일반화할 수 있지만, 개발자에게는 다루기 어려울 수 있습니다. 예를 들어:

- 결과를 수집하기 위해 일부 집계기 또는 다른 스트림을 명시적으로 초기화하고 관리해야 합니다.
- 실행 순서가 명시적으로 보장되지 않으며, 이론적으로는 `.invoke()` 메서드가 완료된 후 콜백이 실행될 수 있습니다.
- 제공자는 종종 출력을 스트리밍하기 위해 추가 매개변수를 전달해야 하며, 모든 출력을 한 번에 반환하지 않습니다.
- 실제 모델 호출의 결과를 무시하고 콜백 결과를 선호하는 경우가 많습니다.

#### 토큰

대부분의 모델 제공자가 입력 및 출력을 측정하는 데 사용하는 단위는 **토큰**이라고 불리는 단위입니다. 토큰은 언어 모델이 텍스트를 처리하거나 생성할 때 읽고 생성하는 기본 단위입니다. 토큰의 정확한 정의는 모델이 훈련된 특정 방식에 따라 달라질 수 있습니다 - 예를 들어, 영어에서 토큰은 "apple"과 같은 단일 단어이거나 "app"과 같은 단어의 일부일 수 있습니다.

모델에 프롬프트를 보내면, 프롬프트의 단어와 문자가 **토크나이저**를 사용하여 토큰으로 인코딩됩니다. 그런 다음 모델은 생성된 출력 토큰을 스트리밍하여, 토크나이저가 이를 사람이 읽을 수 있는 텍스트로 디코딩합니다. 아래 예시는 OpenAI 모델이 `LangChain is cool!`을 어떻게 토큰화하는지를 보여줍니다:

![](/img/tokenization.png)

5개의 서로 다른 토큰으로 분할되며, 토큰 간의 경계가 정확히 단어 경계와 동일하지 않음을 볼 수 있습니다.

언어 모델이 "문자"와 같은 보다 직관적인 무언가 대신 토큰을 사용하는 이유는 텍스트를 처리하고 이해하는 방식과 관련이 있습니다. 높은 수준에서 언어 모델은 초기 입력과 이전 생성에 따라 다음 생성 출력을 반복적으로 예측합니다. 모델을 토큰을 사용하여 훈련하는 것은 언어적 단위(단어 또는 하위 단어와 같은)로 의미를 전달하는 데 도움이 되며, 개별 문자보다 언어 구조(문법 및 맥락 포함)를 배우고 이해하기 쉽게 만듭니다. 또한, 토큰을 사용하면 모델이 문자 수준 처리에 비해 더 적은 텍스트 단위를 처리하므로 효율성을 높일 수 있습니다.

### 함수/도구 호출

:::info
우리는 도구 호출이라는 용어를 함수 호출과 교환하여 사용합니다. 함수 호출은 때때로 단일 함수의 호출을 의미하지만, 우리는 모든 모델이 각 메시지에서 여러 도구 또는 함수 호출을 반환할 수 있는 것으로 간주합니다.
:::

도구 호출은 [채팅 모델](/docs/concepts/#chat-models)이 주어진 프롬프트에 응답하여 사용자 정의 스키마와 일치하는 출력을 생성할 수 있게 합니다.

이름에서 알 수 있듯이 모델이 어떤 작업을 수행하는 것처럼 보이지만, 실제로는 그렇지 않습니다! 모델은 도구에 대한 인수만 생성하며, 도구를 실제로 실행하는 것은 사용자에게 달려 있습니다. 생성된 인수로 함수를 호출하고 싶지 않은 일반적인 예는 비구조적 텍스트에서 [스키마와 일치하는 구조화된 출력을 추출](/docs/concepts/#structured-output)하려는 경우입니다. 이 경우 모델에 원하는 스키마와 일치하는 매개변수를 사용하는 "추출" 도구를 제공하고, 생성된 출력을 최종 결과로 처리합니다.

![채팅 모델에 의한 도구 호출 다이어그램](/img/tool_call.png)

도구 호출은 보편적이지 않지만, [Anthropic](/docs/integrations/chat/anthropic/), [Cohere](/docs/integrations/chat/cohere/), [Google](/docs/integrations/chat/google_vertex_ai_palm/), [Mistral](/docs/integrations/chat/mistralai/), [OpenAI](/docs/integrations/chat/openai/) 및 [Ollama](/docs/integrations/chat/ollama/)와 같은 많은 인기 있는 LLM 제공업체에서 지원됩니다.

LangChain은 다양한 모델에서 일관된 도구 호출을 위한 표준화된 인터페이스를 제공합니다.

표준 인터페이스는 다음으로 구성됩니다:

* `ChatModel.bind_tools()`: 모델이 호출할 수 있는 도구를 지정하는 메서드입니다. 이 메서드는 [LangChain 도구](/docs/concepts/#tools)와 [Pydantic](https://pydantic.dev/) 객체를 허용합니다.
* `AIMessage.tool_calls`: 모델에서 반환된 `AIMessage`의 속성으로, 모델이 요청한 도구 호출에 접근할 수 있습니다.
#### 도구 사용법

모델이 도구를 호출한 후, 도구를 사용하려면 이를 호출하고 인수를 모델에 다시 전달하면 됩니다. LangChain은 이를 처리하는 데 도움이 되는 [`Tool`](/docs/concepts/#tools) 추상화를 제공합니다.

일반적인 흐름은 다음과 같습니다:

1. 쿼리에 대한 응답으로 채팅 모델로 도구 호출을 생성합니다.
2. 생성된 도구 호출을 인수로 사용하여 적절한 도구를 호출합니다.
3. 도구 호출 결과를 [`ToolMessages`](/docs/concepts/#toolmessage) 형식으로 포맷합니다.
4. 전체 메시지 목록을 모델에 다시 전달하여 최종 답변을 생성하게 하거나(또는 더 많은 도구를 호출하게) 합니다.

![완전한 도구 호출 흐름의 다이어그램](/img/tool_calling_flow.png)

이것이 도구 호출 [에이전트](/docs/concepts/#agents)가 작업을 수행하고 쿼리에 응답하는 방법입니다.

아래에서 더 집중된 가이드를 확인하세요:

- [채팅 모델을 사용하여 도구 호출하는 방법](/docs/how_to/tool_calling/)
- [도구 출력을 채팅 모델에 전달하는 방법](/docs/how_to/tool_results_pass_to_model/)
- [LangGraph로 에이전트 구축하기](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

### 구조화된 출력

LLM은 임의의 텍스트를 생성할 수 있습니다. 이는 모델이 다양한 입력에 적절하게 응답할 수 있게 하지만, 일부 사용 사례에서는 LLM의 출력을 특정 형식이나 구조로 제한하는 것이 유용할 수 있습니다. 이를 **구조화된 출력**이라고 합니다.

예를 들어, 출력이 관계형 데이터베이스에 저장되어야 하는 경우, 모델이 정의된 스키마나 형식에 맞는 출력을 생성하는 것이 훨씬 더 쉽습니다. 비구조적 텍스트에서 [특정 정보 추출하기](/docs/tutorials/extraction/)는 특히 유용한 경우입니다. 가장 일반적으로 출력 형식은 JSON이지만, [YAML](/docs/how_to/output_parser_yaml/)과 같은 다른 형식도 유용할 수 있습니다. 아래에서는 LangChain에서 모델로부터 구조화된 출력을 얻는 몇 가지 방법을 논의하겠습니다.

#### `.with_structured_output()`

편의를 위해 일부 LangChain 채팅 모델은 [`.with_structured_output()`](/docs/how_to/structured_output/#the-with_structured_output-method) 메서드를 지원합니다. 이 메서드는 스키마만 입력으로 요구하며, dict 또는 Pydantic 객체를 반환합니다. 일반적으로 이 메서드는 아래에 설명된 더 고급 메서드 중 하나를 지원하는 모델에만 존재하며, 내부적으로 그 중 하나를 사용합니다. 적절한 출력 파서를 가져오고 모델에 맞는 형식으로 스키마를 포맷하는 작업을 처리합니다.

예시입니다:

```python
from typing import Optional

from langchain_core.pydantic_v1 import BaseModel, Field


class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")

structured_llm = llm.with_structured_output(Joke)

structured_llm.invoke("Tell me a joke about cats")
```


```
Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)

```


구조화된 출력을 작업할 때 이 메서드를 시작점으로 추천합니다:

- 출력 파서를 가져올 필요 없이 내부적으로 다른 모델 특정 기능을 사용합니다.
- 도구 호출을 사용하는 모델의 경우 특별한 프롬프트가 필요하지 않습니다.
- 여러 기본 기술이 지원되는 경우, `method` 매개변수를 제공하여 [어떤 것이 사용될지 전환할 수 있습니다](/docs/how_to/structured_output/#advanced-specifying-the-method-for-structuring-outputs).

다음과 같은 경우 다른 기술을 사용해야 할 수도 있습니다:

- 사용 중인 채팅 모델이 도구 호출을 지원하지 않는 경우.
- 매우 복잡한 스키마로 작업 중이며 모델이 일치하는 출력을 생성하는 데 어려움을 겪는 경우.

자세한 내용은 [이 가이드](/docs/how_to/structured_output/#the-with_structured_output-method)를 확인하세요.

`with_structured_output()`을 지원하는 모델 목록은 [이 표](/docs/integrations/chat/#advanced-features)에서 확인할 수 있습니다.

#### 원시 프롬프트

모델이 출력을 구조화하도록 요청하는 가장 직관적인 방법은 정중하게 요청하는 것입니다. 쿼리 외에도 원하는 출력 유형을 설명하는 지침을 제공한 다음, [출력 파서](/docs/concepts/#output-parsers)를 사용하여 원시 모델 메시지 또는 문자열 출력을 더 쉽게 조작할 수 있는 것으로 변환합니다.

원시 프롬프트의 가장 큰 장점은 유연성입니다:

- 원시 프롬프트는 특별한 모델 기능을 요구하지 않으며, 전달된 스키마를 이해할 수 있는 충분한 추론 능력만 필요합니다.
- 원하는 모든 형식으로 프롬프트할 수 있으며, JSON뿐만 아니라 XML이나 YAML과 같은 특정 유형의 데이터에 대해 더 많이 훈련된 모델에 유용할 수 있습니다.

하지만 몇 가지 단점도 있습니다:

- LLM은 비결정적이며, LLM에 원활한 파싱을 위해 정확한 형식으로 데이터를 일관되게 출력하도록 요청하는 것은 놀랍도록 어렵고 모델에 따라 다를 수 있습니다.
- 개별 모델은 훈련된 데이터에 따라 특성이 다르며, 프롬프트 최적화는 상당히 어려울 수 있습니다. 일부는 [JSON 스키마](https://json-schema.org/) 해석에 더 뛰어나고, 다른 일부는 TypeScript 정의에 더 적합하며, 또 다른 일부는 XML을 선호할 수 있습니다.

모델 제공자가 제공하는 기능이 신뢰성을 높일 수 있지만, 어떤 방법을 선택하든 결과를 조정하는 데 프롬프트 기술이 여전히 중요합니다.

#### JSON 모드
<span data-heading-keywords="json mode"></span>

[Mistral](/docs/integrations/chat/mistralai/), [OpenAI](/docs/integrations/chat/openai/), [Together AI](/docs/integrations/chat/together/) 및 [Ollama](/docs/integrations/chat/ollama/)와 같은 일부 모델은 **JSON 모드**라는 기능을 지원하며, 일반적으로 구성에서 활성화됩니다.

활성화되면 JSON 모드는 모델의 출력을 항상 유효한 JSON의 일종으로 제한합니다. 종종 사용자 지정 프롬프트가 필요하지만, 일반적으로 완전히 원시 프롬프트보다 부담이 적고 "항상 JSON을 반환해야 한다"는 식으로 간단합니다. [출력은 일반적으로 더 쉽게 파싱할 수 있습니다](/docs/how_to/output_parser_json/).

또한 일반적으로 도구 호출보다 직접 사용하기가 더 간단하고 더 일반적으로 사용 가능하며, 도구 호출보다 프롬프트 및 결과 형성에 더 많은 유연성을 제공합니다.

예시입니다:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Conceptual guide"}, {"imported": "SimpleJsonOutputParser", "source": "langchain.output_parsers.json", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.SimpleJsonOutputParser.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.output_parsers.json import SimpleJsonOutputParser

model = ChatOpenAI(
    model="gpt-4o",
    model_kwargs={ "response_format": { "type": "json_object" } },
)

prompt = ChatPromptTemplate.from_template(
    "Answer the user's question to the best of your ability."
    'You must always output a JSON object with an "answer" key and a "followup_question" key.'
    "{question}"
)

chain = prompt | model | SimpleJsonOutputParser()

chain.invoke({ "question": "What is the powerhouse of the cell?" })
```


```
{'answer': 'The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.',
 'followup_question': 'Would you like to know more about how mitochondria produce energy?'}
```


JSON 모드를 지원하는 모델 제공자의 전체 목록은 [이 표](/docs/integrations/chat/#advanced-features)에서 확인하세요.

#### 도구 호출 {#structured-output-tool-calling}

지원하는 모델의 경우, [도구 호출](/docs/concepts/#functiontool-calling)은 구조화된 출력을 얻는 데 매우 편리할 수 있습니다. 이는 스키마를 프롬프트하는 최선의 방법에 대한 추측을 제거하고 내장된 모델 기능을 선호합니다.

먼저 원하는 스키마를 직접 또는 [LangChain 도구](/docs/concepts/#tools)를 통해 [채팅 모델](/docs/concepts/#chat-models)에 `.bind_tools()` 메서드를 사용하여 바인딩합니다. 그러면 모델은 원하는 형식에 맞는 `args`를 포함하는 `tool_calls` 필드가 있는 `AIMessage`를 생성합니다.

LangChain에서 도구를 모델에 바인딩하는 데 사용할 수 있는 여러 허용되는 형식이 있습니다. 예시입니다:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Conceptual guide"}]-->
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI

class ResponseFormatter(BaseModel):
    """Always use this tool to structure your response to the user."""

    answer: str = Field(description="The answer to the user's question")
    followup_question: str = Field(description="A followup question the user could ask")

model = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
)

model_with_tools = model.bind_tools([ResponseFormatter])

ai_msg = model_with_tools.invoke("What is the powerhouse of the cell?")

ai_msg.tool_calls[0]["args"]
```


```
{'answer': "The powerhouse of the cell is the mitochondrion. It generates most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.",
 'followup_question': 'How do mitochondria generate ATP?'}
```


도구 호출은 모델이 구조화된 출력을 생성하도록 하는 일반적으로 일관된 방법이며, 모델이 이를 지원할 때 [`.with_structured_output()`](/docs/concepts/#with_structured_output) 메서드에 사용되는 기본 기술입니다.

구조화된 출력을 위한 함수/도구 호출을 사용하는 데 유용한 실용적인 리소스는 다음과 같은 가이드입니다:

- [LLM에서 구조화된 데이터 반환하는 방법](/docs/how_to/structured_output/)
- [모델을 사용하여 도구 호출하는 방법](/docs/how_to/tool_calling)

도구 호출을 지원하는 모델 제공자의 전체 목록은 [이 표](/docs/integrations/chat/#advanced-features)에서 확인하세요.

### 검색

LLM은 크고 고정된 데이터 세트에서 훈련되므로 개인적이거나 최근의 정보를 추론하는 능력이 제한됩니다. 특정 사실로 LLM을 미세 조정하는 것은 이를 완화하는 한 가지 방법이지만, 종종 [사실 회상에 부적합합니다](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) 및 [비용이 많이 들 수 있습니다](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise). 검색은 주어진 입력에 대한 응답을 개선하기 위해 LLM에 관련 정보를 제공하는 과정입니다. 검색 증강 생성(RAG)은 검색된 정보를 사용하여 LLM 생성을 기반으로 하는 과정입니다.

:::tip

* RAG from Scratch [코드](https://github.com/langchain-ai/rag-from-scratch) 및 [비디오 시리즈](https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&feature=shared)를 확인하세요.
* 검색에 대한 고급 가이드는 [이 RAG 튜토리얼](/docs/tutorials/rag/)을 참조하세요.

:::

RAG는 검색된 문서의 관련성과 품질만큼 좋습니다. 다행히도 RAG 시스템을 설계하고 개선하기 위해 사용할 수 있는 새로운 기술 세트가 등장하고 있습니다. 우리는 이러한 기술을 분류하고 요약하는 데 집중했으며(아래 그림 참조), 다음 섹션에서 몇 가지 고급 전략적 지침을 공유할 것입니다. 다양한 조합을 사용하여 실험할 수 있으며, [이 LangSmith 가이드](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application)가 앱의 다양한 반복을 평가하는 방법을 보여주는 데 유용할 수 있습니다.

![](/img/rag_landscape.png)

#### 쿼리 번역

먼저, RAG 시스템에 대한 사용자 입력을 고려하세요. 이상적으로 RAG 시스템은 잘못된 질문부터 복잡한 다중 쿼리에 이르기까지 다양한 입력을 처리할 수 있어야 합니다. **LLM을 사용하여 입력을 검토하고 선택적으로 수정하는 것이 쿼리 번역의 핵심 아이디어입니다.** 이는 원시 사용자 입력을 검색 시스템에 최적화하는 일반적인 완충 역할을 합니다. 예를 들어, 이는 키워드를 추출하는 것만큼 간단할 수 있으며, 복잡한 쿼리에 대해 여러 하위 질문을 생성하는 것만큼 복잡할 수 있습니다.

| 이름          | 사용 시기 | 설명 |
|---------------|-------------|-------------|
| [다중 쿼리](/docs/how_to/MultiQueryRetriever/)   | 질문의 여러 관점을 다루어야 할 때. | 사용자의 질문을 여러 관점에서 다시 작성하고, 각 다시 작성된 질문에 대해 문서를 검색하고, 모든 쿼리에 대한 고유한 문서를 반환합니다. |
| [분해](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb) | 질문을 더 작은 하위 문제로 나눌 수 있을 때. | 질문을 하위 문제/질문 집합으로 분해하며, 이는 순차적으로 해결하거나(첫 번째 답변 + 검색을 사용하여 두 번째 질문에 답변) 병렬로(각 답변을 최종 답변으로 통합) 해결할 수 있습니다. |
| [스텝백](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | 더 높은 수준의 개념적 이해가 필요할 때. | 먼저 LLM에 더 높은 수준의 개념이나 원칙에 대한 일반적인 스텝백 질문을 하도록 프롬프트하고, 이에 대한 관련 사실을 검색합니다. 이 기반을 사용하여 사용자 질문에 답변합니다. |
| [HyDE](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)          | 원시 사용자 입력을 사용하여 관련 문서를 검색하는 데 어려움이 있을 때. | LLM을 사용하여 질문을 질문에 대한 가상의 문서로 변환합니다. 가상의 문서를 임베딩하여 실제 문서를 검색하는 데 사용하며, 문서 간 유사성 검색이 더 관련성 높은 일치를 생성할 수 있다는 전제를 기반으로 합니다. |

:::tip

RAG from Scratch 비디오에서 몇 가지 특정 접근 방식을 확인하세요:
- [다중 쿼리](https://youtu.be/JChPi0CRnDY?feature=shared)
- [분해](https://youtu.be/h0OPWlEOank?feature=shared)
- [스텝백](https://youtu.be/xn1jEjRyJ2U?feature=shared)
- [HyDE](https://youtu.be/SaDzIVkYqyY?feature=shared)

:::

#### 라우팅

둘째, RAG 시스템에서 사용할 수 있는 데이터 소스를 고려하세요. 하나 이상의 데이터베이스 또는 구조화된 데이터와 비구조화된 데이터 소스 간에 쿼리하려고 합니다. **LLM을 사용하여 입력을 검토하고 적절한 데이터 소스로 라우팅하는 것은 소스 간 쿼리를 위한 간단하고 효과적인 접근 방식입니다.**

| 이름             | 사용 시기                                | 설명 |
|------------------|--------------------------------------------|-------------|
| [논리적 라우팅](/docs/how_to/routing/)  | 입력을 라우팅할 위치를 결정하기 위해 LLM에 규칙으로 프롬프트할 수 있을 때. | 논리적 라우팅은 LLM을 사용하여 쿼리에 대해 추론하고 가장 적합한 데이터 저장소를 선택할 수 있습니다. |
| [의미적 라우팅](/docs/how_to/routing/#routing-by-semantic-similarity) | 의미적 유사성이 입력을 라우팅할 위치를 결정하는 효과적인 방법일 때. | 의미적 라우팅은 쿼리와 일반적으로 일련의 프롬프트를 모두 임베딩합니다. 그런 다음 유사성에 따라 적절한 프롬프트를 선택합니다. |

:::tip

라우팅에 대한 RAG from Scratch 비디오를 확인하세요 [라우팅](https://youtu.be/pfpIndq7Fi8?feature=shared).  

:::
#### 쿼리 구성

셋째, 데이터 소스 중 특정 쿼리 형식을 요구하는 것이 있는지 고려하십시오. 많은 구조화된 데이터베이스는 SQL을 사용합니다. 벡터 저장소는 문서 메타데이터에 키워드 필터를 적용하기 위한 특정 구문을 자주 사용합니다. **자연어 쿼리를 쿼리 구문으로 변환하는 데 LLM을 사용하는 것은 인기 있고 강력한 접근 방식입니다.** 특히, [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), 및 [메타데이터 필터를 위한 쿼리 분석](/docs/tutorials/query_analysis/#query-analysis)은 각각 구조화된 데이터베이스, 그래프 데이터베이스 및 벡터 데이터베이스와 상호작용하는 유용한 방법입니다.

| 이름                                        | 사용 시기                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                      |
|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Text to SQL](/docs/tutorials/sql_qa/)      | 사용자가 SQL을 통해 접근할 수 있는 관계형 데이터베이스에 저장된 정보를 요구하는 질문을 할 때.                                   | 이는 LLM을 사용하여 사용자 입력을 SQL 쿼리로 변환합니다.                                             |
| [Text-to-Cypher](/docs/tutorials/graph/)    | 사용자가 Cypher를 통해 접근할 수 있는 그래프 데이터베이스에 저장된 정보를 요구하는 질문을 할 때.                                     | 이는 LLM을 사용하여 사용자 입력을 Cypher 쿼리로 변환합니다.                                              |
| [Self Query](/docs/how_to/self_query/)      | 사용자가 텍스트와의 유사성보다는 메타데이터를 기반으로 문서를 가져오는 것이 더 잘 답변되는 질문을 할 때.          | 이는 LLM을 사용하여 사용자 입력을 두 가지로 변환합니다: (1) 의미적으로 조회할 문자열, (2) 함께 사용할 메타데이터 필터. 이는 종종 질문이 문서의 메타데이터(내용 자체가 아님)에 관한 것이기 때문에 유용합니다.                                              |

:::tip

우리의 [블로그 게시물 개요](https://blog.langchain.dev/query-construction/)와 [쿼리 구성](https://youtu.be/kl6NwWYxvbM?feature=shared) 비디오를 참조하십시오. 이는 DSL이 주어진 데이터베이스와 상호작용하는 데 필요한 도메인 특화 언어로서 텍스트를 DSL로 변환하는 과정입니다. 이는 사용자 질문을 구조화된 쿼리로 변환합니다.

:::

#### 인덱싱

넷째, 문서 인덱스의 설계를 고려하십시오. 간단하고 강력한 아이디어는 **검색을 위해 인덱싱하는 문서와 LLM에 생성하기 위해 전달하는 문서를 분리하는 것입니다.** 인덱싱은 종종 벡터 저장소와 함께 임베딩 모델을 사용하며, 이는 [문서의 의미 정보를 고정 크기 벡터로 압축합니다](/docs/concepts/#embedding-models).

많은 RAG 접근 방식은 문서를 조각으로 나누고 LLM에 대한 입력 질문과의 유사성을 기반으로 일부를 검색하는 데 중점을 둡니다. 그러나 조각 크기와 조각 수는 설정하기 어려울 수 있으며, LLM이 질문에 답변하기 위한 전체 맥락을 제공하지 않으면 결과에 영향을 미칠 수 있습니다. 더욱이, LLM은 점점 더 수백만 개의 토큰을 처리할 수 있는 능력을 갖추고 있습니다.

이 긴장을 해결할 수 있는 두 가지 접근 방식이 있습니다: (1) [Multi Vector](/docs/how_to/multi_vector/) 검색기는 LLM을 사용하여 문서를 인덱싱에 적합한 형태(예: 종종 요약으로)로 변환하지만, 생성에 대해 전체 문서를 LLM에 반환합니다. (2) [ParentDocument](/docs/how_to/parent_document_retriever/) 검색기는 문서 조각을 임베딩하지만, 전체 문서도 반환합니다. 이 아이디어는 두 가지 장점을 얻는 것입니다: 검색을 위해 간결한 표현(요약 또는 조각)을 사용하지만, 답변 생성을 위해 전체 문서를 사용합니다.

| 이름                      | 인덱스 유형                   | LLM 사용 여부               | 사용 시기                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                      |
|---------------------------|------------------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Vector store](/docs/how_to/vectorstore_retriever/)               | 벡터 저장소                  | 아니오                        | 시작하는 단계에서 빠르고 쉽게 찾고 있는 경우.                                                                     | 이는 가장 간단한 방법이며 시작하기 가장 쉬운 방법입니다. 각 텍스트 조각에 대한 임베딩을 생성하는 것을 포함합니다.                                                                                                                                                             |
| [ParentDocument](/docs/how_to/parent_document_retriever/)            | 벡터 저장소 + 문서 저장소 | 아니오                        | 페이지에 서로 다른 정보 조각이 많고, 이들을 개별적으로 인덱싱하는 것이 최선이지만, 모두 함께 검색하는 것이 최선인 경우.       | 이는 각 문서에 대해 여러 조각을 인덱싱하는 것을 포함합니다. 그런 다음 임베딩 공간에서 가장 유사한 조각을 찾지만, 전체 부모 문서를 검색하여 반환합니다(개별 조각이 아닌).                                                                         |
| [Multi Vector](/docs/how_to/multi_vector/)              | 벡터 저장소 + 문서 저장소 | 때때로 인덱싱 중에 | 문서에서 텍스트 자체보다 인덱싱하는 것이 더 관련성이 있다고 생각되는 정보를 추출할 수 있는 경우.                          | 이는 각 문서에 대해 여러 벡터를 생성하는 것을 포함합니다. 각 벡터는 다양한 방법으로 생성될 수 있습니다 - 예를 들어, 텍스트의 요약 및 가상의 질문 등이 있습니다.                                                                                                                 |
| [Time-Weighted Vector store](/docs/how_to/time_weighted_vectorstore/) | 벡터 저장소                  | 아니오                        | 문서와 관련된 타임스탬프가 있고, 가장 최근의 문서를 검색하고자 하는 경우                                          | 이는 의미적 유사성(일반 벡터 검색과 같이)과 최신성(인덱싱된 문서의 타임스탬프를 살펴보는 것)의 조합을 기반으로 문서를 가져옵니다.                                                                                                                                    |

:::tip

- 우리의 RAG from Scratch 비디오에서 [인덱싱 기초](https://youtu.be/bjb_EMsTDKI?feature=shared)를 참조하십시오.
- 우리의 RAG from Scratch 비디오에서 [다중 벡터 검색기](https://youtu.be/gTCU9I6QqCE?feature=shared)를 참조하십시오.

:::

다섯째, 유사성 검색의 품질을 개선할 방법을 고려하십시오. 임베딩 모델은 텍스트를 고정 길이(벡터) 표현으로 압축하여 문서의 의미 내용을 포착합니다. 이 압축은 검색/검색에 유용하지만, 단일 벡터 표현이 문서의 의미적 뉘앙스/세부 사항을 포착하는 데 큰 부담을 줍니다. 경우에 따라, 관련 없는 또는 중복된 내용이 임베딩의 의미적 유용성을 희석할 수 있습니다.

[ColBERT](https://docs.google.com/presentation/d/1IRhAdGjIevrrotdplHNcc4aXgIYyKamUKTWtB3m3aMU/edit?usp=sharing)는 더 높은 세분화 임베딩으로 이 문제를 해결하는 흥미로운 접근 방식입니다: (1) 문서와 쿼리의 각 토큰에 대해 맥락적으로 영향을 받은 임베딩을 생성하고, (2) 각 쿼리 토큰과 모든 문서 토큰 간의 유사성을 점수화하고, (3) 최대값을 취하고, (4) 모든 쿼리 토큰에 대해 이를 수행하고, (5) 모든 쿼리 토큰에 대해 최대 점수의 합계를 취하여 쿼리-문서 유사성 점수를 얻습니다; 이 토큰 단위 점수화는 강력한 결과를 낼 수 있습니다.

![](/img/colbert.png)

검색 품질을 개선하기 위한 몇 가지 추가 트릭이 있습니다. 임베딩은 의미 정보를 포착하는 데 뛰어나지만, 키워드 기반 쿼리에는 어려움을 겪을 수 있습니다. 많은 [벡터 저장소](/docs/integrations/retrievers/pinecone_hybrid_search/)는 키워드와 의미적 유사성을 결합하는 내장 [하이브리드 검색](https://docs.pinecone.io/guides/data/understanding-hybrid-search)을 제공하여 두 접근 방식의 이점을 결합합니다. 더욱이, 많은 벡터 저장소는 [최대 한계 관련성](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/)을 제공하여 유사하고 중복된 문서를 반환하지 않도록 검색 결과를 다양화하려고 합니다.

| 이름              | 사용 시기                                              | 설명 |
|-------------------|----------------------------------------------------------|-------------|
| [ColBERT](/docs/integrations/providers/ragatouille/#using-colbert-as-a-reranker)           | 더 높은 세분화 임베딩이 필요할 때.           | ColBERT는 문서와 쿼리의 각 토큰에 대해 맥락적으로 영향을 받은 임베딩을 사용하여 세분화된 쿼리-문서 유사성 점수를 얻습니다. |
| [하이브리드 검색](/docs/integrations/retrievers/pinecone_hybrid_search/)     | 키워드 기반 유사성과 의미적 유사성을 결합할 때.    | 하이브리드 검색은 키워드와 의미적 유사성을 결합하여 두 접근 방식의 이점을 결합합니다. |
| [최대 한계 관련성 (MMR)](/docs/integrations/vectorstores/pinecone/#maximal-marginal-relevance-searches) | 검색 결과를 다양화해야 할 때. | MMR은 검색 결과를 다양화하여 유사하고 중복된 문서를 반환하지 않도록 시도합니다. |

:::tip

우리의 RAG from Scratch 비디오에서 [ColBERT](https://youtu.be/cN6S0Ehm7_8?feature=shared)를 참조하십시오.

:::

#### 후처리

여섯째, 검색된 문서를 필터링하거나 순위를 매기는 방법을 고려하십시오. 이는 [여러 소스에서 반환된 문서를 결합하는 경우](/docs/integrations/retrievers/cohere-reranker/#doing-reranking-with-coherererank) 매우 유용합니다. 이는 덜 관련성이 높은 문서의 순위를 낮추거나 [유사한 문서를 압축하는 것](/docs/how_to/contextual_compression/#more-built-in-compressors-filters)을 포함할 수 있습니다.

| 이름                      | 인덱스 유형                   | LLM 사용 여부               | 사용 시기                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                      |
|---------------------------|------------------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Contextual Compression](/docs/how_to/contextual_compression/)    | 모든                          | 때때로                 | 검색된 문서에 너무 많은 관련 없는 정보가 포함되어 LLM을 방해하는 경우.                         | 이는 다른 검색기 위에 후처리 단계를 두고 검색된 문서에서 가장 관련성이 높은 정보만 추출합니다. 이는 임베딩 또는 LLM을 사용하여 수행할 수 있습니다.                                                                                                               |
| [Ensemble](/docs/how_to/ensemble_retriever/)                  | 모든                          | 아니오                        | 여러 검색 방법이 있고 이를 결합해 보려는 경우.                                                                        | 이는 여러 검색기에서 문서를 가져오고 이를 결합합니다.                                                                                                                                                                                                                          |
| [Re-ranking](/docs/integrations/retrievers/cohere-reranker/)                  | 모든                          | 예                        | 특히 여러 검색 방법의 결과를 결합하고자 할 때 검색된 문서의 관련성에 따라 순위를 매기고자 하는 경우.                                                                         | 쿼리와 문서 목록이 주어지면, Rerank는 문서를 쿼리에 대해 가장 의미적으로 관련성이 높은 것부터 가장 낮은 것까지 인덱싱합니다.                                                                                                                                                                                                                         |

:::tip

우리의 RAG from Scratch 비디오에서 [RAG-Fusion](https://youtu.be/77qELPbNgxA?feature=shared)을 참조하십시오. 이는 여러 쿼리에 대한 후처리 접근 방식입니다: 여러 관점에서 사용자 질문을 다시 작성하고, 각 재작성된 질문에 대해 문서를 검색하고, 여러 검색 결과 목록의 순위를 결합하여 [상호 순위 융합 (RRF)](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)로 단일 통합 순위를 생성합니다.

:::

#### 생성

**마지막으로, RAG 시스템에 자기 수정 기능을 구축하는 방법을 고려하십시오.** RAG 시스템은 낮은 품질의 검색(예: 사용자의 질문이 인덱스의 도메인에 맞지 않는 경우) 및/또는 생성에서의 환각으로 고통받을 수 있습니다. 단순한 검색-생성 파이프라인은 이러한 종류의 오류를 감지하거나 자기 수정할 수 있는 능력이 없습니다. ["흐름 엔지니어링"](https://x.com/karpathy/status/1748043513156272416) 개념은 [코드 생성의 맥락에서](https://arxiv.org/abs/2401.08500) 도입되었습니다: 단위 테스트를 통해 코드 질문에 대한 답변을 반복적으로 구축하여 오류를 확인하고 자기 수정합니다. 여러 작업이 이 RAG를 적용했습니다. 예를 들어 Self-RAG 및 Corrective-RAG가 있습니다. 두 경우 모두 RAG 답변 생성 흐름에서 문서 관련성, 환각 및/또는 답변 품질에 대한 검사가 수행됩니다.

우리는 그래프가 논리적 흐름을 신뢰성 있게 표현하는 훌륭한 방법이라고 생각하며, 여러 논문에서 아이디어를 구현했습니다 [LangGraph](https://github.com/langchain-ai/langgraph/tree/main/examples/rag)를 사용하여 아래 그림과 같이 나타냈습니다(빨간색 - 라우팅, 파란색 - 폴백, 초록색 - 자기 수정):
- **라우팅:** 적응형 RAG ([논문](https://arxiv.org/abs/2403.14403)). 위에서 논의한 대로 질문을 서로 다른 검색 접근 방식으로 라우팅합니다.
- **폴백:** 교정 RAG ([논문](https://arxiv.org/pdf/2401.15884.pdf)). 문서가 쿼리와 관련이 없을 경우 웹 검색으로 폴백합니다.
- **자기 수정:** Self-RAG ([논문](https://arxiv.org/abs/2310.11511)). 환각이 있는 답변을 수정하거나 질문을 해결하지 않습니다.

![](/img/langgraph_rag.png)

| 이름              | 사용 시기                                               | 설명 |
|-------------------|-----------------------------------------------------------|-------------|
| Self-RAG          | 환각이나 관련 없는 콘텐츠로 답변을 수정해야 할 때. | Self-RAG는 RAG 답변 생성 흐름에서 문서 관련성, 환각 및 답변 품질을 확인하며, 답변을 반복적으로 구축하고 오류를 자기 수정합니다. |
| Corrective-RAG    | 낮은 관련 문서에 대한 폴백 메커니즘이 필요할 때. | Corrective-RAG는 검색된 문서가 쿼리와 관련이 없는 경우 웹 검색으로 폴백하는 기능을 포함하여 더 높은 품질과 더 관련성 높은 검색을 보장합니다. |

:::tip

LangGraph와 함께 RAG를 보여주는 여러 비디오와 요리책을 참조하십시오:
- [LangGraph 교정 RAG](https://www.youtube.com/watch?v=E2shqsYwxck)
- [적응형, Self-RAG 및 교정 RAG 결합하는 LangGraph](https://www.youtube.com/watch?v=-ROS6gfYIts)
- [LangGraph를 사용한 RAG 요리책](https://github.com/langchain-ai/langgraph/tree/main/examples/rag)

파트너와 함께하는 LangGraph RAG 레시피를 참조하십시오:
- [Meta](https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/langchain)
- [Mistral](https://github.com/mistralai/cookbook/tree/main/third_party/langchain)

:::
### 텍스트 분할

LangChain은 다양한 유형의 `텍스트 분할기`를 제공합니다.  
이 모든 것은 `langchain-text-splitters` 패키지에 포함되어 있습니다.

테이블 열:

- **이름**: 텍스트 분할기의 이름
- **클래스**: 이 텍스트 분할기를 구현하는 클래스
- **분할 기준**: 이 텍스트 분할기가 텍스트를 분할하는 방법
- **메타데이터 추가**: 이 텍스트 분할기가 각 청크가 어디에서 왔는지에 대한 메타데이터를 추가하는지 여부.
- **설명**: 분할기에 대한 설명, 사용 시 권장 사항 포함.

| 이름     | 클래스                                                                                                                                                                                                             | 분할 기준                                                   | 메타데이터 추가 | 설명                                                                                                                                                                                                                                                                  |
|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Recursive | [RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter/), [RecursiveJsonSplitter](/docs/how_to/recursive_json_splitter/) | 사용자가 정의한 문자 목록     |               | 텍스트를 재귀적으로 분할합니다. 이 분할은 관련된 텍스트 조각을 서로 가까이 두려고 합니다. 이는 텍스트 분할을 시작하는 `권장 방법`입니다.                                                                                                                    |
| HTML      | [HTMLHeaderTextSplitter](/docs/how_to/HTML_header_metadata_splitter/), [HTMLSectionSplitter](/docs/how_to/HTML_section_aware_splitter/)          | HTML 특정 문자                                                                                                 | ✅             | HTML 특정 문자를 기반으로 텍스트를 분할합니다. 특히, 이 분할기는 해당 청크가 어디에서 왔는지에 대한 관련 정보를 추가합니다 (HTML 기반).                                                                                                                               |
| Markdown  | [MarkdownHeaderTextSplitter](/docs/how_to/markdown_header_metadata_splitter/),                                                                                                           | Markdown 특정 문자                                                                                    | ✅             | Markdown 특정 문자를 기반으로 텍스트를 분할합니다. 특히, 이 분할기는 해당 청크가 어디에서 왔는지에 대한 관련 정보를 추가합니다 (Markdown 기반).                                                                                                                       |
| Code      | [many languages](/docs/how_to/code_splitter/)                                                                                                                                 | 코드 (Python, JS) 특정 문자                                                                           |               | 코딩 언어에 특정한 문자를 기반으로 텍스트를 분할합니다. 15가지의 다양한 언어를 선택할 수 있습니다.                                                                                                                                                           |
| Token    | [many classes](/docs/how_to/split_by_token/)                                                                                                                                  | 토큰                                                                                                          |               | 토큰을 기준으로 텍스트를 분할합니다. 토큰을 측정하는 몇 가지 방법이 존재합니다.                                                                                                                                                                                                   |
| Character  | [CharacterTextSplitter](/docs/how_to/character_text_splitter/)                                                                                                                | 사용자가 정의한 문자                                                                                        |               | 사용자가 정의한 문자를 기반으로 텍스트를 분할합니다. 가장 간단한 방법 중 하나입니다.                                                                                                                                                                                                   |
| Semantic Chunker (Experimental) | [SemanticChunker](/docs/how_to/semantic-chunker/)                                                                                                                             | 문장                                                                                                       |               | 먼저 문장을 기준으로 분할합니다. 그런 다음 의미적으로 충분히 유사한 문장을 서로 결합합니다. [Greg Kamradt](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)에서 가져옴 |
| Integration: AI21 Semantic | [AI21SemanticTextSplitter](/docs/integrations/document_transformers/ai21_semantic_text_splitter/)                                                                                                 |                  |    ✅           | 일관된 텍스트 조각을 형성하는 독특한 주제를 식별하고 그에 따라 분할합니다.                                                                                                                                                                                         |

### 평가
<span data-heading-keywords="evaluation,evaluate"></span>

평가는 LLM 기반 애플리케이션의 성능과 효과를 평가하는 과정입니다.  
이는 모델의 응답을 미리 정의된 기준이나 벤치마크와 비교하여 원하는 품질 기준을 충족하고 의도된 목적을 이행하는지 확인하는 것을 포함합니다.  
이 과정은 신뢰할 수 있는 애플리케이션을 구축하는 데 필수적입니다.

![](/img/langsmith_evaluate.png)

[LangSmith](https://docs.smith.langchain.com/)는 이 과정을 몇 가지 방법으로 지원합니다:

- 추적 및 주석 기능을 통해 데이터셋을 생성하고 관리하는 것을 쉽게 만듭니다.
- 메트릭을 정의하고 데이터셋에 대해 애플리케이션을 실행하는 데 도움이 되는 평가 프레임워크를 제공합니다.
- 시간에 따른 결과를 추적하고 평가자를 일정에 따라 자동으로 실행하거나 CI/코드의 일부로 실행할 수 있습니다.

자세한 내용을 보려면 [이 LangSmith 가이드](https://docs.smith.langchain.com/concepts/evaluation)를 확인하세요.

### 추적
<span data-heading-keywords="trace,tracing"></span>

추적은 본질적으로 입력에서 출력으로 이동하기 위해 애플리케이션이 수행하는 일련의 단계입니다.  
추적에는 `실행`이라고 불리는 개별 단계가 포함됩니다. 이는 모델, 검색기, 도구 또는 하위 체인에서의 개별 호출일 수 있습니다.  
추적은 체인과 에이전트 내부에서 관찰 가능성을 제공하며, 문제 진단에 필수적입니다.

더 깊이 알아보려면 [이 LangSmith 개념 가이드](https://docs.smith.langchain.com/concepts/tracing)를 확인하세요.