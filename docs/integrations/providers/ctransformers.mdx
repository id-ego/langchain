---
description: 이 문서는 LangChain 내에서 C Transformers 라이브러리를 사용하는 방법을 설치, 설정 및 래퍼에 대한 참조와
  함께 설명합니다.
---

# C Transformers

이 페이지에서는 LangChain 내에서 [C Transformers](https://github.com/marella/ctransformers) 라이브러리를 사용하는 방법을 다룹니다. 설치 및 설정과 특정 C Transformers 래퍼에 대한 참조로 나뉘어 있습니다.

## 설치 및 설정

- `pip install ctransformers`로 Python 패키지를 설치합니다.
- 지원되는 [GGML 모델](https://huggingface.co/TheBloke)을 다운로드합니다 (자세한 내용은 [지원되는 모델](https://github.com/marella/ctransformers#supported-models)을 참조하십시오).

## 래퍼

### LLM

CTransformers LLM 래퍼가 존재하며, 다음과 같이 접근할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "CTransformers", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ctransformers.CTransformers.html", "title": "C Transformers"}]-->
from langchain_community.llms import CTransformers
```


모든 모델에 대한 통합 인터페이스를 제공합니다:

```python
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2')

print(llm.invoke('AI is going to'))
```


`illegal instruction` 오류가 발생하는 경우, `lib='avx'` 또는 `lib='basic'`를 사용해 보십시오:

```py
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx')
```


Hugging Face Hub에 호스팅된 모델과 함께 사용할 수 있습니다:

```py
llm = CTransformers(model='marella/gpt-2-ggml')
```


모델 리포지토리에 여러 모델 파일(`.bin` 파일)이 있는 경우, 다음을 사용하여 모델 파일을 지정하십시오:

```py
llm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin')
```


추가 매개변수는 `config` 매개변수를 사용하여 전달할 수 있습니다:

```py
config = {'max_new_tokens': 256, 'repetition_penalty': 1.1}

llm = CTransformers(model='marella/gpt-2-ggml', config=config)
```


사용 가능한 매개변수 목록은 [문서](https://github.com/marella/ctransformers#config)를 참조하십시오.

자세한 안내는 [이 노트북](/docs/integrations/llms/ctransformers)을 참조하십시오.