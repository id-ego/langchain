---
description: TruLens는 대규모 언어 모델(LLM) 애플리케이션을 위한 도구와 평가 기능을 제공하는 오픈소스 패키지입니다.
---

# TruLens

> [TruLens](https://trulens.org)는 대형 언어 모델(LLM) 기반 애플리케이션을 위한 도구와 평가 도구를 제공하는 [오픈 소스](https://github.com/truera/trulens) 패키지입니다.

이 페이지에서는 langchain을 기반으로 구축된 LLM 앱을 평가하고 추적하기 위해 [TruLens](https://trulens.org)를 사용하는 방법을 다룹니다.

## 설치 및 설정

`trulens-eval` 파이썬 패키지를 설치합니다.

```bash
pip install trulens-eval
```


## 빠른 시작

[TruLens 문서](https://www.trulens.org/trulens_eval/getting_started/quickstarts/langchain_quickstart/)에서 통합 세부정보를 확인하세요.

### 추적

LLM 체인을 생성한 후, TruLens를 사용하여 평가 및 추적을 할 수 있습니다.  
TruLens는 여러 가지 [즉시 사용 가능한 피드백 기능](https://www.trulens.org/trulens_eval/evaluation/feedback_functions/)을 제공하며, LLM 평가를 위한 확장 가능한 프레임워크이기도 합니다.

피드백 기능을 생성합니다:

```python
from trulens_eval.feedback import Feedback, Huggingface, 

# Initialize HuggingFace-based feedback function collection class:
hugs = Huggingface()
openai = OpenAI()

# Define a language match feedback function using HuggingFace.
lang_match = Feedback(hugs.language_match).on_input_output()
# By default this will check language match on the main app input and main app
# output.

# Question/answer relevance between overall question and answer.
qa_relevance = Feedback(openai.relevance).on_input_output()
# By default this will evaluate feedback on main app input and main app output.

# Toxicity of input
toxicity = Feedback(openai.toxicity).on_input()
```


### 체인

LLM을 평가하기 위한 피드백 기능을 설정한 후, TruChain으로 애플리케이션을 감싸서 LLM 앱의 자세한 추적, 로깅 및 평가를 받을 수 있습니다.

참고: `chain` 생성에 대한 코드는 [TruLens 문서](https://www.trulens.org/trulens_eval/getting_started/quickstarts/langchain_quickstart/)에서 확인하세요.

```python
from trulens_eval import TruChain

# wrap your chain with TruChain
truchain = TruChain(
    chain,
    app_id='Chain1_ChatApplication',
    feedbacks=[lang_match, qa_relevance, toxicity]
)
# Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.
truchain("que hora es?")
```


### 평가

이제 LLM 기반 애플리케이션을 탐색할 수 있습니다!

이렇게 하면 LLM 애플리케이션의 성능을 한눈에 이해하는 데 도움이 됩니다. LLM 애플리케이션의 새로운 버전을 반복할 때 설정한 다양한 품질 메트릭에 따라 성능을 비교할 수 있습니다. 또한 각 레코드에 대한 평가를 기록 수준에서 보고, 각 레코드의 체인 메타데이터를 탐색할 수 있습니다.

```python
from trulens_eval import Tru

tru = Tru()
tru.run_dashboard() # open a Streamlit app to explore
```


TruLens에 대한 자세한 정보는 [trulens.org](https://www.trulens.org/)를 방문하세요.