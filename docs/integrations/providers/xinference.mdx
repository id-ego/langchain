---
description: 이 문서는 LangChain과 함께 Xorbits Inference(Xinference)를 사용하는 방법을 설명하며, 다양한
  모델을 쉽게 배포하고 사용할 수 있습니다.
---

# Xorbits Inference (Xinference)

이 페이지는 LangChain과 함께 [Xinference](https://github.com/xorbitsai/inference)를 사용하는 방법을 보여줍니다.

`Xinference`는 LLM, 음성 인식 모델 및 다중 모달 모델을 노트북에서도 제공할 수 있도록 설계된 강력하고 다재다능한 라이브러리입니다. Xorbits Inference를 사용하면 단일 명령어로 최첨단 내장 모델을 쉽게 배포하고 제공할 수 있습니다.

## 설치 및 설정

Xinference는 PyPI에서 pip를 통해 설치할 수 있습니다:

```bash
pip install "xinference[all]"
```


## LLM

Xinference는 chatglm, baichuan, whisper, vicuna 및 orca를 포함하여 GGML과 호환되는 다양한 모델을 지원합니다. 내장 모델을 보려면 다음 명령어를 실행하십시오:

```bash
xinference list --all
```


### Xinference 래퍼

다음 명령어를 실행하여 Xinference의 로컬 인스턴스를 시작할 수 있습니다:

```bash
xinference
```


또한 분산 클러스터에서 Xinference를 배포할 수 있습니다. 그렇게 하려면, 먼저 실행할 서버에서 Xinference 감독자를 시작하십시오:

```bash
xinference-supervisor -H "${supervisor_host}"
```


그런 다음, 실행할 다른 각 서버에서 Xinference 작업자를 시작하십시오:

```bash
xinference-worker -e "http://${supervisor_host}:9997"
```


다음 명령어를 실행하여 Xinference의 로컬 인스턴스를 시작할 수도 있습니다:

```bash
xinference
```


Xinference가 실행되면 CLI 또는 Xinference 클라이언트를 통해 모델 관리를 위한 엔드포인트에 접근할 수 있습니다.

로컬 배포의 경우, 엔드포인트는 http://localhost:9997입니다.

클러스터 배포의 경우, 엔드포인트는 http://${supervisor_host}:9997입니다.

그런 다음 모델을 시작해야 합니다. 모델 이름 및 model_size_in_billions 및 quantization을 포함한 기타 속성을 지정할 수 있습니다. 이를 위해 명령줄 인터페이스(CLI)를 사용할 수 있습니다. 예를 들어,

```bash
xinference launch -n orca -s 3 -q q4_0
```


모델 uid가 반환됩니다.

사용 예:

```python
<!--IMPORTS:[{"imported": "Xinference", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.xinference.Xinference.html", "title": "Xorbits Inference (Xinference)"}]-->
from langchain_community.llms import Xinference

llm = Xinference(
    server_url="http://0.0.0.0:9997",
    model_uid = {model_uid} # replace model_uid with the model UID return from launching the model
)

llm(
    prompt="Q: where can we visit in the capital of France? A:",
    generate_config={"max_tokens": 1024, "stream": True},
)

```


### 사용법

자세한 정보와 예제는 [xinference LLMs에 대한 예제](/docs/integrations/llms/xinference)를 참조하십시오.

### 임베딩

Xinference는 쿼리 및 문서 임베딩도 지원합니다. 더 자세한 데모는 [xinference 임베딩에 대한 예제](/docs/integrations/text_embedding/xinference)를 참조하십시오.