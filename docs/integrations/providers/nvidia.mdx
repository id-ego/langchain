---
description: NVIDIA NIM 추론 마이크로서비스를 활용한 LangChain 통합 애플리케이션 구축을 위한 패키지입니다. 다양한 모델을
  지원합니다.
---

# NVIDIA
`langchain-nvidia-ai-endpoints` 패키지는 NVIDIA NIM 추론 마이크로서비스에서 모델로 애플리케이션을 구축하는 LangChain 통합을 포함합니다. NIM은 커뮤니티와 NVIDIA의 채팅, 임베딩 및 재순위 모델과 같은 다양한 도메인에서 모델을 지원합니다. 이러한 모델은 NVIDIA에 의해 최적화되어 NVIDIA 가속 인프라에서 최고의 성능을 제공하며, 단일 명령으로 어디서나 배포할 수 있는 사용하기 쉬운 미리 구축된 컨테이너인 NIM으로 배포됩니다.

NVIDIA가 호스팅하는 NIM의 배포는 [NVIDIA API 카탈로그](https://build.nvidia.com/)에서 테스트할 수 있습니다. 테스트 후, NIM은 NVIDIA AI Enterprise 라이센스를 사용하여 NVIDIA의 API 카탈로그에서 내보낼 수 있으며, 온프레미스 또는 클라우드에서 실행할 수 있어 기업이 자신의 IP 및 AI 애플리케이션에 대한 소유권과 완전한 제어를 가질 수 있습니다.

NIM은 모델별로 컨테이너 이미지로 패키징되며 NVIDIA NGC 카탈로그를 통해 NGC 컨테이너 이미지로 배포됩니다. NIM의 핵심은 AI 모델에 대한 추론을 실행하기 위한 쉽고 일관되며 친숙한 API를 제공하는 것입니다.

아래는 텍스트 생성 및 임베딩 모델과 관련된 일반적인 기능을 사용하는 방법의 예입니다.

## 설치

```python
pip install -U --quiet langchain-nvidia-ai-endpoints
```


## 설정

**시작하려면:**

1. NVIDIA AI Foundation 모델을 호스팅하는 [NVIDIA](https://build.nvidia.com/)에서 무료 계정을 생성합니다.
2. 원하는 모델을 클릭합니다.
3. 입력에서 Python 탭을 선택하고 `Get API Key`를 클릭합니다. 그런 다음 `Generate Key`를 클릭합니다.
4. 생성된 키를 NVIDIA_API_KEY로 복사하여 저장합니다. 그 후, 엔드포인트에 접근할 수 있어야 합니다.

```python
import getpass
import os

if not os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
    nvidia_api_key = getpass.getpass("Enter your NVIDIA API key: ")
    assert nvidia_api_key.startswith("nvapi-"), f"{nvidia_api_key[:5]}... is not a valid key"
    os.environ["NVIDIA_API_KEY"] = nvidia_api_key
```

## NVIDIA API 카탈로그 작업하기

```python
<!--IMPORTS:[{"imported": "ChatNVIDIA", "source": "langchain_nvidia_ai_endpoints", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html", "title": "NVIDIA"}]-->
from langchain_nvidia_ai_endpoints import ChatNVIDIA

llm = ChatNVIDIA(model="mistralai/mixtral-8x22b-instruct-v0.1")
result = llm.invoke("Write a ballad about LangChain.")
print(result.content)
```


API를 사용하여 NVIDIA API 카탈로그에서 사용할 수 있는 실시간 엔드포인트를 쿼리하여 DGX 호스팅 클라우드 컴퓨팅 환경에서 빠른 결과를 얻을 수 있습니다. 모든 모델은 소스 접근이 가능하며 NVIDIA AI Enterprise의 일부인 NVIDIA NIM을 사용하여 자신의 컴퓨팅 클러스터에 배포할 수 있습니다. 다음 섹션 [NVIDIA NIM 작업하기](##working-with-nvidia-nims)에서 설명합니다.

## NVIDIA NIM 작업하기
배포할 준비가 되면 NVIDIA NIM으로 모델을 자체 호스팅할 수 있으며, 이는 NVIDIA AI Enterprise 소프트웨어 라이센스에 포함되어 있으며, 어디서나 실행할 수 있어 사용자 정의에 대한 소유권과 지적 재산(IP) 및 AI 애플리케이션에 대한 완전한 제어를 제공합니다.

[NIM에 대해 더 알아보기](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)

```python
<!--IMPORTS:[{"imported": "ChatNVIDIA", "source": "langchain_nvidia_ai_endpoints", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html", "title": "NVIDIA"}, {"imported": "NVIDIAEmbeddings", "source": "langchain_nvidia_ai_endpoints", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_nvidia_ai_endpoints.embeddings.NVIDIAEmbeddings.html", "title": "NVIDIA"}, {"imported": "NVIDIARerank", "source": "langchain_nvidia_ai_endpoints", "docs": "https://api.python.langchain.com/en/latest/reranking/langchain_nvidia_ai_endpoints.reranking.NVIDIARerank.html", "title": "NVIDIA"}]-->
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank

# connect to a chat NIM running at localhost:8000, specifying a model
llm = ChatNVIDIA(base_url="http://localhost:8000/v1", model="meta/llama3-8b-instruct")

# connect to an embedding NIM running at localhost:8080
embedder = NVIDIAEmbeddings(base_url="http://localhost:8080/v1")

# connect to a reranking NIM running at localhost:2016
ranker = NVIDIARerank(base_url="http://localhost:2016/v1")
```


## NVIDIA AI Foundation 엔드포인트 사용하기

NVIDIA AI Foundation 모델의 선택이 LangChain에서 친숙한 API로 직접 지원됩니다.

지원되는 활성 모델은 [API 카탈로그](https://build.nvidia.com/)에서 찾을 수 있습니다.

**다음은 시작하는 데 유용할 수 있는 예입니다:**
- **[`ChatNVIDIA` 모델](/docs/integrations/chat/nvidia_ai_endpoints).**
- **[`NVIDIAEmbeddings` 모델 RAG 워크플로우용](/docs/integrations/text_embedding/nvidia_ai_endpoints).**