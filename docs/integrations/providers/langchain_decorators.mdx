---
description: LangChain ë°ì½”ë ˆì´í„°ëŠ” ì‚¬ìš©ì ì •ì˜ LangChain í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ì„ ì‘ì„±í•˜ê¸° ìœ„í•œ íŒŒì´ì¬ ìŠ¤íƒ€ì¼ì˜ ê°„í¸í•œ ë°©ë²•ì„
  ì œê³µí•©ë‹ˆë‹¤.
---

# LangChain Decorators âœ¨

```
Disclaimer: `LangChain decorators` is not created by the LangChain team and is not supported by it.
```


> `LangChain decorators`ëŠ” ì‚¬ìš©ì ì •ì˜ langchain í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ì„ ì‘ì„±í•˜ê¸° ìœ„í•œ êµ¬ë¬¸ ì„¤íƒ• ğŸ­ì„ ì œê³µí•˜ëŠ” LangChain ìœ„ì˜ ë ˆì´ì–´ì…ë‹ˆë‹¤.
> 
> í”¼ë“œë°±, ë¬¸ì œ, ê¸°ì—¬ì— ëŒ€í•´ì„œëŠ” ì—¬ê¸°ì— ë¬¸ì œë¥¼ ì œê¸°í•´ ì£¼ì„¸ìš”:
[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)

ì£¼ìš” ì›ì¹™ ë° ì´ì :

- ë” `íŒŒì´ì¬ìŠ¤ëŸ¬ìš´` ì½”ë“œ ì‘ì„± ë°©ì‹
- ì½”ë“œ íë¦„ì„ ê¹¨ì§€ ì•Šë„ë¡ ë“¤ì—¬ì“°ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹°ë¼ì¸ í”„ë¡¬í”„íŠ¸ ì‘ì„±
- **íŒíŠ¸**, **íƒ€ì… ì²´í¬** ë° **ë¬¸ì„œ íŒì—…**ì— ëŒ€í•œ IDE ë‚´ì¥ ì§€ì› í™œìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸, ì†Œëª¨í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ ë“±ì„ ë¹ ë¥´ê²Œ í™•ì¸
- ğŸ¦œğŸ”— LangChain ìƒíƒœê³„ì˜ ëª¨ë“  í˜ í™œìš©
- **ì„ íƒì  ë§¤ê°œë³€ìˆ˜** ì§€ì› ì¶”ê°€
- í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°”ì¸ë”©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°„ì— ì‰½ê²Œ ê³µìœ 

ë‹¤ìŒì€ **LangChain Decorators âœ¨**ë¡œ ì‘ì„±ëœ ê°„ë‹¨í•œ ì½”ë“œ ì˜ˆì œì…ë‹ˆë‹¤.

```python

@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:
    """
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    return

# run it naturally
write_me_short_post(topic="starwars")
# or
write_me_short_post(topic="starwars", platform="redit")
```


# ë¹ ë¥¸ ì‹œì‘
## ì„¤ì¹˜
```bash
pip install langchain_decorators
```


## ì˜ˆì œ

ì‹œì‘í•˜ëŠ” ì¢‹ì€ ë°©ë²•ì€ ì—¬ê¸°ì˜ ì˜ˆì œë¥¼ ê²€í† í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:
- [ì£¼í”¼í„° ë…¸íŠ¸ë¶](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)
- [ì½œë© ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)

# ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ ì •ì˜
ì—¬ê¸°ì„œëŠ” `llm_prompt` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ í‘œì‹œí•˜ê³ , ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ LLMChainìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤. ì‹¤í–‰í•˜ëŠ” ëŒ€ì‹ 

í‘œì¤€ LLMchainì€ ì…ë ¥ ë³€ìˆ˜ì™€ í”„ë¡¬í”„íŠ¸ ì™¸ì— í›¨ì”¬ ë” ë§ì€ ì´ˆê¸° ë§¤ê°œë³€ìˆ˜ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤... ì—¬ê¸°ì„œëŠ” ì´ êµ¬í˜„ ì„¸ë¶€ì •ë³´ê°€ ë°ì½”ë ˆì´í„°ì— ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤.
ì‘ë™ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ì „ì—­ ì„¤ì •** ì‚¬ìš©:

```python
# define global settings for all prompty (if not set - chatGPT is the current default)
from langchain_decorators import GlobalSettings

GlobalSettings.define_settings(
    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally
    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming
)
```


2. ë¯¸ë¦¬ ì •ì˜ëœ **í”„ë¡¬í”„íŠ¸ ìœ í˜•** ì‚¬ìš©

```python
#You can change the default prompt types
from langchain_decorators import PromptTypes, PromptTypeSettings

PromptTypes.AGENT_REASONING.llm = ChatOpenAI()

# Or you can just define your own ones:
class MyCustomPromptTypes(PromptTypes):
    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))

@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) 
def write_a_complicated_code(app_idea:str)->str:
    ...

```


3. **ë°ì½”ë ˆì´í„°**ì—ì„œ ì§ì ‘ ì„¤ì • ì •ì˜

```python
<!--IMPORTS:[{"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "LangChain Decorators \u2728"}]-->
from langchain_openai import OpenAI

@llm_prompt(
    llm=OpenAI(temperature=0.7),
    stop_tokens=["\nObservation"],
    ...
    )
def creative_writer(book_title:str)->str:
    ...
```


## ë©”ëª¨ë¦¬ ë°/ë˜ëŠ” ì½œë°± ì „ë‹¬:

ì´ë“¤ ì¤‘ ì–´ë–¤ ê²ƒì´ë“  ì „ë‹¬í•˜ë ¤ë©´ í•¨ìˆ˜ì—ì„œ ì„ ì–¸í•˜ê±°ë‚˜ kwargsë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.

```python

@llm_prompt()
async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):
    """
    {history_key}
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass

await write_me_short_post(topic="old movies")

```


# ë‹¨ìˆœí™”ëœ ìŠ¤íŠ¸ë¦¬ë°

ìŠ¤íŠ¸ë¦¬ë°ì„ í™œìš©í•˜ë ¤ë©´:
- í”„ë¡¬í”„íŠ¸ë¥¼ ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
- ë°ì½”ë ˆì´í„°ì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ì„ ì¼œê±°ë‚˜, ìŠ¤íŠ¸ë¦¬ë°ì´ ìˆëŠ” PromptTypeì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- StreamingContextë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ì„ ìº¡ì²˜í•©ë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ë©´ ì–´ë–¤ LLMì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì¡°ì •í•  í•„ìš” ì—†ì´ ì–´ë–¤ í”„ë¡¬í”„íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•´ì•¼ í•˜ëŠ”ì§€ë§Œ í‘œì‹œí•˜ë©´ ë©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸/í”„ë¡¬í”„íŠ¸ ìœ í˜•ì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ì„ ì¼œê³  ë„ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤...

ìŠ¤íŠ¸ë¦¬ë°ì€ ìŠ¤íŠ¸ë¦¬ë° ì»¨í…ìŠ¤íŠ¸ì—ì„œ í˜¸ì¶œí•  ë•Œë§Œ ë°œìƒí•©ë‹ˆë‹¤... ì—¬ê¸°ì„œ ìŠ¤íŠ¸ë¦¼ì„ ì²˜ë¦¬í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# this code example is complete and should run as it is

from langchain_decorators import StreamingContext, llm_prompt

# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)
# note that only async functions can be streamed (will get an error if it's not)
@llm_prompt(capture_stream=True) 
async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass



# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real world
tokens=[]
def capture_stream_func(new_token:str):
    tokens.append(new_token)

# if we want to capture the stream, we need to wrap the execution into StreamingContext... 
# this will allow us to capture the stream even if the prompt call is hidden inside higher level method
# only the prompts marked with capture_stream will be captured here
with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):
    result = await run_prompt()
    print("Stream finished ... we can distinguish tokens thanks to alternating colors")


print("\nWe've captured",len(tokens),"tokensğŸ‰\n")
print("Here is the result:")
print(result)
```


# í”„ë¡¬í”„íŠ¸ ì„ ì–¸
ê¸°ë³¸ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ëŠ” ì „ì²´ í•¨ìˆ˜ ë¬¸ì„œì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ í‘œì‹œí•˜ì§€ ì•ŠëŠ” í•œ

## í”„ë¡¬í”„íŠ¸ ë¬¸ì„œí™”

í”„ë¡¬í”„íŠ¸ ì •ì˜ì˜ ë¬¸ì„œ ë¶€ë¶„ì„ ì§€ì •í•  ìˆ˜ ìˆìœ¼ë©°, `<prompt>` ì–¸ì–´ íƒœê·¸ê°€ ìˆëŠ” ì½”ë“œ ë¸”ë¡ì„ ì§€ì •í•˜ì—¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.

    It needs to be a code block, marked as a `<prompt>` language
    ```<prompt>
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    return 
```


## ì±„íŒ… ë©”ì‹œì§€ í”„ë¡¬í”„íŠ¸

ì±„íŒ… ëª¨ë¸ì˜ ê²½ìš° ë©”ì‹œì§€ í…œí”Œë¦¿ ì§‘í•©ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒì´ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤... ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```python
@llm_prompt
def simulate_conversation(human_input:str, agent_role:str="a pirate"):
    """
    ## System message
     - note the `:system` suffix inside the <prompt:_role_> tag
     

    ```<prompt:system>
    You are a {agent_role} hacker. You mus act like one.
    You reply always in code, using python or javascript code block...
    for example:
    
    ... do not reply with anything else.. just with code - respecting your role.
    ```

    # human message 
    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)
    ``` <prompt:user>
    Helo, who are you
    ```
    a reply:
    

    ``` <prompt:assistant>
    \``` python <<- escaping inner code block with \ that should be part of the prompt
    def hello():
        print("Argh... hello you pesky pirate")
    \```
    ```
    
    we can also add some history using placeholder
    ```<prompt:placeholder>
    {history}
    ```
    ```<prompt:user>
    {human_input}
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    pass

```


ì—¬ê¸°ì„œ ì—­í• ì€ ëª¨ë¸ ê³ ìœ ì˜ ì—­í• (assistant, user, system for chatGPT)ì…ë‹ˆë‹¤.

# ì„ íƒì  ì„¹ì…˜
- ì„ íƒì ì´ì–´ì•¼ í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì˜ ì „ì²´ ì„¹ì…˜ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„¹ì…˜ì˜ ì…ë ¥ì´ ëˆ„ë½ë˜ë©´ ì „ì²´ ì„¹ì…˜ì´ ë Œë”ë§ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

êµ¬ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```python
@llm_prompt
def prompt_with_optional_partials():
    """
    this text will be rendered always, but

    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}

    you can also place it in between the words
    this too will be rendered{? , but
        this  block will be rendered only if {this_value} and {this_value}
        is not empty?} !
    """
```


# ì¶œë ¥ íŒŒì„œ

- llm_prompt ë°ì½”ë ˆì´í„°ëŠ” ì¶œë ¥ ìœ í˜•ì— ë”°ë¼ ìµœìƒì˜ ì¶œë ¥ íŒŒì„œë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ë ¤ê³  í•©ë‹ˆë‹¤. (ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì›ì‹œ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.)
- ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬ ë° pydantic ì¶œë ¥ë„ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›ë©ë‹ˆë‹¤ (ìë™ìœ¼ë¡œ).

```python
# this code example is complete and should run as it is

from langchain_decorators import llm_prompt

@llm_prompt
def write_name_suggestions(company_business:str, count:int)->list:
    """ Write me {count} good name suggestions for company that {company_business}
    """
    pass

write_name_suggestions(company_business="sells cookies", count=5)
```


## ë” ë³µì¡í•œ êµ¬ì¡°

ë”•ì…”ë„ˆë¦¬ / pydanticì˜ ê²½ìš° í˜•ì‹ ì§€ì¹¨ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤...
ì´ê²ƒì€ ë²ˆê±°ë¡œìš¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¶œë ¥ íŒŒì„œê°€ ëª¨ë¸(pydantic)ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì¹¨ì„ ìƒì„±í•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from langchain_decorators import llm_prompt
from pydantic import BaseModel, Field


class TheOutputStructureWeExpect(BaseModel):
    name:str = Field (description="The name of the company")
    headline:str = Field( description="The description of the company (for landing page)")
    employees:list[str] = Field(description="5-8 fake employee names with their positions")

@llm_prompt()
def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:
    """ Generate a fake company that {company_business}
    {FORMAT_INSTRUCTIONS}
    """
    return

company = fake_company_generator(company_business="sells cookies")

# print the result nicely formatted
print("Company name: ",company.name)
print("company headline: ",company.headline)
print("company employees: ",company.employees)

```


# í”„ë¡¬í”„íŠ¸ë¥¼ ê°ì²´ì— ë°”ì¸ë”©

```python
from pydantic import BaseModel
from langchain_decorators import llm_prompt

class AssistantPersonality(BaseModel):
    assistant_name:str
    assistant_role:str
    field:str

    @property
    def a_property(self):
        return "whatever"

    def hello_world(self, function_kwarg:str=None):
        """
        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method
        """

    
    @llm_prompt
    def introduce_your_self(self)->str:
        """
        ```Â <prompt:system>
        You are an assistant named {assistant_name}. 
        Your role is to act as {assistant_role}
        ```
        ```<prompt:user>
        Introduce your self (in less than 20 words)
        ```
        """

    

personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")

print(personality.introduce_your_self(personality))
```


# ë” ë§ì€ ì˜ˆì œ:

- ì´ëŸ¬í•œ ì˜ˆì œì™€ ëª‡ ê°€ì§€ ë” ë§ì€ ì˜ˆì œëŠ” [ì½œë© ë…¸íŠ¸ë¶ ì—¬ê¸°](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)ì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ìˆœìˆ˜í•˜ê²Œ langchain ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•œ [ReAct Agent ì¬êµ¬í˜„](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp)ë„ í¬í•¨ë©ë‹ˆë‹¤.