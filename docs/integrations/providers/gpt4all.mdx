---
description: 이 문서는 LangChain 내에서 `GPT4All` 래퍼를 사용하는 방법을 설치, 설정 및 예제를 통해 안내합니다.
---

# GPT4All

이 페이지는 LangChain 내에서 `GPT4All` 래퍼를 사용하는 방법을 다룹니다. 튜토리얼은 설치 및 설정, 사용 예제로 나뉘어 있습니다.

## 설치 및 설정

- `pip install gpt4all`로 Python 패키지를 설치합니다.
- 원하는 디렉토리에 [GPT4All 모델](https://gpt4all.io/index.html)을 다운로드하여 배치합니다.

이 예제에서는 `mistral-7b-openorca.Q4_0.gguf`를 사용하고 있습니다:

```bash
mkdir models
wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf
```


## 사용법

### GPT4All

GPT4All 래퍼를 사용하려면 사전 훈련된 모델 파일의 경로와 모델의 구성을 제공해야 합니다.

```python
<!--IMPORTS:[{"imported": "GPT4All", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.gpt4all.GPT4All.html", "title": "GPT4All"}]-->
from langchain_community.llms import GPT4All

# Instantiate the model. Callbacks support token-wise streaming
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text
response = model.invoke("Once upon a time, ")
```


생성 매개변수(`n_predict`, `temp`, `top_p`, `top_k` 등)를 사용자 정의할 수도 있습니다.

모델의 예측을 스트리밍하려면 CallbackManager를 추가하세요.

```python
<!--IMPORTS:[{"imported": "GPT4All", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.gpt4all.GPT4All.html", "title": "GPT4All"}, {"imported": "StreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html", "title": "GPT4All"}, {"imported": "StreamlitCallbackHandler", "source": "langchain.callbacks.streamlit", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streamlit.StreamlitCallbackHandler.html", "title": "GPT4All"}]-->
from langchain_community.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# There are many CallbackHandlers supported, such as
# from langchain.callbacks.streamlit import StreamlitCallbackHandler

callbacks = [StreamingStdOutCallbackHandler()]
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text. Tokens are streamed through the callback manager.
model.invoke("Once upon a time, ", callbacks=callbacks)
```


## 모델 파일

GPT4All 클라이언트에서 모델 파일을 다운로드할 수 있습니다. [GPT4All](https://gpt4all.io/index.html) 웹사이트에서 클라이언트를 다운로드할 수 있습니다.

자세한 안내는 [이 노트북](/docs/integrations/llms/gpt4all)을 참조하세요.