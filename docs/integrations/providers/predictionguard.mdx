---
description: ì´ ë¬¸ì„œëŠ” LangChain ë‚´ Prediction Guard ìƒíƒœê³„ ì‚¬ìš© ë°©ë²•ì„ ë‹¤ë£¨ë©°, ì„¤ì¹˜, ì„¤ì • ë° LLM ë˜í¼ì— ëŒ€í•œ
  ì°¸ì¡°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
---

# ì˜ˆì¸¡ ê°€ë“œ

ì´ í˜ì´ì§€ì—ì„œëŠ” LangChain ë‚´ì—ì„œ ì˜ˆì¸¡ ê°€ë“œ ìƒíƒœê³„ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì„¤ì¹˜ ë° ì„¤ì •ê³¼ íŠ¹ì • ì˜ˆì¸¡ ê°€ë“œ ë˜í¼ì— ëŒ€í•œ ì°¸ì¡°ë¡œ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

## ì„¤ì¹˜ ë° ì„¤ì •
- `pip install predictionguard`ë¡œ Python SDKë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
- ì˜ˆì¸¡ ê°€ë“œ ì•¡ì„¸ìŠ¤ í† í°ì„ ì–»ê³ (ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://docs.predictionguard.com/) ì°¸ì¡°) ì´ë¥¼ í™˜ê²½ ë³€ìˆ˜(`PREDICTIONGUARD_TOKEN`)ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

## LLM ë˜í¼

ì˜ˆì¸¡ ê°€ë“œ LLM ë˜í¼ê°€ ì¡´ì¬í•˜ë©°, ë‹¤ìŒì„ í†µí•´ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```python
<!--IMPORTS:[{"imported": "PredictionGuard", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.predictionguard.PredictionGuard.html", "title": "Prediction Guard"}]-->
from langchain_community.llms import PredictionGuard
```


LLMì„ ì´ˆê¸°í™”í•  ë•Œ ì˜ˆì¸¡ ê°€ë“œ ëª¨ë¸ì˜ ì´ë¦„ì„ ì¸ìˆ˜ë¡œ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```python
pgllm = PredictionGuard(model="MPT-7B-Instruct")
```


ì•¡ì„¸ìŠ¤ í† í°ì„ ì§ì ‘ ì¸ìˆ˜ë¡œ ì œê³µí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:
```python
pgllm = PredictionGuard(model="MPT-7B-Instruct", token="<your access token>")
```


ë§ˆì§€ë§‰ìœ¼ë¡œ, LLMì˜ ì¶œë ¥ì„ êµ¬ì¡°í™”/ì œì–´í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” "output" ì¸ìˆ˜ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```python
pgllm = PredictionGuard(model="MPT-7B-Instruct", output={"type": "boolean"})
```


## ì‚¬ìš© ì˜ˆì‹œ

ì œì–´ë˜ê±°ë‚˜ ë³´í˜¸ëœ LLM ë˜í¼ì˜ ê¸°ë³¸ ì‚¬ìš©ë²•:
```python
<!--IMPORTS:[{"imported": "PredictionGuard", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.predictionguard.PredictionGuard.html", "title": "Prediction Guard"}, {"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Prediction Guard"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Prediction Guard"}]-->
import os

import predictionguard as pg
from langchain_community.llms import PredictionGuard
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

# Your Prediction Guard API key. Get one at predictionguard.com
os.environ["PREDICTIONGUARD_TOKEN"] = "<your Prediction Guard access token>"

# Define a prompt template
template = """Respond to the following query based on the context.

Context: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! ğŸ‰ We have officially added TWO new candle subscription box options! ğŸ“¦
Exclusive Candle Box - $80 
Monthly Candle Box - $45 (NEW!)
Scent of The Month Box - $28 (NEW!)
Head to stories to get ALL the deets on each box! ğŸ‘† BONUS: Save 50% on your first box with code 50OFF! ğŸ‰

Query: {query}

Result: """
prompt = PromptTemplate.from_template(template)

# With "guarding" or controlling the output of the LLM. See the 
# Prediction Guard docs (https://docs.predictionguard.com) to learn how to 
# control the output with integer, float, boolean, JSON, and other types and
# structures.
pgllm = PredictionGuard(model="MPT-7B-Instruct", 
                        output={
                                "type": "categorical",
                                "categories": [
                                    "product announcement", 
                                    "apology", 
                                    "relational"
                                    ]
                                })
pgllm(prompt.format(query="What kind of post is this?"))
```


ì˜ˆì¸¡ ê°€ë“œ ë˜í¼ë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ LLM ì²´ì´ë‹:
```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Prediction Guard"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Prediction Guard"}, {"imported": "PredictionGuard", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.predictionguard.PredictionGuard.html", "title": "Prediction Guard"}]-->
import os

from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import PredictionGuard

# Optional, add your OpenAI API Key. This is optional, as Prediction Guard allows
# you to access all the latest open access models (see https://docs.predictionguard.com)
os.environ["OPENAI_API_KEY"] = "<your OpenAI api key>"

# Your Prediction Guard API key. Get one at predictionguard.com
os.environ["PREDICTIONGUARD_TOKEN"] = "<your Prediction Guard access token>"

pgllm = PredictionGuard(model="OpenAI-gpt-3.5-turbo-instruct")

template = """Question: {question}

Answer: Let's think step by step."""
prompt = PromptTemplate.from_template(template)
llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)

question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"

llm_chain.predict(question=question)
```