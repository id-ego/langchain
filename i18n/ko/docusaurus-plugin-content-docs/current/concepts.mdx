---
description: LangChain의 핵심 구성 요소와 아키텍처를 소개하는 가이드입니다. 다양한 패키지와 구성 방법에 대해 설명합니다.
---

# 개념적 가이드

import ThemedImage from '@theme/ThemedImage';
import useBaseUrl from '@docusaurus/useBaseUrl';

이 섹션은 LangChain의 주요 부분에 대한 소개를 포함합니다.

## 아키텍처

LangChain은 여러 패키지로 구성된 프레임워크입니다.

### `langchain-core`
이 패키지는 다양한 구성 요소의 기본 추상화와 이를 함께 구성하는 방법을 포함합니다.
LLM, 벡터 저장소, 검색기 등과 같은 핵심 구성 요소의 인터페이스가 여기에서 정의됩니다.
타사 통합은 여기에서 정의되지 않습니다.
의존성은 목적에 맞게 매우 가볍게 유지됩니다.

### 파트너 패키지

긴 통합의 꼬리 부분은 `langchain-community`에 있지만, 인기 있는 통합을 별도의 패키지로 분리했습니다 (예: `langchain-openai`, `langchain-anthropic` 등).
이는 이러한 중요한 통합에 대한 지원을 개선하기 위해 수행되었습니다.

### `langchain`

주요 `langchain` 패키지는 애플리케이션의 인지 아키텍처를 구성하는 체인, 에이전트 및 검색 전략을 포함합니다.
이들은 타사 통합이 아닙니다.
여기 있는 모든 체인, 에이전트 및 검색 전략은 특정 통합에 국한되지 않고 모든 통합에 대해 일반적입니다.

### `langchain-community`

이 패키지는 LangChain 커뮤니티에서 유지 관리하는 타사 통합을 포함합니다.
주요 파트너 패키지는 분리되어 있습니다 (아래 참조).
이 패키지는 다양한 구성 요소(LLM, 벡터 저장소, 검색기)에 대한 모든 통합을 포함합니다.
이 패키지의 모든 의존성은 패키지를 가능한 한 가볍게 유지하기 위해 선택적입니다.

### [`langgraph`](https://langchain-ai.github.io/langgraph)

`langgraph`는 LLM을 사용하여 단계를 그래프의 엣지와 노드로 모델링하여 강력하고 상태 유지 가능한 다중 액터 애플리케이션을 구축하는 것을 목표로 하는 `langchain`의 확장입니다.

LangGraph는 일반적인 유형의 에이전트를 생성하기 위한 고급 인터페이스와 사용자 정의 흐름을 구성하기 위한 저급 API를 제공합니다.

### [`langserve`](/docs/langserve)

LangChain 체인을 REST API로 배포하는 패키지입니다. 프로덕션 준비가 된 API를 쉽게 설정할 수 있습니다.

### [LangSmith](https://docs.smith.langchain.com)

LLM 애플리케이션을 디버깅, 테스트, 평가 및 모니터링할 수 있는 개발자 플랫폼입니다.

<ThemedImage
alt="LangChain 프레임워크의 계층적 조직을 설명하는 다이어그램으로, 여러 레이어에 걸쳐 상호 연결된 부분을 표시합니다."
sources={{
light: useBaseUrl('/svg/langchain_stack_062024.svg'),
dark: useBaseUrl('/svg/langchain_stack_062024_dark.svg'),
}}
title="LangChain 프레임워크 개요"
style={{ width: "100%" }}
/>

## LangChain 표현 언어 (LCEL)
<span data-heading-keywords="lcel"></span>

LangChain 표현 언어, 또는 LCEL은 LangChain 구성 요소를 연결하는 선언적 방법입니다.
LCEL은 **코드 변경 없이 프로토타입을 프로덕션에 배포하는 것을 지원하기 위해** 처음부터 설계되었습니다. 가장 간단한 “프롬프트 + LLM” 체인부터 가장 복잡한 체인까지 (우리는 사람들이 프로덕션에서 100단계의 LCEL 체인을 성공적으로 실행하는 것을 보았습니다). LCEL을 사용해야 하는 몇 가지 이유를 강조하면:

**일급 스트리밍 지원**
LCEL로 체인을 구축하면 가능한 최상의 첫 번째 토큰까지의 시간(첫 번째 출력 청크가 나올 때까지 경과한 시간)을 얻을 수 있습니다. 일부 체인의 경우, 예를 들어 LLM에서 스트리밍 출력 파서로 직접 토큰을 스트리밍하여 LLM 공급자가 원시 토큰을 출력하는 것과 동일한 속도로 구문 분석된 점진적인 출력 청크를 반환받습니다.

**비동기 지원**
LCEL로 구축된 모든 체인은 동기 API(예: 프로토타입을 만들 때 Jupyter 노트북에서)와 비동기 API(예: [LangServe](/docs/langserve/) 서버에서) 모두에서 호출할 수 있습니다. 이는 프로토타입과 프로덕션에서 동일한 코드를 사용하면서 뛰어난 성능과 동일한 서버에서 많은 동시 요청을 처리할 수 있는 능력을 제공합니다.

**최적화된 병렬 실행**
LCEL 체인에 병렬로 실행할 수 있는 단계가 있을 때마다(예: 여러 검색기에서 문서를 가져오는 경우) 우리는 자동으로 이를 수행하며, 동기 및 비동기 인터페이스 모두에서 최소한의 대기 시간으로 처리합니다.

**재시도 및 대체**
LCEL 체인의 어떤 부분에 대해서도 재시도 및 대체를 구성할 수 있습니다. 이는 체인을 대규모로 더 신뢰할 수 있게 만드는 훌륭한 방법입니다. 우리는 현재 재시도/대체에 대한 스트리밍 지원을 추가하고 있어 대기 시간 비용 없이 추가적인 신뢰성을 얻을 수 있습니다.

**중간 결과 접근**
더 복잡한 체인의 경우 최종 출력이 생성되기 전에 중간 단계의 결과에 접근하는 것이 매우 유용합니다. 이는 최종 사용자에게 무언가가 발생하고 있음을 알리거나 체인을 디버깅하는 데 사용할 수 있습니다. 중간 결과를 스트리밍할 수 있으며, 이는 모든 [LangServe](/docs/langserve) 서버에서 사용할 수 있습니다.

**입력 및 출력 스키마**
입력 및 출력 스키마는 모든 LCEL 체인에 대해 체인의 구조에서 유추된 Pydantic 및 JSONSchema 스키마를 제공합니다. 이는 입력 및 출력을 검증하는 데 사용될 수 있으며, LangServe의 필수 부분입니다.

[**매끄러운 LangSmith 추적**](https://docs.smith.langchain.com)
체인이 점점 더 복잡해짐에 따라 모든 단계에서 정확히 무슨 일이 일어나고 있는지를 이해하는 것이 점점 더 중요해집니다.
LCEL을 사용하면 **모든** 단계가 최대 관찰 가능성과 디버깅 가능성을 위해 [LangSmith](https://docs.smith.langchain.com/)에 자동으로 기록됩니다.

LCEL은 `LLMChain` 및 `ConversationalRetrievalChain`과 같은 레거시 서브클래스 체인에 대한 동작 및 사용자 정의의 일관성을 제공하는 것을 목표로 합니다. 이러한 레거시 체인의 많은 부분은 프롬프트와 같은 중요한 세부 정보를 숨기며, 더 다양한 유효 모델이 등장함에 따라 사용자 정의가 점점 더 중요해졌습니다.

현재 이러한 레거시 체인 중 하나를 사용하고 있다면, [이 가이드를 참조하여 마이그레이션 방법을 확인하세요](/docs/versions/migrating_chains).

LCEL로 특정 작업을 수행하는 방법에 대한 가이드는 [관련 작업 가이드를 확인하세요](/docs/how_to/#langchain-expression-language-lcel).

### 실행 가능 인터페이스
<span data-heading-keywords="invoke,runnable"></span>

사용자 정의 체인을 만들기 쉽게 하기 위해 ["Runnable"](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) 프로토콜을 구현했습니다. 많은 LangChain 구성 요소가 `Runnable` 프로토콜을 구현하고 있으며, 여기에는 채팅 모델, LLM, 출력 파서, 검색기, 프롬프트 템플릿 등이 포함됩니다. 실행 가능 항목과 함께 작업하기 위한 여러 유용한 원시 요소도 있으며, 아래에서 자세히 설명합니다.

이는 표준 인터페이스로, 사용자 정의 체인을 정의하고 표준 방식으로 호출하는 것을 쉽게 만듭니다.
표준 인터페이스에는 다음이 포함됩니다:

- `stream`: 응답의 청크를 스트리밍하여 반환
- `invoke`: 입력에 대해 체인을 호출
- `batch`: 입력 목록에 대해 체인을 호출

이들은 또한 비동기성을 위해 [asyncio](https://docs.python.org/3/library/asyncio.html) `await` 구문과 함께 사용해야 하는 해당 비동기 메서드를 가지고 있습니다:

- `astream`: 응답의 청크를 비동기적으로 스트리밍하여 반환
- `ainvoke`: 입력에 대해 비동기적으로 체인을 호출
- `abatch`: 입력 목록에 대해 비동기적으로 체인을 호출
- `astream_log`: 최종 응답 외에도 발생하는 중간 단계를 스트리밍하여 반환
- `astream_events`: **베타** 체인에서 발생하는 이벤트를 스트리밍 ( `langchain-core` 0.1.14에서 도입됨)

**입력 유형** 및 **출력 유형**은 구성 요소에 따라 다릅니다:

| 구성 요소 | 입력 유형 | 출력 유형 |
| --- | --- | --- |
| 프롬프트 | 사전 | PromptValue |
| ChatModel | 단일 문자열, 채팅 메시지 목록 또는 PromptValue | ChatMessage |
| LLM | 단일 문자열, 채팅 메시지 목록 또는 PromptValue | 문자열 |
| OutputParser | LLM 또는 ChatModel의 출력 | 파서에 따라 다름 |
| Retriever | 단일 문자열 | 문서 목록 |
| Tool | 도구에 따라 단일 문자열 또는 사전 | 도구에 따라 다름 |

모든 실행 가능 항목은 입력 및 출력 **스키마**를 노출하여 입력 및 출력을 검사할 수 있습니다:
- `input_schema`: Runnable의 구조에서 자동 생성된 입력 Pydantic 모델
- `output_schema`: Runnable의 구조에서 자동 생성된 출력 Pydantic 모델

## 구성 요소

LangChain은 LLM으로 구축하는 데 유용한 다양한 구성 요소에 대해 표준화되고 확장 가능한 인터페이스 및 외부 통합을 제공합니다.
LangChain이 구현하는 일부 구성 요소가 있으며, 일부 구성 요소는 타사 통합에 의존하고, 다른 구성 요소는 혼합되어 있습니다.

### 채팅 모델
<span data-heading-keywords="chat model,chat models"></span>

메시지의 시퀀스를 입력으로 사용하고 채팅 메시지를 출력으로 반환하는 언어 모델(일반 텍스트를 사용하는 대신).
이들은 전통적으로 더 새로운 모델입니다(구형 모델은 일반적으로 `LLMs`입니다. 아래 참조).
채팅 모델은 대화 메시지에 서로 다른 역할을 할당하여 AI, 사용자 및 시스템 메시지와 같은 지침의 메시지를 구분하는 데 도움을 줍니다.

기본 모델은 메시지를 입력으로 받고 메시지를 출력으로 반환하지만, LangChain 래퍼는 이러한 모델이 문자열을 입력으로 받을 수 있도록 허용합니다. 이는 채팅 모델을 LLM 대신 쉽게 사용할 수 있음을 의미합니다.

문자열이 입력으로 전달되면 `HumanMessage`로 변환된 후 기본 모델에 전달됩니다.

LangChain은 어떤 채팅 모델도 호스팅하지 않으며, 대신 타사 통합에 의존합니다.

채팅 모델을 구성할 때 몇 가지 표준화된 매개변수가 있습니다:
- `model`: 모델의 이름
- `temperature`: 샘플링 온도
- `timeout`: 요청 시간 초과
- `max_tokens`: 생성할 최대 토큰 수
- `stop`: 기본 중지 시퀀스
- `max_retries`: 요청을 재시도할 최대 횟수
- `api_key`: 모델 공급자의 API 키
- `base_url`: 요청을 보낼 엔드포인트

중요한 사항 몇 가지:
- 표준 매개변수는 의도된 기능으로 매개변수를 노출하는 모델 공급자에게만 적용됩니다. 예를 들어, 일부 공급자는 최대 출력 토큰에 대한 구성을 노출하지 않으므로 max_tokens는 이러한 공급자에서 지원되지 않습니다.
- 표준 매개변수는 현재 자체 통합 패키지가 있는 통합에만 적용됩니다(예: `langchain-openai`, `langchain-anthropic` 등). `langchain-community`의 모델에는 적용되지 않습니다.

ChatModels는 해당 통합에 특화된 다른 매개변수도 수용합니다. ChatModel이 지원하는 모든 매개변수를 찾으려면 해당 모델의 API 참조로 이동하세요.

:::important
일부 채팅 모델은 **도구 호출**을 위해 미세 조정되어 있으며, 이를 위한 전용 API를 제공합니다.
일반적으로 이러한 모델은 비미세 조정된 모델보다 도구 호출에 더 뛰어나며, 도구 호출이 필요한 사용 사례에 권장됩니다.
자세한 내용은 [도구 호출 섹션](/docs/concepts/#functiontool-calling)을 참조하세요.
:::

채팅 모델을 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 작업 가이드를 확인하세요](/docs/how_to/#chat-models).

#### 다중 모달성

일부 채팅 모델은 다중 모달이며, 이미지, 오디오 및 비디오를 입력으로 수용합니다. 이러한 모델은 여전히 덜 일반적이므로 모델 공급자가 API를 정의하는 "최고의" 방법에 대해 표준화하지 않았습니다. 다중 모달 **출력**은 더욱 드뭅니다. 따라서 우리는 다중 모달 추상화를 상당히 가볍게 유지했으며, 이 분야가 성숙함에 따라 다중 모달 API 및 상호 작용 패턴을 더욱 확고히 할 계획입니다.

LangChain에서 다중 모달 입력을 지원하는 대부분의 채팅 모델은 OpenAI의 콘텐츠 블록 형식으로 해당 값을 수용합니다. 지금까지 이는 이미지 입력으로 제한됩니다. 비디오 및 기타 바이트 입력을 지원하는 Gemini와 같은 모델의 경우 API는 기본 모델별 표현도 지원합니다.

다중 모달 모델을 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 작업 가이드를 확인하세요](/docs/how_to/#multimodal).

다중 모달 모델을 가진 LangChain 모델 공급자의 전체 목록은 [이 표를 확인하세요](/docs/integrations/chat/#advanced-features).

### LLMs
<span data-heading-keywords="llm,llms"></span>

:::caution
순수 텍스트 입력/출력 LLM은 일반적으로 오래되었거나 낮은 수준입니다. 많은 인기 있는 모델은 비채팅 사용 사례에도 [채팅 완성 모델](/docs/concepts/#chat-models)로 사용하는 것이 가장 좋습니다.

대신 [위 섹션](/docs/concepts/#chat-models)을 찾고 있을 가능성이 높습니다.
:::

문자열을 입력으로 받아 문자열을 반환하는 언어 모델입니다.
이들은 전통적으로 오래된 모델입니다(더 새로운 모델은 일반적으로 [채팅 모델](/docs/concepts/#chat-models)입니다. 위 참조).

기본 모델은 문자열 입력, 문자열 출력을 사용하지만, LangChain 래퍼는 이러한 모델이 메시지를 입력으로 받을 수 있도록 허용합니다.
이는 [채팅 모델](/docs/concepts/#chat-models)과 동일한 인터페이스를 제공합니다.
메시지가 입력으로 전달되면, 내부적으로 문자열로 포맷된 후 기본 모델에 전달됩니다.

LangChain은 어떤 LLM도 호스팅하지 않으며, 대신 타사 통합에 의존합니다.

LLM을 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 작업 가이드를 확인하세요](/docs/how_to/#llms).

### 메시지

일부 언어 모델은 메시지 목록을 입력으로 받아 메시지를 반환합니다.
메시지에는 몇 가지 다른 유형이 있습니다.
모든 메시지는 `role`, `content`, 및 `response_metadata` 속성을 가집니다.

`role`은 메시지를 누가 말하는지를 설명합니다.
LangChain은 서로 다른 역할에 대해 서로 다른 메시지 클래스를 가지고 있습니다.

`content` 속성은 메시지의 내용을 설명합니다.
이는 몇 가지 다른 것일 수 있습니다:

- 문자열(대부분의 모델이 이 유형의 콘텐츠를 처리)
- 사전 목록(이것은 다중 모달 입력에 사용되며, 사전은 해당 입력 유형 및 입력 위치에 대한 정보를 포함)

#### HumanMessage

이는 사용자로부터의 메시지를 나타냅니다.
#### AIMessage

이것은 모델의 메시지를 나타냅니다. `content` 속성 외에도 이러한 메시지에는 다음이 포함됩니다:

**`response_metadata`**

`response_metadata` 속성은 응답에 대한 추가 메타데이터를 포함합니다. 여기의 데이터는 종종 각 모델 제공자에 따라 다릅니다. 로그 확률 및 토큰 사용량과 같은 정보가 여기에 저장될 수 있습니다.

**`tool_calls`**

이들은 언어 모델이 도구를 호출하기로 결정한 것을 나타냅니다. 이들은 `AIMessage` 출력의 일부로 포함됩니다. 여기에서 `.tool_calls` 속성으로 접근할 수 있습니다.

이 속성은 `ToolCall`의 목록을 반환합니다. `ToolCall`은 다음 인수를 가진 사전입니다:

- `name`: 호출해야 하는 도구의 이름.
- `args`: 해당 도구에 대한 인수.
- `id`: 해당 도구 호출의 ID.

#### SystemMessage

이것은 모델이 어떻게 행동해야 하는지를 알려주는 시스템 메시지를 나타냅니다. 모든 모델 제공자가 이를 지원하는 것은 아닙니다.

#### ToolMessage

이것은 도구 호출의 결과를 나타냅니다. `role` 및 `content` 외에도 이 메시지에는 다음이 있습니다:

- 호출된 도구의 ID를 전달하는 `tool_call_id` 필드.
- 추적에 유용하지만 모델에 전송되어서는 안 되는 도구 실행의 임의 아티팩트를 전달하는 데 사용할 수 있는 `artifact` 필드.

#### (Legacy) FunctionMessage

이것은 OpenAI의 레거시 함수 호출 API에 해당하는 레거시 메시지 유형입니다. `ToolMessage`는 업데이트된 도구 호출 API에 해당하기 위해 대신 사용해야 합니다.

이것은 함수 호출의 결과를 나타냅니다. `role` 및 `content` 외에도 이 메시지에는 이 결과를 생성하기 위해 호출된 함수의 이름을 전달하는 `name` 매개변수가 있습니다.

### Prompt templates
<span data-heading-keywords="prompt,prompttemplate,chatprompttemplate"></span>

프롬프트 템플릿은 사용자 입력 및 매개변수를 언어 모델에 대한 지침으로 변환하는 데 도움을 줍니다. 이는 모델의 응답을 안내하고, 맥락을 이해하고, 관련 있고 일관된 언어 기반 출력을 생성하는 데 도움이 됩니다.

프롬프트 템플릿은 입력으로 사전을 사용하며, 각 키는 프롬프트 템플릿에서 채워야 할 변수를 나타냅니다.

프롬프트 템플릿은 PromptValue를 출력합니다. 이 PromptValue는 LLM 또는 ChatModel에 전달될 수 있으며, 문자열이나 메시지 목록으로 변환될 수도 있습니다. 이 PromptValue가 존재하는 이유는 문자열과 메시지 간의 전환을 쉽게 하기 위함입니다.

프롬프트 템플릿에는 몇 가지 유형이 있습니다:

#### String PromptTemplates

이 프롬프트 템플릿은 단일 문자열을 형식화하는 데 사용되며, 일반적으로 더 간단한 입력에 사용됩니다. 예를 들어, 프롬프트 템플릿을 구성하고 사용하는 일반적인 방법은 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")

prompt_template.invoke({"topic": "cats"})
```


#### ChatPromptTemplates

이 프롬프트 템플릿은 메시지 목록을 형식화하는 데 사용됩니다. 이러한 "템플릿"은 템플릿 자체의 목록으로 구성됩니다. 예를 들어, ChatPromptTemplate을 구성하고 사용하는 일반적인 방법은 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

prompt_template.invoke({"topic": "cats"})
```


위의 예에서 이 ChatPromptTemplate은 호출될 때 두 개의 메시지를 구성합니다. 첫 번째는 변수가 형식화되지 않은 시스템 메시지입니다. 두 번째는 HumanMessage이며, 사용자가 전달한 `topic` 변수로 형식화됩니다.

#### MessagesPlaceholder
<span data-heading-keywords="messagesplaceholder"></span>

이 프롬프트 템플릿은 특정 위치에 메시지 목록을 추가하는 역할을 합니다. 위의 ChatPromptTemplate에서 우리는 두 개의 메시지를 형식화하는 방법을 보았습니다. 각 메시지는 문자열입니다. 그러나 사용자가 특정 위치에 삽입할 메시지 목록을 전달하고 싶다면 어떻게 해야 할까요? 이것이 MessagesPlaceholder를 사용하는 방법입니다.

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}, {"imported": "MessagesPlaceholder", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html", "title": "Conceptual guide"}, {"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})
```


이것은 두 개의 메시지 목록을 생성합니다. 첫 번째는 시스템 메시지이고, 두 번째는 우리가 전달한 HumanMessage입니다. 만약 우리가 5개의 메시지를 전달했다면, 총 6개의 메시지가 생성됩니다(시스템 메시지와 전달된 5개 메시지 포함). 이는 메시지 목록을 특정 위치에 삽입할 수 있도록 유용합니다.

`MessagesPlaceholder` 클래스를 명시적으로 사용하지 않고 동일한 작업을 수행하는 대안적인 방법은 다음과 같습니다:

```python
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("placeholder", "{msgs}") # <-- This is the changed part
])
```


프롬프트 템플릿을 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 가이드](/docs/how_to/#prompt-templates)를 참조하십시오.

### Example selectors
더 나은 성능을 달성하기 위한 일반적인 프롬프트 기법 중 하나는 프롬프트의 일부로 예제를 포함하는 것입니다. 이는 언어 모델에게 어떻게 행동해야 하는지에 대한 구체적인 예를 제공합니다. 때때로 이러한 예제는 프롬프트에 하드코딩되지만, 더 고급 상황에서는 동적으로 선택하는 것이 좋습니다. 예제 선택기는 예제를 선택하고 이를 프롬프트로 형식화하는 역할을 하는 클래스입니다.

예제 선택기를 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 가이드](/docs/how_to/#example-selectors)를 참조하십시오.

### Output parsers
<span data-heading-keywords="output parser"></span>

:::note

여기의 정보는 모델의 텍스트 출력을 가져와 더 구조화된 표현으로 구문 분석하는 파서에 관한 것입니다. 점점 더 많은 모델이 함수(또는 도구) 호출을 지원하며, 이는 자동으로 처리됩니다. 출력 구문 분석보다 함수/도구 호출을 사용하는 것이 권장됩니다. 이에 대한 문서는 [여기에서](/docs/concepts/#function-tool-calling)를 참조하십시오.

:::

모델의 출력을 가져와 다운스트림 작업에 더 적합한 형식으로 변환하는 역할을 합니다. LLM을 사용하여 구조화된 데이터를 생성하거나 채팅 모델 및 LLM의 출력을 정규화할 때 유용합니다.

LangChain은 다양한 유형의 출력 파서를 가지고 있습니다. 다음은 LangChain이 지원하는 출력 파서 목록입니다. 아래 표에는 다양한 정보가 포함되어 있습니다:

**이름**: 출력 파서의 이름

**스트리밍 지원**: 출력 파서가 스트리밍을 지원하는지 여부.

**형식 지침 있음**: 출력 파서에 형식 지침이 있는지 여부. 이는 일반적으로 사용 가능하지만 (a) 원하는 스키마가 프롬프트가 아닌 다른 매개변수(예: OpenAI 함수 호출)에서 지정된 경우 또는 (b) OutputParser가 다른 OutputParser를 래핑하는 경우에는 그렇지 않습니다.

**LLM 호출**: 이 출력 파서가 LLM을 호출하는지 여부. 이는 일반적으로 잘못 형식화된 출력을 수정하려고 하는 출력 파서에서만 수행됩니다.

**입력 유형**: 예상 입력 유형. 대부분의 출력 파서는 문자열과 메시지 모두에서 작동하지만, 일부(예: OpenAI Functions)는 특정 kwargs가 포함된 메시지가 필요합니다.

**출력 유형**: 파서가 반환하는 객체의 출력 유형.

**설명**: 이 출력 파서에 대한 우리의 주석 및 사용 시기.

| 이름            | 스트리밍 지원 | 형식 지침 있음       | LLM 호출 | 입력 유형                       | 출력 유형          | 설명                                                                                                                                                                                                                                              |
|-----------------|--------------------|-------------------------------|-----------|----------------------------------|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [JSON](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html#langchain_core.output_parsers.json.JsonOutputParser)            | ✅                  | ✅                             |           | `str` \| `Message`               | JSON 객체          | 지정된 대로 JSON 객체를 반환합니다. Pydantic 모델을 지정할 수 있으며, 해당 모델에 대한 JSON을 반환합니다. 함수 호출을 사용하지 않는 구조화된 데이터를 얻기 위한 가장 신뢰할 수 있는 출력 파서일 것입니다.                                    |
| [XML](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html#langchain_core.output_parsers.xml.XMLOutputParser)            | ✅                  | ✅                             |           | `str` \| `Message`                 | `dict`               | XML 출력이 필요할 때 사용합니다. XML 작성을 잘하는 모델(예: Anthropic의 모델)과 함께 사용합니다.                                                                                                                            |
| [CSV](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#langchain_core.output_parsers.list.CommaSeparatedListOutputParser)           | ✅                  | ✅                             |           | `str` \| `Message`                 | `List[str]`          | 쉼표로 구분된 값 목록을 반환합니다.                                                                                                                                                                                                                |
| [OutputFixing](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html#langchain.output_parsers.fix.OutputFixingParser)    |                    |                               | ✅         | `str` \| `Message`                 |                      | 다른 출력 파서를 래핑합니다. 해당 출력 파서에서 오류가 발생하면, 이 출력 파서는 오류 메시지와 잘못된 출력을 LLM에 전달하고 출력을 수정하도록 요청합니다.                                                                                              |
| [RetryWithError](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html#langchain.output_parsers.retry.RetryWithErrorOutputParser)  |                    |                               | ✅         | `str` \| `Message`                 |                      | 다른 출력 파서를 래핑합니다. 해당 출력 파서에서 오류가 발생하면, 이 출력 파서는 원래 입력, 잘못된 출력 및 오류 메시지를 LLM에 전달하고 수정하도록 요청합니다. OutputFixingParser와 비교할 때, 이 출력 파서는 원래 지침도 전송합니다. |
| [Pydantic](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html#langchain_core.output_parsers.pydantic.PydanticOutputParser)        |                    | ✅                             |           | `str` \| `Message`                 | `pydantic.BaseModel` | 사용자 정의 Pydantic 모델을 가져와 해당 형식으로 데이터를 반환합니다.                                                                                                                                                                                     |
| [YAML](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html#langchain.output_parsers.yaml.YamlOutputParser)        |                    | ✅                             |           | `str` \| `Message`                 | `pydantic.BaseModel` | 사용자 정의 Pydantic 모델을 가져와 해당 형식으로 데이터를 반환합니다. YAML을 사용하여 인코딩합니다.                                                                                                                                                                                    |
| [PandasDataFrame](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser) |                    | ✅                             |           | `str` \| `Message`                 | `dict`               | pandas DataFrame으로 작업할 때 유용합니다.                                                                                                                                                                                                      |
| [Enum](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)            |                    | ✅                             |           | `str` \| `Message`                 | `Enum`               | 응답을 제공된 열거형 값 중 하나로 구문 분석합니다.                                                                                                                                                                                                    |
| [Datetime](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)        |                    | ✅                             |           | `str` \| `Message`                 | `datetime.datetime`  | 응답을 날짜 및 시간 문자열로 구문 분석합니다.                                                                                                                                                                                                                  |
| [Structured](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)      |                    | ✅                             |           | `str` \| `Message`                 | `Dict[str, str]`     | 구조화된 정보를 반환하는 출력 파서입니다. 이는 필드가 문자열로만 허용되기 때문에 다른 출력 파서보다 덜 강력합니다. 이는 더 작은 LLM과 작업할 때 유용할 수 있습니다.                                            |

출력 파서를 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 가이드](/docs/how_to/#output-parsers)를 참조하십시오.

### Chat history
대부분의 LLM 애플리케이션은 대화형 인터페이스를 가지고 있습니다. 대화의 필수 요소는 대화 중에 이전에 소개된 정보를 참조할 수 있는 것입니다. 최소한 대화 시스템은 과거 메시지의 일부 창에 직접 접근할 수 있어야 합니다.

`ChatHistory`의 개념은 임의의 체인을 래핑하는 데 사용할 수 있는 LangChain의 클래스를 나타냅니다. 이 `ChatHistory`는 기본 체인의 입력 및 출력을 추적하고 이를 메시지 데이터베이스에 메시지로 추가합니다. 이후 상호작용은 이러한 메시지를 로드하고 입력의 일부로 체인에 전달합니다.

### Documents
<span data-heading-keywords="document,documents"></span>

LangChain의 Document 객체는 일부 데이터에 대한 정보를 포함합니다. 두 개의 속성이 있습니다:

- `page_content: str`: 이 문서의 내용. 현재는 문자열만 포함됩니다.
- `metadata: dict`: 이 문서와 관련된 임의의 메타데이터. 문서 ID, 파일 이름 등을 추적할 수 있습니다.

### Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

이 클래스들은 Document 객체를 로드합니다. LangChain은 Slack, Notion, Google Drive 등 다양한 데이터 소스와의 수백 가지 통합을 가지고 있습니다.

각 DocumentLoader는 고유한 특정 매개변수를 가지고 있지만, 모두 `.load` 메서드를 사용하여 동일한 방식으로 호출할 수 있습니다. 예시 사용 사례는 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "CSVLoader", "source": "langchain_community.document_loaders.csv_loader", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html", "title": "Conceptual guide"}]-->
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # <-- Integration specific parameters here
)
data = loader.load()
```


문서 로더를 사용하는 방법에 대한 구체적인 내용은 [여기에서 관련 가이드](/docs/how_to/#document-loaders)를 참조하십시오.
### 텍스트 분할기

문서를 로드한 후에는 애플리케이션에 더 적합하도록 변환하고 싶을 것입니다. 가장 간단한 예는 긴 문서를 모델의 컨텍스트 창에 맞는 더 작은 조각으로 나누고 싶다는 것입니다. LangChain은 문서를 쉽게 분할, 결합, 필터링 및 조작할 수 있는 여러 내장 문서 변환기를 제공합니다.

긴 텍스트를 처리하려면 해당 텍스트를 조각으로 나누는 것이 필요합니다. 간단하게 들리지만 여기에는 많은 잠재적 복잡성이 있습니다. 이상적으로는 의미적으로 관련된 텍스트 조각을 함께 유지하고 싶습니다. "의미적으로 관련된" 것이 무엇인지에 따라 텍스트의 유형이 달라질 수 있습니다. 이 노트북은 이를 수행하는 여러 가지 방법을 보여줍니다.

높은 수준에서 텍스트 분할기는 다음과 같이 작동합니다:

1. 텍스트를 작고 의미 있는 조각(종종 문장)으로 나눕니다.
2. 특정 크기에 도달할 때까지 이러한 작은 조각을 더 큰 조각으로 결합하기 시작합니다(어떤 함수로 측정됨).
3. 해당 크기에 도달하면 그 조각을 독립적인 텍스트 조각으로 만들고, 일부 중첩을 가진 새로운 텍스트 조각을 생성하기 시작합니다(조각 간의 맥락을 유지하기 위해).

즉, 텍스트 분할기를 사용자 정의할 수 있는 두 가지 축이 있습니다:

1. 텍스트가 분할되는 방식
2. 조각 크기가 측정되는 방식

텍스트 분할기를 사용하는 방법에 대한 구체적인 내용은 [관련 사용 가이드를 여기서 확인하세요](/docs/how_to/#text-splitters).

### 임베딩 모델
<span data-heading-key워드="embedding,embeddings"></span>

임베딩 모델은 텍스트 조각의 벡터 표현을 생성합니다. 벡터는 텍스트의 의미를 포착하는 숫자 배열로 생각할 수 있습니다. 이렇게 텍스트를 표현함으로써 의미가 가장 유사한 다른 텍스트 조각을 검색하는 등의 수학적 작업을 수행할 수 있습니다. 이러한 자연어 검색 기능은 많은 유형의 [컨텍스트 검색](/docs/concepts/#retrieval)을 뒷받침하며, 여기서 우리는 LLM에 쿼리에 효과적으로 응답하는 데 필요한 관련 데이터를 제공합니다.

![](/img/embeddings.png)

`Embeddings` 클래스는 텍스트 임베딩 모델과 인터페이스하기 위해 설계된 클래스입니다. 다양한 임베딩 모델 제공자(OpenAI, Cohere, Hugging Face 등)와 로컬 모델이 있으며, 이 클래스는 모든 모델에 대한 표준 인터페이스를 제공하도록 설계되었습니다.

LangChain의 기본 Embeddings 클래스는 문서를 임베딩하는 메서드와 쿼리를 임베딩하는 메서드 두 가지를 제공합니다. 전자는 여러 텍스트를 입력으로 받고, 후자는 단일 텍스트를 입력으로 받습니다. 이 두 가지를 별도의 메서드로 두는 이유는 일부 임베딩 제공자가 문서(검색할 수 있는)와 쿼리(검색 쿼리 자체)에 대해 다른 임베딩 방법을 가지고 있기 때문입니다.

임베딩 모델을 사용하는 방법에 대한 구체적인 내용은 [관련 사용 가이드를 여기서 확인하세요](/docs/how_to/#embedding-models).

### 벡터 저장소
<span data-heading-key워드="vector,vectorstore,vectorstores,vector store,vector stores"></span>

비구조화된 데이터를 저장하고 검색하는 가장 일반적인 방법 중 하나는 이를 임베딩하고 결과 임베딩 벡터를 저장한 다음, 쿼리 시 비구조화된 쿼리를 임베딩하고 임베딩된 쿼리와 '가장 유사한' 임베딩 벡터를 검색하는 것입니다. 벡터 저장소는 임베딩된 데이터를 저장하고 벡터 검색을 수행하는 역할을 합니다.

대부분의 벡터 저장소는 임베딩된 벡터에 대한 메타데이터를 저장할 수 있으며, 유사성 검색 전에 해당 메타데이터에 대한 필터링을 지원하여 반환된 문서에 대한 더 많은 제어를 제공합니다.

벡터 저장소는 다음과 같이 검색기 인터페이스로 변환할 수 있습니다:

```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```


벡터 저장소를 사용하는 방법에 대한 구체적인 내용은 [관련 사용 가이드를 여기서 확인하세요](/docs/how_to/#vector-stores).

### 검색기
<span data-heading-key워드="retriever,retrievers"></span>

검색기는 비구조화된 쿼리를 주면 문서를 반환하는 인터페이스입니다. 이는 벡터 저장소보다 더 일반적입니다. 검색기는 문서를 저장할 필요는 없으며, 단지 문서를 반환(또는 검색)하면 됩니다. 검색기는 벡터 저장소에서 생성할 수 있지만, [위키피디아 검색](/docs/integrations/retrievers/wikipedia/) 및 [아마존 켄드라](/docs/integrations/retrievers/amazon_kendra_retriever/)와 같은 더 넓은 범위를 포함합니다.

검색기는 문자열 쿼리를 입력으로 받아 문서 목록을 출력으로 반환합니다.

검색기를 사용하는 방법에 대한 구체적인 내용은 [관련 사용 가이드를 여기서 확인하세요](/docs/how_to/#retrievers).

### 키-값 저장소

[문서당 여러 벡터로 인덱싱 및 검색하기](/docs/how_to/multi_vector/) 또는 [임베딩 캐싱](/docs/how_to/caching_embeddings/)와 같은 일부 기술에서는 키-값(KV) 저장소 형태가 유용합니다.

LangChain은 임의의 데이터를 저장할 수 있는 [`BaseStore`](https://api.python.langchain.com/en/latest/stores/langchain_core.stores.BaseStore.html) 인터페이스를 포함하고 있습니다. 그러나 KV 저장소가 필요한 LangChain 구성 요소는 이진 데이터를 저장하는 보다 구체적인 `BaseStore[str, bytes]` 인스턴스를 수용하며(이를 `ByteStore`라고 함), 내부적으로 특정 요구 사항에 맞게 데이터를 인코딩하고 디코딩하는 역할을 합니다.

즉, 사용자는 서로 다른 유형의 데이터에 대해 서로 다른 저장소를 생각할 필요 없이 하나의 저장소 유형만 고려하면 됩니다.

#### 인터페이스

모든 [`BaseStores`](https://api.python.langchain.com/en/latest/stores/langchain_core.stores.BaseStore.html)는 다음 인터페이스를 지원합니다. 이 인터페이스는 **여러** 키-값 쌍을 동시에 수정할 수 있도록 허용합니다:

- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: 여러 키의 내용을 가져오고, 키가 존재하지 않으면 `None`을 반환합니다.
- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: 여러 키의 내용을 설정합니다.
- `mdelete(key: Sequence[str]) -> None`: 여러 키를 삭제합니다.
- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: 선택적으로 접두사로 필터링하여 저장소의 모든 키를 반환합니다.

키-값 저장소 구현에 대한 내용은 [이 섹션](/docs/integrations/stores/)을 참조하세요.

### 도구
<span data-heading-key워드="tool,tools"></span>

도구는 모델에 의해 호출되도록 설계된 유틸리티입니다: 그들의 입력은 모델에 의해 생성되도록 설계되었고, 그들의 출력은 모델에 다시 전달되도록 설계되었습니다. 모델이 코드의 일부를 제어하거나 외부 API를 호출해야 할 때 도구가 필요합니다.

도구는 다음으로 구성됩니다:

1. 도구의 이름.
2. 도구가 수행하는 작업에 대한 설명.
3. 도구의 입력을 정의하는 JSON 스키마.
4. 함수(선택적으로 비동기 버전의 함수).

도구가 모델에 바인딩되면 이름, 설명 및 JSON 스키마가 모델에 대한 컨텍스트로 제공됩니다. 도구 목록과 일련의 지침이 주어지면 모델은 특정 입력으로 하나 이상의 도구를 호출하도록 요청할 수 있습니다. 일반적인 사용 예는 다음과 같습니다:

```python
tools = [...] # Define a list of tools
llm_with_tools = llm.bind_tools(tools)
ai_msg = llm_with_tools.invoke("do xyz...")
# -> AIMessage(tool_calls=[ToolCall(...), ...], ...)
```


모델에서 반환된 `AIMessage`는 `tool_calls`와 연관될 수 있습니다. 응답 유형이 어떻게 생겼는지에 대한 더 많은 정보는 [이 가이드](/docs/concepts/#aimessage)를 참조하세요.

선택한 도구가 호출되면 결과는 모델에 다시 전달되어 모델이 수행 중인 작업을 완료할 수 있습니다. 도구를 호출하고 응답을 전달하는 방법은 일반적으로 두 가지입니다:

#### 인수만으로 호출하기

인수만으로 도구를 호출하면 원시 도구 출력을 받게 됩니다(보통 문자열). 일반적으로 다음과 같이 보입니다:

```python
# You will want to previously check that the LLM returned tool calls
tool_call = ai_msg.tool_calls[0]
# ToolCall(args={...}, id=..., ...)
tool_output = tool.invoke(tool_call["args"])
tool_message = ToolMessage(
    content=tool_output,
    tool_call_id=tool_call["id"],
    name=tool_call["name"]
)
```


`content` 필드는 일반적으로 모델에 다시 전달됩니다. 원시 도구 응답을 모델에 전달하고 싶지 않지만 여전히 보관하고 싶다면 도구 출력을 변환할 수 있지만 이를 아티팩트로 전달할 수 있습니다(자세한 내용은 [`ToolMessage.artifact`를 여기서 읽어보세요](/docs/concepts/#toolmessage)).

```python
... # Same code as above
response_for_llm = transform(response)
tool_message = ToolMessage(
    content=response_for_llm,
    tool_call_id=tool_call["id"],
    name=tool_call["name"],
    artifact=tool_output
)
```


#### `ToolCall`로 호출하기

도구를 호출하는 다른 방법은 모델이 생성한 전체 `ToolCall`로 호출하는 것입니다. 이렇게 하면 도구가 ToolMessage를 반환합니다. 이 방법의 장점은 도구 출력을 ToolMessage로 변환하는 로직을 직접 작성할 필요가 없다는 것입니다. 일반적으로 다음과 같이 보입니다:

```python
tool_call = ai_msg.tool_calls[0]
# -> ToolCall(args={...}, id=..., ...)
tool_message = tool.invoke(tool_call)
# -> ToolMessage(
    content="tool result foobar...",
    tool_call_id=...,
    name="tool_name"
)
```


이 방법으로 도구를 호출하고 ToolMessage에 대한 [아티팩트](/docs/concepts/#toolmessage)를 포함하려면 도구가 두 가지를 반환해야 합니다. 아티팩트를 반환하는 도구 정의에 대한 자세한 내용은 [여기에서 읽어보세요](/docs/how_to/tool_artifacts/) .

#### 모범 사례

모델에서 사용할 도구를 설계할 때 다음 사항을 염두에 두는 것이 중요합니다:

- 명시적인 [도구 호출 API](/docs/concepts/#functiontool-calling)를 가진 채팅 모델이 비세밀하게 조정된 모델보다 도구 호출을 더 잘 수행합니다.
- 도구의 이름, 설명 및 JSON 스키마가 잘 선택되면 모델의 성능이 향상됩니다. 이는 또 다른 형태의 프롬프트 엔지니어링입니다.
- 단순하고 좁은 범위의 도구가 복잡한 도구보다 모델이 사용하기 더 쉽습니다.

#### 관련

도구를 사용하는 방법에 대한 구체적인 내용은 [도구 사용 가이드](/docs/how_to/#tools)를 참조하세요.

미리 구축된 도구를 사용하려면 [도구 통합 문서](/docs/integrations/tools/)를 참조하세요.

### 툴킷
<span data-heading-key워드="toolkit,toolkits"></span>

툴킷은 특정 작업을 위해 함께 사용하도록 설계된 도구 모음입니다. 편리한 로딩 방법이 있습니다.

모든 툴킷은 도구 목록을 반환하는 `get_tools` 메서드를 노출합니다. 따라서 다음과 같이 할 수 있습니다:

```python
# Initialize a toolkit
toolkit = ExampleTookit(...)

# Get list of tools
tools = toolkit.get_tools()
```


### 에이전트

자체적으로 언어 모델은 행동을 취할 수 없습니다 - 단지 텍스트를 출력할 뿐입니다. LangChain의 주요 사용 사례 중 하나는 **에이전트**를 만드는 것입니다. 에이전트는 LLM을 추론 엔진으로 사용하여 어떤 행동을 취할지와 그 행동에 대한 입력이 무엇이어야 하는지를 결정하는 시스템입니다. 이러한 행동의 결과는 다시 에이전트에 피드백되어 추가 행동이 필요한지, 아니면 종료해도 되는지를 결정합니다.

[LangGraph](https://github.com/langchain-ai/langgraph)는 매우 제어 가능하고 사용자 정의 가능한 에이전트를 만들기 위해 특별히 설계된 LangChain의 확장입니다. 에이전트 개념에 대한 더 깊은 개요는 해당 문서를 확인하세요.

LangChain에는 우리가 더 이상 사용하지 않으려는 레거시 에이전트 개념인 `AgentExecutor`가 있습니다. AgentExecutor는 본질적으로 에이전트를 위한 런타임이었습니다. 시작하기에 좋은 장소였지만, 더 많은 사용자 정의 에이전트를 갖기 시작하면서 유연성이 부족했습니다. 이를 해결하기 위해 LangGraph를 구축하여 유연하고 제어 가능한 런타임이 되도록 했습니다.

여전히 AgentExecutor를 사용하고 있다면 걱정하지 마세요: [AgentExecutor 사용 방법에 대한 가이드](/docs/how_to/agent_executor)가 여전히 있습니다. 그러나 LangGraph로 전환하는 것이 좋습니다. 이를 지원하기 위해 [전환 가이드](/docs/how_to/migrate_agent)를 마련했습니다.

#### ReAct 에이전트
<span data-heading-key워드="react,react agent"></span>

에이전트를 구축하기 위한 인기 있는 아키텍처 중 하나는 [**ReAct**](https://arxiv.org/abs/2210.03629)입니다. ReAct는 추론과 행동을 반복적인 프로세스에서 결합합니다 - 사실 "ReAct"라는 이름은 "Reason"과 "Act"의 약자입니다.

일반적인 흐름은 다음과 같습니다:

- 모델은 입력 및 이전 관찰에 대한 응답으로 어떤 단계를 취할지 "생각"합니다.
- 모델은 사용 가능한 도구 중에서 행동을 선택합니다(또는 사용자에게 응답하기로 선택합니다).
- 모델은 해당 도구에 대한 인수를 생성합니다.
- 에이전트 런타임(실행기)은 선택한 도구를 파싱하고 생성된 인수로 호출합니다.
- 실행기는 도구 호출의 결과를 모델에 대한 관찰로 반환합니다.
- 이 프로세스는 에이전트가 응답하기로 선택할 때까지 반복됩니다.

모델 특정 기능이 필요 없는 일반적인 프롬프트 기반 구현이 있지만, 가장 신뢰할 수 있는 구현은 [도구 호출](/docs/how_to/tool_calling/)과 같은 기능을 사용하여 출력을 신뢰성 있게 형식화하고 변동성을 줄입니다.

자세한 내용은 [LangGraph 문서](https://langchain-ai.github.io/langgraph/)를 참조하거나 LangGraph로 마이그레이션하는 방법에 대한 [이 사용 가이드](/docs/how_to/migrate_agent/)를 참조하세요.

### 콜백

LangChain은 LLM 애플리케이션의 다양한 단계에 후킹할 수 있는 콜백 시스템을 제공합니다. 이는 로깅, 모니터링, 스트리밍 및 기타 작업에 유용합니다.

API 전반에 걸쳐 사용할 수 있는 `callbacks` 인수를 사용하여 이러한 이벤트에 구독할 수 있습니다. 이 인수는 하나 이상의 아래에 자세히 설명된 메서드를 구현할 것으로 예상되는 핸들러 객체의 목록입니다.
#### 콜백 이벤트

| 이벤트               | 이벤트 트리거                                 | 관련 메서드              |
|---------------------|----------------------------------------------|-------------------------|
| 채팅 모델 시작      | 채팅 모델이 시작될 때                        | `on_chat_model_start`   |
| LLM 시작            | LLM이 시작될 때                             | `on_llm_start`         |
| LLM 새 토큰         | LLM 또는 채팅 모델이 새 토큰을 발행할 때   | `on_llm_new_token`     |
| LLM 종료            | LLM 또는 채팅 모델이 종료될 때              | `on_llm_end`           |
| LLM 오류            | LLM 또는 채팅 모델에서 오류가 발생할 때    | `on_llm_error`         |
| 체인 시작           | 체인이 실행되기 시작할 때                  | `on_chain_start`       |
| 체인 종료           | 체인이 종료될 때                            | `on_chain_end`         |
| 체인 오류           | 체인에서 오류가 발생할 때                   | `on_chain_error`       |
| 도구 시작           | 도구가 실행되기 시작할 때                  | `on_tool_start`        |
| 도구 종료           | 도구가 종료될 때                            | `on_tool_end`          |
| 도구 오류           | 도구에서 오류가 발생할 때                   | `on_tool_error`        |
| 에이전트 액션      | 에이전트가 액션을 취할 때                   | `on_agent_action`      |
| 에이전트 종료       | 에이전트가 종료될 때                        | `on_agent_finish`      |
| 검색기 시작         | 검색기가 시작될 때                          | `on_retriever_start`   |
| 검색기 종료         | 검색기가 종료될 때                          | `on_retriever_end`     |
| 검색기 오류         | 검색기에서 오류가 발생할 때                 | `on_retriever_error`   |
| 텍스트              | 임의의 텍스트가 실행될 때                   | `on_text`              |
| 재시도              | 재시도 이벤트가 실행될 때                   | `on_retry`             |

#### 콜백 핸들러

콜백 핸들러는 `sync` 또는 `async`일 수 있습니다:

* 동기 콜백 핸들러는 [BaseCallbackHandler](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) 인터페이스를 구현합니다.
* 비동기 콜백 핸들러는 [AsyncCallbackHandler](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html) 인터페이스를 구현합니다.

런타임 동안 LangChain은 이벤트가 트리거될 때 "등록된" 각 콜백 핸들러에서 적절한 메서드를 호출할 책임이 있는 적절한 콜백 관리자를 구성합니다 (예: [CallbackManager](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.manager.CallbackManager.html) 또는 [AsyncCallbackManager](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.manager.AsyncCallbackManager.html)).

#### 콜백 전달

`callbacks` 속성은 API 전반의 대부분의 객체(모델, 도구, 에이전트 등)에서 두 가지 다른 장소에서 사용할 수 있습니다:

콜백은 API 전반의 대부분의 객체(모델, 도구, 에이전트 등)에서 두 가지 다른 장소에서 사용할 수 있습니다:

- **요청 시간 콜백**: 입력 데이터와 함께 요청 시 전달됩니다. 모든 표준 `Runnable` 객체에서 사용할 수 있습니다. 이러한 콜백은 정의된 객체의 모든 자식에게 상속됩니다. 예를 들어, `chain.invoke({"number": 25}, {"callbacks": [handler]})`.
- **생성자 콜백**: `chain = TheNameOfSomeChain(callbacks=[handler])`. 이러한 콜백은 객체의 생성자에 인수로 전달됩니다. 콜백은 정의된 객체에만 범위가 제한되며, 객체의 자식에게는 **상속되지 않습니다**.

:::warning
생성자 콜백은 정의된 객체에만 범위가 제한됩니다. 자식 객체에게는 **상속되지 않습니다**.
:::

사용자가 사용자 정의 체인이나 실행 가능한 것을 생성하는 경우 요청 시간 콜백을 모든 자식 객체로 전파해야 함을 기억해야 합니다.

:::important Python<=3.10에서의 비동기

다른 실행 가능한 것을 호출하고 Python<=3.10에서 비동기로 실행되는 모든 `RunnableLambda`, `RunnableGenerator` 또는 `Tool`은 콜백을 자식 객체로 수동으로 전파해야 합니다. 이는 LangChain이 이 경우 자식 객체로 콜백을 자동으로 전파할 수 없기 때문입니다.

이는 사용자 정의 실행 가능한 것 또는 도구에서 이벤트가 발생하지 않는 일반적인 이유입니다.
:::

콜백을 사용하는 방법에 대한 구체적인 내용은 [관련 사용 방법 가이드 여기](/docs/how_to/#callbacks)를 참조하십시오.

## 기술

### 스트리밍
<span data-heading-keywords="stream,streaming"></span>

개별 LLM 호출은 종종 전통적인 리소스 요청보다 훨씬 더 오랜 시간 동안 실행됩니다. 이는 여러 추론 단계를 요구하는 더 복잡한 체인이나 에이전트를 구축할 때 더욱 심화됩니다.

다행히도 LLM은 출력을 반복적으로 생성하므로 최종 응답이 준비되기 전에 합리적인 중간 결과를 보여줄 수 있습니다. 출력이 사용 가능해지는 즉시 소비하는 것은 LLM으로 앱을 구축할 때 지연 문제를 완화하는 데 중요한 부분이 되었으며, LangChain은 스트리밍에 대한 일급 지원을 목표로 하고 있습니다.

아래에서는 LangChain의 스트리밍에 대한 몇 가지 개념과 고려 사항을 논의하겠습니다.

#### `.stream()` 및 `.astream()`

LangChain의 대부분 모듈에는 인체공학적인 스트리밍 인터페이스로서 `.stream()` 메서드(비동기 환경을 위한 동등한 `.astream()` 메서드 포함)가 포함되어 있습니다. `.stream()`은 반복자를 반환하며, 이를 간단한 `for` 루프를 사용하여 소비할 수 있습니다. 다음은 채팅 모델의 예입니다:

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Conceptual guide"}]-->
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")

for chunk in model.stream("what color is the sky?"):
    print(chunk.content, end="|", flush=True)
```


스트리밍을 본래 지원하지 않는 모델(또는 다른 구성 요소)의 경우 이 반복자는 단일 청크만 생성하지만, 호출할 때 동일한 일반 패턴을 여전히 사용할 수 있습니다. `.stream()`을 사용하면 추가 구성을 제공할 필요 없이 자동으로 모델을 스트리밍 모드로 호출합니다.

출력된 각 청크의 유형은 구성 요소의 유형에 따라 다릅니다 - 예를 들어, 채팅 모델은 [`AIMessageChunks`](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessageChunk.html)를 생성합니다. 이 메서드는 [LangChain 표현 언어](/docs/concepts/#langchain-expression-language-lcel)의 일부이므로, 각 생성된 청크를 변환하기 위해 [출력 파서](/docs/concepts/#output-parsers)를 사용하여 서로 다른 출력의 형식 차이를 처리할 수 있습니다.

`.stream()` 사용 방법에 대한 자세한 내용은 [이 가이드](/docs/how_to/streaming/#using-stream)를 참조하십시오.

#### `.astream_events()`
<span data-heading-keywords="astream_events,stream_events,stream events"></span>

`.stream()` 메서드는 직관적이지만 체인의 최종 생성 값만 반환할 수 있습니다. 이는 단일 LLM 호출에는 괜찮지만, 여러 LLM 호출이 함께 이루어지는 더 복잡한 체인을 구축할 때는 체인의 중간 값을 최종 출력과 함께 사용하고 싶을 수 있습니다 - 예를 들어, 문서에 대한 채팅 앱을 구축할 때 최종 생성과 함께 출처를 반환하는 경우입니다.

이를 위해 [콜백](/docs/concepts/#callbacks-1)을 사용하거나 체인을 구성하여 중간 값을 최종에 전달하는 방법이 있지만, LangChain은 또한 콜백의 유연성과 `.stream()`의 인체공학을 결합한 `.astream_events()` 메서드를 포함합니다. 호출 시, 다양한 유형의 [이벤트](/docs/how_to/streaming/#event-reference)를 생성하는 반복자를 반환하며, 이를 프로젝트의 필요에 따라 필터링하고 처리할 수 있습니다.

다음은 스트리밍된 채팅 모델 출력을 포함하는 이벤트만 인쇄하는 작은 예입니다:

```python
<!--IMPORTS:[{"imported": "StrOutputParser", "source": "langchain_core.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html", "title": "Conceptual guide"}, {"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}, {"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Conceptual guide"}]-->
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}, version="v2"):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```


이를 콜백 이벤트에 대한 반복자로 대략적으로 생각할 수 있습니다(형식은 다르지만) - 거의 모든 LangChain 구성 요소에서 사용할 수 있습니다!

`.astream_events()` 사용 방법에 대한 자세한 정보는 [이 가이드](/docs/how_to/streaming/#using-stream-events)를 참조하십시오. 여기에는 사용 가능한 이벤트를 나열한 표가 포함되어 있습니다.

#### 콜백

LangChain에서 LLM의 출력을 스트리밍하는 가장 낮은 수준의 방법은 [콜백](/docs/concepts/#callbacks) 시스템을 통해 이루어집니다. 콜백 핸들러를 전달하여 [`on_llm_new_token`](https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html#langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.on_llm_new_token) 이벤트를 처리할 수 있습니다. 해당 구성 요소가 호출되면, 구성 요소에 포함된 모든 [LLM](/docs/concepts/#llms) 또는 [채팅 모델](/docs/concepts/#chat-models)은 생성된 토큰으로 콜백을 호출합니다. 콜백 내에서 토큰을 다른 목적지로 파이프할 수 있습니다. 예를 들어, HTTP 응답으로 파이프할 수 있습니다. 또한 [`on_llm_end`](https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html#langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.on_llm_end) 이벤트를 처리하여 필요한 정리를 수행할 수 있습니다.

콜백 사용에 대한 구체적인 내용은 [이 사용 방법 섹션](/docs/how_to/#callbacks)을 참조하십시오.

콜백은 LangChain에서 스트리밍을 위한 첫 번째 기술이었습니다. 강력하고 일반화 가능하지만, 개발자에게는 다루기 어려울 수 있습니다. 예를 들어:

- 결과를 수집하기 위해 일부 집계기 또는 다른 스트림을 명시적으로 초기화하고 관리해야 합니다.
- 실행 순서가 명시적으로 보장되지 않으며, 이론적으로 `.invoke()` 메서드가 완료된 후에 콜백이 실행될 수 있습니다.
- 제공자는 종종 출력을 스트리밍하기 위해 추가 매개변수를 전달하도록 요구하며, 대신 모든 출력을 한 번에 반환하지 않습니다.
- 실제 모델 호출의 결과를 콜백 결과보다 무시하는 경우가 많습니다.

#### 토큰

대부분의 모델 제공자가 입력 및 출력을 측정하는 데 사용하는 단위는 **토큰**이라고 불리는 단위입니다. 토큰은 언어 모델이 텍스트를 처리하거나 생성할 때 읽고 생성하는 기본 단위입니다. 토큰의 정확한 정의는 모델이 훈련된 특정 방식에 따라 다를 수 있습니다 - 예를 들어, 영어에서 토큰은 "apple"과 같은 단일 단어일 수도 있고, "app"과 같은 단어의 일부일 수도 있습니다.

모델에 프롬프트를 보낼 때, 프롬프트의 단어와 문자는 **토크나이저**를 사용하여 토큰으로 인코딩됩니다. 그런 다음 모델은 생성된 출력 토큰을 스트리밍하여, 토크나이저가 이를 사람이 읽을 수 있는 텍스트로 디코딩합니다. 아래 예시는 OpenAI 모델이 `LangChain is cool!`를 어떻게 토큰화하는지를 보여줍니다:

![](/img/tokenization.png)

5개의 서로 다른 토큰으로 나뉘며, 토큰 간의 경계가 단어 경계와 정확히 일치하지 않음을 볼 수 있습니다.

언어 모델이 "문자"와 같은 즉각적으로 직관적인 것보다 토큰을 사용하는 이유는 텍스트를 처리하고 이해하는 방식과 관련이 있습니다. 고수준에서 언어 모델은 초기 입력과 이전 생성에 기반하여 다음 생성된 출력을 반복적으로 예측합니다. 토큰을 사용하여 모델을 훈련시키면 언어 모델이 의미를 전달하는 언어 단위(단어 또는 하위 단어)를 처리하도록 하여, 모델이 문법 및 맥락을 포함한 언어 구조를 학습하고 이해하기 쉽게 만듭니다. 또한, 토큰을 사용하면 문자 수준 처리에 비해 처리하는 텍스트 단위가 줄어들어 효율성을 개선할 수 있습니다.

### 함수/도구 호출

:::info
우리는 도구 호출이라는 용어를 함수 호출과 교환하여 사용합니다. 함수 호출은 때때로 단일 함수의 호출을 의미하지만, 우리는 모든 모델이 각 메시지에서 여러 도구 또는 함수 호출을 반환할 수 있다고 간주합니다.
:::

도구 호출은 [채팅 모델](/docs/concepts/#chat-models)이 주어진 프롬프트에 응답하여 사용자 정의 스키마와 일치하는 출력을 생성할 수 있게 합니다.

이름은 모델이 어떤 작업을 수행하는 것처럼 암시하지만, 실제로는 그렇지 않습니다! 모델은 도구에 대한 인수만 생성하며, 실제로 도구를 실행하는 것은 사용자에게 달려 있습니다. 생성된 인수로 함수를 호출하고 싶지 않은 일반적인 예는 비구조적 텍스트에서 원하는 스키마와 일치하는 구조화된 출력을 [추출](/docs/concepts/#structured-output)하려는 경우입니다. 이 경우 모델에 원하는 스키마와 일치하는 매개변수를 사용하는 "추출" 도구를 제공하고, 생성된 출력을 최종 결과로 취급합니다.

![채팅 모델에 의한 도구 호출 다이어그램](/img/tool_call.png)

도구 호출은 보편적이지 않지만, [Anthropic](/docs/integrations/chat/anthropic/), [Cohere](/docs/integrations/chat/cohere/), [Google](/docs/integrations/chat/google_vertex_ai_palm/), [Mistral](/docs/integrations/chat/mistralai/), [OpenAI](/docs/integrations/chat/openai/) 및 심지어 [Ollama](/docs/integrations/chat/ollama/)를 통해 로컬에서 실행되는 모델을 포함하여 많은 인기 있는 LLM 제공자가 지원합니다.

LangChain은 다양한 모델에서 일관된 도구 호출을 위한 표준화된 인터페이스를 제공합니다.

표준 인터페이스는 다음으로 구성됩니다:

* `ChatModel.bind_tools()`: 모델이 호출할 수 있는 도구를 지정하는 메서드입니다. 이 메서드는 [LangChain 도구](/docs/concepts/#tools)와 [Pydantic](https://pydantic.dev/) 객체를 허용합니다.
* `AIMessage.tool_calls`: 모델에서 반환된 `AIMessage`의 속성으로, 모델이 요청한 도구 호출에 접근할 수 있습니다.
#### 도구 사용

모델이 도구를 호출한 후, 도구를 호출하여 인수를 모델에 다시 전달하여 사용할 수 있습니다. LangChain은 이를 처리하는 데 도움이 되는 [`Tool`](/docs/concepts/#tools) 추상화를 제공합니다.

일반적인 흐름은 다음과 같습니다:

1. 쿼리에 대한 응답으로 채팅 모델을 사용하여 도구 호출을 생성합니다.
2. 생성된 도구 호출을 인수로 사용하여 적절한 도구를 호출합니다.
3. 도구 호출 결과를 [`ToolMessages`](/docs/concepts/#toolmessage) 형식으로 포맷합니다.
4. 전체 메시지 목록을 모델에 전달하여 최종 답변을 생성하거나(또는 더 많은 도구를 호출할 수 있도록) 합니다.

![완전한 도구 호출 흐름의 다이어그램](/img/tool_calling_flow.png)

이것이 도구 호출 [에이전트](/docs/concepts/#agents)가 작업을 수행하고 쿼리에 응답하는 방법입니다.

아래에서 더 집중된 가이드를 확인하세요:

- [채팅 모델을 사용하여 도구 호출하는 방법](/docs/how_to/tool_calling/)
- [도구 출력을 채팅 모델에 전달하는 방법](/docs/how_to/tool_results_pass_to_model/)
- [LangGraph로 에이전트 구축하기](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

### 구조화된 출력

LLM은 임의의 텍스트를 생성할 수 있습니다. 이는 모델이 다양한 입력에 적절하게 응답할 수 있게 하지만, 일부 사용 사례에서는 LLM의 출력을 특정 형식이나 구조로 제한하는 것이 유용할 수 있습니다. 이를 **구조화된 출력**이라고 합니다.

예를 들어, 출력이 관계형 데이터베이스에 저장되어야 하는 경우, 모델이 정의된 스키마나 형식에 맞는 출력을 생성하는 것이 훨씬 더 쉽습니다. 비구조적 텍스트에서 [특정 정보 추출하기](/docs/tutorials/extraction/)는 특히 유용한 경우입니다. 가장 일반적으로 출력 형식은 JSON이지만, [YAML](/docs/how_to/output_parser_yaml/)과 같은 다른 형식도 유용할 수 있습니다. 아래에서는 LangChain에서 모델로부터 구조화된 출력을 얻는 몇 가지 방법에 대해 논의하겠습니다.

#### `.with_structured_output()`

편의를 위해, 일부 LangChain 채팅 모델은 [`.with_structured_output()`](/docs/how_to/structured_output/#the-with_structured_output-method) 메서드를 지원합니다. 이 메서드는 입력으로 스키마만 필요하며, dict 또는 Pydantic 객체를 반환합니다. 일반적으로 이 메서드는 아래에 설명된 더 고급 메서드 중 하나를 지원하는 모델에만 존재하며, 내부적으로 그 중 하나를 사용합니다. 적절한 출력 파서를 가져오고 모델에 맞는 형식으로 스키마를 포맷하는 일을 처리합니다.

예시입니다:

```python
from typing import Optional

from langchain_core.pydantic_v1 import BaseModel, Field


class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")

structured_llm = llm.with_structured_output(Joke)

structured_llm.invoke("Tell me a joke about cats")
```


```
Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)

```


구조화된 출력을 작업할 때 이 방법을 시작점으로 권장합니다:

- 출력 파서를 가져올 필요 없이 내부적으로 다른 모델 특정 기능을 사용합니다.
- 도구 호출을 사용하는 모델의 경우, 특별한 프롬프트가 필요하지 않습니다.
- 여러 기본 기술이 지원되는 경우, `method` 매개변수를 제공하여 [어떤 것이 사용되는지 전환할 수 있습니다](/docs/how_to/structured_output/#advanced-specifying-the-method-for-structuring-outputs).

다음과 같은 경우 다른 기술을 사용해야 할 수 있습니다:

- 사용 중인 채팅 모델이 도구 호출을 지원하지 않는 경우.
- 매우 복잡한 스키마로 작업하고 있으며 모델이 일치하는 출력을 생성하는 데 어려움을 겪는 경우.

자세한 내용은 [이 가이드](/docs/how_to/structured_output/#the-with_structured_output-method)를 확인하세요.

`with_structured_output()`을 지원하는 모델 목록은 [이 표](/docs/integrations/chat/#advanced-features)에서 확인할 수 있습니다.

#### 원시 프롬프트

모델이 출력을 구조화하도록 하는 가장 직관적인 방법은 정중하게 요청하는 것입니다. 쿼리 외에도 원하는 출력 유형을 설명하는 지침을 제공한 다음, [출력 파서](/docs/concepts/#output-parsers)를 사용하여 원시 모델 메시지 또는 문자열 출력을 더 쉽게 조작할 수 있는 것으로 변환합니다.

원시 프롬프트의 가장 큰 이점은 유연성입니다:

- 원시 프롬프트는 특별한 모델 기능이 필요하지 않으며, 전달된 스키마를 이해할 수 있는 충분한 추론 능력만 필요합니다.
- 원하는 형식으로 프롬프트할 수 있으며, JSON만이 아닙니다. 이는 사용 중인 모델이 XML 또는 YAML과 같은 특정 유형의 데이터에 더 많이 훈련된 경우 유용할 수 있습니다.

그러나 몇 가지 단점도 있습니다:

- LLM은 비결정적이며, LLM에게 매끄럽게 파싱할 수 있는 정확한 형식으로 데이터를 일관되게 출력하도록 요청하는 것은 놀랍도록 어려울 수 있으며, 모델에 따라 다릅니다.
- 개별 모델은 훈련된 데이터에 따라 특성이 다르며, 프롬프트를 최적화하는 것은 꽤 어려울 수 있습니다. 일부는 [JSON 스키마](https://json-schema.org/) 해석에 더 뛰어난 반면, 다른 모델은 TypeScript 정의에 더 적합할 수 있으며, 또 다른 모델은 XML을 선호할 수 있습니다.

모델 제공자가 제공하는 기능이 신뢰성을 높일 수 있지만, 어떤 방법을 선택하든 결과를 조정하는 데 프롬프트 기술은 여전히 중요합니다.

#### JSON 모드
<span data-heading-keywords="json mode"></span>

[Mistral](/docs/integrations/chat/mistralai/), [OpenAI](/docs/integrations/chat/openai/), [Together AI](/docs/integrations/chat/together/) 및 [Ollama](/docs/integrations/chat/ollama/)와 같은 일부 모델은 **JSON 모드**라는 기능을 지원하며, 일반적으로 구성에서 활성화됩니다.

활성화되면 JSON 모드는 모델의 출력을 항상 유효한 JSON의 형태로 제한합니다. 종종 일부 사용자 정의 프롬프트가 필요하지만, 일반적으로 완전히 원시 프롬프트보다 훨씬 덜 부담스럽고, `"항상 JSON을 반환해야 합니다"`와 같은 방식으로 진행됩니다. [출력도 일반적으로 더 쉽게 파싱할 수 있습니다](/docs/how_to/output_parser_json/).

또한 일반적으로 도구 호출보다 직접 사용하기가 더 간단하고 더 일반적으로 사용 가능하며, 도구 호출보다 프롬프트 및 결과 형성에 대한 더 많은 유연성을 제공할 수 있습니다.

예시입니다:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Conceptual guide"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Conceptual guide"}, {"imported": "SimpleJsonOutputParser", "source": "langchain.output_parsers.json", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.SimpleJsonOutputParser.html", "title": "Conceptual guide"}]-->
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.output_parsers.json import SimpleJsonOutputParser

model = ChatOpenAI(
    model="gpt-4o",
    model_kwargs={ "response_format": { "type": "json_object" } },
)

prompt = ChatPromptTemplate.from_template(
    "Answer the user's question to the best of your ability."
    'You must always output a JSON object with an "answer" key and a "followup_question" key.'
    "{question}"
)

chain = prompt | model | SimpleJsonOutputParser()

chain.invoke({ "question": "What is the powerhouse of the cell?" })
```


```
{'answer': 'The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.',
 'followup_question': 'Would you like to know more about how mitochondria produce energy?'}
```


JSON 모드를 지원하는 모델 제공자의 전체 목록은 [이 표](/docs/integrations/chat/#advanced-features)에서 확인하세요.

#### 도구 호출 {#structured-output-tool-calling}

지원하는 모델의 경우, [도구 호출](/docs/concepts/#functiontool-calling)은 구조화된 출력을 위한 매우 편리한 방법이 될 수 있습니다. 이는 스키마를 프롬프트하는 최선의 방법에 대한 추측을 제거하고 내장된 모델 기능을 선호합니다.

이는 먼저 원하는 스키마를 직접 또는 [LangChain 도구](/docs/concepts/#tools)를 통해 [채팅 모델](/docs/concepts/#chat-models)에 `.bind_tools()` 메서드를 사용하여 바인딩함으로써 작동합니다. 모델은 그런 다음 원하는 형태와 일치하는 `args`를 포함하는 `tool_calls` 필드를 포함하는 `AIMessage`를 생성합니다.

LangChain에서 도구를 모델에 바인딩하는 데 사용할 수 있는 몇 가지 허용 가능한 형식이 있습니다. 다음은 하나의 예입니다:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Conceptual guide"}]-->
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI

class ResponseFormatter(BaseModel):
    """Always use this tool to structure your response to the user."""

    answer: str = Field(description="The answer to the user's question")
    followup_question: str = Field(description="A followup question the user could ask")

model = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
)

model_with_tools = model.bind_tools([ResponseFormatter])

ai_msg = model_with_tools.invoke("What is the powerhouse of the cell?")

ai_msg.tool_calls[0]["args"]
```


```
{'answer': "The powerhouse of the cell is the mitochondrion. It generates most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.",
 'followup_question': 'How do mitochondria generate ATP?'}
```


도구 호출은 모델이 구조화된 출력을 생성하도록 하는 일반적으로 일관된 방법이며, 모델이 이를 지원할 때 [`.with_structured_output()`](/docs/concepts/#with_structured_output) 메서드에 사용되는 기본 기술입니다.

구조화된 출력을 위한 함수/도구 호출을 사용하는 데 유용한 실용적인 리소스인 다음의 가이드를 확인하세요:

- [LLM에서 구조화된 데이터 반환하는 방법](/docs/how_to/structured_output/)
- [모델을 사용하여 도구 호출하는 방법](/docs/how_to/tool_calling)

도구 호출을 지원하는 모델 제공자의 전체 목록은 [이 표](/docs/integrations/chat/#advanced-features)에서 확인하세요.

### 검색

LLM은 크지만 고정된 데이터 세트에서 훈련되어 개인적이거나 최근의 정보에 대한 추론 능력이 제한됩니다. 특정 사실로 LLM을 미세 조정하는 것은 이를 완화하는 한 가지 방법이지만, 종종 [사실 회상에 적합하지 않으며](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) [비용이 많이 들 수 있습니다](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise). 검색은 주어진 입력에 대한 응답을 개선하기 위해 LLM에 관련 정보를 제공하는 과정입니다. 검색 증강 생성(RAG)은 검색된 정보를 사용하여 LLM 생성을 기반으로 하는 과정입니다.

:::tip

* RAG from Scratch [코드](https://github.com/langchain-ai/rag-from-scratch) 및 [비디오 시리즈](https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&feature=shared)를 확인하세요.
* 검색에 대한 고급 가이드는 [이 RAG 튜토리얼](/docs/tutorials/rag/)을 참조하세요.

:::

RAG는 검색된 문서의 관련성과 품질만큼 좋습니다. 다행히도, RAG 시스템을 설계하고 개선하는 데 사용할 수 있는 새로운 기술 세트가 등장하고 있습니다. 우리는 이러한 기술의 많은 부분을 분류하고 요약하는 데 집중했으며(아래 그림 참조), 다음 섹션에서 몇 가지 고급 전략적 지침을 공유할 것입니다. 다양한 조합을 사용하여 실험할 수 있으며, 앱의 다양한 반복을 평가하는 방법을 보여주는 [이 LangSmith 가이드](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application)도 유용할 수 있습니다.

![](/img/rag_landscape.png)

#### 쿼리 변환

먼저, RAG 시스템에 대한 사용자 입력을 고려하세요. 이상적으로 RAG 시스템은 잘못된 질문부터 복잡한 다중 부분 쿼리까지 다양한 입력을 처리할 수 있어야 합니다. **LLM을 사용하여 입력을 검토하고 선택적으로 수정하는 것이 쿼리 변환의 핵심 아이디어입니다.** 이는 원시 사용자 입력을 검색 시스템에 최적화하는 일반적인 완충 역할을 합니다. 예를 들어, 이는 키워드를 추출하는 것만큼 간단할 수 있으며, 복잡한 쿼리에 대해 여러 하위 질문을 생성하는 것만큼 복잡할 수 있습니다.

| 이름          | 사용 시기 | 설명 |
|---------------|-------------|-------------|
| [다중 쿼리](/docs/how_to/MultiQueryRetriever/)   | 질문의 여러 관점을 다루어야 할 때. | 여러 관점에서 사용자 질문을 재작성하고, 각 재작성된 질문에 대한 문서를 검색하여 모든 쿼리에 대한 고유한 문서를 반환합니다. |
| [분해](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb) | 질문을 더 작은 하위 문제로 나눌 수 있을 때. | 질문을 하위 문제/질문 집합으로 분해하며, 이는 순차적으로 해결하거나(첫 번째 답변 + 검색을 사용하여 두 번째 질문에 답변) 병렬로(각 답변을 최종 답변으로 통합) 해결할 수 있습니다. |
| [스텝백](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | 더 높은 수준의 개념적 이해가 필요할 때. | 먼저 LLM에게 더 높은 수준의 개념이나 원칙에 대한 일반적인 스텝백 질문을 하도록 프롬프트하고, 이에 대한 관련 사실을 검색합니다. 이 기반을 사용하여 사용자 질문에 답변합니다. |
| [HyDE](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)          | 원시 사용자 입력을 사용하여 관련 문서를 검색하는 데 어려움이 있을 때. | LLM을 사용하여 질문을 질문에 대한 가상의 문서로 변환합니다. 가상의 문서를 임베딩하여 실제 문서를 검색하는 데 사용하며, 문서 간 유사성 검색이 더 관련성 높은 일치를 생성할 수 있다는 전제를 사용합니다. |

:::tip

RAG from Scratch 비디오에서 몇 가지 특정 접근 방식을 확인하세요:
- [다중 쿼리](https://youtu.be/JChPi0CRnDY?feature=shared)
- [분해](https://youtu.be/h0OPWlEOank?feature=shared)
- [스텝백](https://youtu.be/xn1jEjRyJ2U?feature=shared)
- [HyDE](https://youtu.be/SaDzIVkYqyY?feature=shared)

:::

#### 라우팅

둘째, RAG 시스템에서 사용할 수 있는 데이터 소스를 고려하세요. 하나 이상의 데이터베이스 또는 구조화된 데이터와 비구조화된 데이터 소스 간에 쿼리하고 싶습니다. **LLM을 사용하여 입력을 검토하고 적절한 데이터 소스로 라우팅하는 것은 소스를 가로질러 쿼리하는 간단하고 효과적인 접근 방식입니다.**

| 이름             | 사용 시기                                | 설명 |
|------------------|--------------------------------------------|-------------|
| [논리적 라우팅](/docs/how_to/routing/)  | 입력을 라우팅할 위치를 결정하기 위해 LLM에 규칙을 프롬프트할 수 있을 때. | 논리적 라우팅은 LLM을 사용하여 쿼리에 대해 추론하고 가장 적합한 데이터 저장소를 선택할 수 있습니다. |
| [의미적 라우팅](/docs/how_to/routing/#routing-by-semantic-similarity) | 의미적 유사성이 입력을 라우팅할 위치를 결정하는 효과적인 방법일 때. | 의미적 라우팅은 쿼리와 일반적으로 일련의 프롬프트를 임베딩합니다. 그런 다음 유사성에 따라 적절한 프롬프트를 선택합니다. |

:::tip

라우팅에 대한 [비디오](https://youtu.be/pfpIndq7Fi8?feature=shared)도 확인하세요.  

:::
#### 쿼리 구성

셋째, 데이터 소스 중 특정 쿼리 형식이 필요한 것이 있는지 고려하십시오. 많은 구조화된 데이터베이스는 SQL을 사용합니다. 벡터 저장소는 문서 메타데이터에 키워드 필터를 적용하기 위한 특정 구문을 자주 사용합니다. **자연어 쿼리를 쿼리 구문으로 변환하는 데 LLM을 사용하는 것은 인기 있고 강력한 접근 방식입니다.**  
특히, [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), 및 [메타데이터 필터를 위한 쿼리 분석](/docs/tutorials/query_analysis/#query-analysis)은 각각 구조화된 데이터베이스, 그래프 데이터베이스 및 벡터 데이터베이스와 상호작용하는 유용한 방법입니다.

| 이름                                        | 사용 시기                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                      |
|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Text to SQL](/docs/tutorials/sql_qa/)      | 사용자가 SQL을 통해 접근할 수 있는 관계형 데이터베이스에 저장된 정보를 요구하는 질문을 할 때.                                   | 사용자의 입력을 SQL 쿼리로 변환하는 LLM을 사용합니다.                                             |
| [Text-to-Cypher](/docs/tutorials/graph/)    | 사용자가 Cypher를 통해 접근할 수 있는 그래프 데이터베이스에 저장된 정보를 요구하는 질문을 할 때.                                     | 사용자의 입력을 Cypher 쿼리로 변환하는 LLM을 사용합니다.                                              |
| [Self Query](/docs/how_to/self_query/)      | 사용자가 텍스트와의 유사성보다는 메타데이터에 기반하여 문서를 가져오는 것이 더 나은 질문을 할 때.          | 사용자의 입력을 두 가지로 변환하는 LLM을 사용합니다: (1) 의미적으로 조회할 문자열, (2) 함께 사용할 메타데이터 필터. 이는 종종 질문이 문서의 메타데이터(내용 자체가 아님)에 관한 것이기 때문에 유용합니다.                                              |

:::tip

우리의 [블로그 게시물 개요](https://blog.langchain.dev/query-construction/)와 [쿼리 구성](https://youtu.be/kl6NwWYxvbM?feature=shared) 관련 RAG 비디오를 참조하십시오. 이는 사용자 질문을 구조화된 쿼리로 변환하는 DSL(도메인 특정 언어)과의 상호작용 프로세스입니다.

:::

#### 인덱싱

넷째, 문서 인덱스의 설계를 고려하십시오. 간단하고 강력한 아이디어는 **검색을 위해 인덱싱하는 문서를 LLM에 전달하는 문서와 분리하는 것입니다.** 인덱싱은 종종 벡터 저장소와 함께 임베딩 모델을 사용하여 [문서의 의미 정보를 고정 크기 벡터로 압축합니다](/docs/concepts/#embedding-models).

많은 RAG 접근 방식은 문서를 청크로 나누고 LLM에 대한 입력 질문과의 유사성에 따라 일부 수를 검색하는 데 중점을 둡니다. 그러나 청크 크기와 청크 수를 설정하는 것은 어려울 수 있으며, LLM이 질문에 답변할 수 있는 전체 맥락을 제공하지 않으면 결과에 영향을 미칠 수 있습니다. 또한, LLM은 점점 더 많은 토큰을 처리할 수 있는 능력을 갖추고 있습니다.

이 긴장을 해결할 수 있는 두 가지 접근 방식이 있습니다: (1) [Multi Vector](/docs/how_to/multi_vector/) 검색기는 LLM을 사용하여 문서를 인덱싱에 적합한 형태(예: 종종 요약)로 변환하지만, 생성 시 LLM에 전체 문서를 반환합니다. (2) [ParentDocument](/docs/how_to/parent_document_retriever/) 검색기는 문서 청크를 임베딩하지만, 전체 문서도 반환합니다. 아이디어는 두 가지 장점을 모두 얻는 것입니다: 검색을 위해 간결한 표현(요약 또는 청크)을 사용하되, 답변 생성을 위해 전체 문서를 사용하는 것입니다.

| 이름                      | 인덱스 유형                   | LLM 사용 여부               | 사용 시기                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                      |
|---------------------------|------------------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Vector store](/docs/how_to/vectorstore_retriever/)               | 벡터 저장소                  | 아니오                        | 시작하는 단계에서 빠르고 간편한 것을 찾고 있을 때.                                                                     | 이는 가장 간단한 방법이며, 시작하기 가장 쉬운 방법입니다. 각 텍스트 조각에 대한 임베딩을 생성하는 것을 포함합니다.                                                                                                                                                             |
| [ParentDocument](/docs/how_to/parent_document_retriever/)            | 벡터 저장소 + 문서 저장소 | 아니오                        | 페이지에 독립적인 정보 조각이 많고, 이들을 개별적으로 인덱싱하는 것이 가장 좋지만, 모두 함께 검색하는 것이 가장 좋을 때.       | 이는 각 문서에 대해 여러 청크를 인덱싱하는 것을 포함합니다. 그런 다음 임베딩 공간에서 가장 유사한 청크를 찾지만, 전체 부모 문서를 검색하여 반환합니다(개별 청크가 아닌).                                                                         |
| [Multi Vector](/docs/how_to/multi_vector/)              | 벡터 저장소 + 문서 저장소 | 인덱싱 중 가끔 사용 | 문서에서 텍스트 자체보다 인덱싱에 더 관련성이 높은 정보를 추출할 수 있을 때.                          | 이는 각 문서에 대해 여러 벡터를 생성하는 것을 포함합니다. 각 벡터는 다양한 방법으로 생성될 수 있습니다 - 예를 들어, 텍스트의 요약 및 가상의 질문 등이 있습니다.                                                                                                                 |
| [Time-Weighted Vector store](/docs/how_to/time_weighted_vectorstore/) | 벡터 저장소                  | 아니오                        | 문서와 관련된 타임스탬프가 있으며, 가장 최근의 문서를 검색하고 싶을 때                                          | 이는 의미적 유사성(일반 벡터 검색과 같이)과 최근성(인덱싱된 문서의 타임스탬프를 살펴보는) 조합을 기반으로 문서를 검색합니다.                                                                                                                                    |

:::tip

- 우리의 RAG from Scratch 비디오에서 [인덱싱 기초](https://youtu.be/bjb_EMsTDKI?feature=shared)를 참조하십시오.
- 우리의 RAG from Scratch 비디오에서 [다중 벡터 검색기](https://youtu.be/gTCU9I6QqCE?feature=shared)를 참조하십시오.

:::

다섯째, 유사성 검색의 품질을 개선할 방법을 고려하십시오. 임베딩 모델은 텍스트를 고정 길이(벡터) 표현으로 압축하여 문서의 의미 내용을 포착합니다. 이 압축은 검색/검색에 유용하지만, 단일 벡터 표현이 문서의 의미적 뉘앙스/세부 사항을 포착하는 데 큰 부담을 줍니다. 경우에 따라, 관련 없는 또는 중복된 콘텐츠가 임베딩의 의미적 유용성을 희석할 수 있습니다.

[ColBERT](https://docs.google.com/presentation/d/1IRhAdGjIevrrotdplHNcc4aXgIYyKamUKTWtB3m3aMU/edit?usp=sharing)는 더 높은 세분화 임베딩으로 이 문제를 해결하는 흥미로운 접근 방식입니다: (1) 문서와 쿼리의 각 토큰에 대해 맥락적으로 영향을 받은 임베딩을 생성하고, (2) 각 쿼리 토큰과 모든 문서 토큰 간의 유사성을 점수화하고, (3) 최대값을 취하고, (4) 모든 쿼리 토큰에 대해 이를 수행하고, (5) 모든 쿼리 토큰에 대한 최대 점수의 합계를 취하여 쿼리-문서 유사성 점수를 얻습니다; 이 토큰 단위 점수화는 강력한 결과를 낼 수 있습니다.

![](/img/colbert.png)

검색 품질을 개선하기 위한 몇 가지 추가 트릭이 있습니다. 임베딩은 의미 정보를 포착하는 데 뛰어나지만, 키워드 기반 쿼리에는 어려움을 겪을 수 있습니다. 많은 [벡터 저장소](/docs/integrations/retrievers/pinecone_hybrid_search/)는 키워드와 의미적 유사성을 결합하는 내장된 [하이브리드 검색](https://docs.pinecone.io/guides/data/understanding-hybrid-search)을 제공하여 두 접근 방식의 장점을 결합합니다. 또한, 많은 벡터 저장소는 [최대 한계 관련성](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/)을 제공하여 유사하고 중복된 문서를 반환하지 않도록 검색 결과를 다양화하려고 합니다.

| 이름              | 사용 시기                                              | 설명 |
|-------------------|----------------------------------------------------------|-------------|
| [ColBERT](/docs/integrations/providers/ragatouille/#using-colbert-as-a-reranker)           | 더 높은 세분화 임베딩이 필요할 때.           | ColBERT는 문서와 쿼리의 각 토큰에 대해 맥락적으로 영향을 받은 임베딩을 사용하여 세분화된 쿼리-문서 유사성 점수를 얻습니다. |
| [하이브리드 검색](/docs/integrations/retrievers/pinecone_hybrid_search/)     | 키워드 기반 유사성과 의미적 유사성을 결합할 때.    | 하이브리드 검색은 키워드와 의미적 유사성을 결합하여 두 접근 방식의 장점을 결합합니다. |
| [최대 한계 관련성 (MMR)](/docs/integrations/vectorstores/pinecone/#maximal-marginal-relevance-searches) | 검색 결과를 다양화해야 할 때. | MMR은 검색 결과를 다양화하여 유사하고 중복된 문서를 반환하지 않도록 시도합니다. |

:::tip

우리의 RAG from Scratch 비디오에서 [ColBERT](https://youtu.be/cN6S0Ehm7_8?feature=shared)를 참조하십시오.

:::

#### 후처리

여섯째, 검색된 문서를 필터링하거나 순위를 매기는 방법을 고려하십시오. 이는 [여러 소스에서 반환된 문서를 결합하는 경우](/docs/integrations/retrievers/cohere-reranker/#doing-reranking-with-coherererank) 매우 유용합니다. 이는 덜 관련성이 높은 문서의 순위를 낮추거나 [유사한 문서를 압축하는](/docs/how_to/contextual_compression/#more-built-in-compressors-filters) 데 도움이 될 수 있습니다.

| 이름                      | 인덱스 유형                   | LLM 사용 여부               | 사용 시기                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                      |
|---------------------------|------------------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Contextual Compression](/docs/how_to/contextual_compression/)    | 모든                          | 가끔                 | 검색된 문서에 너무 많은 관련 없는 정보가 포함되어 LLM을 방해하는 경우.                         | 이는 다른 검색기 위에 후처리 단계를 추가하고 검색된 문서에서 가장 관련성이 높은 정보만 추출합니다. 이는 임베딩 또는 LLM을 사용하여 수행할 수 있습니다.                                                                                                               |
| [Ensemble](/docs/how_to/ensemble_retriever/)                  | 모든                          | 아니오                        | 여러 검색 방법이 있고 이를 결합해 보려는 경우.                                                                        | 이는 여러 검색기에서 문서를 검색한 다음 이를 결합합니다.                                                                                                                                                                                                                          |
| [Re-ranking](/docs/integrations/retrievers/cohere-reranker/)                  | 모든                          | 예                        | 검색된 문서를 관련성에 따라 순위를 매기고 싶을 때, 특히 여러 검색 방법의 결과를 결합하고 싶을 때.                                                                         | 쿼리와 문서 목록이 주어지면, Rerank는 문서를 쿼리에 대해 가장 의미적으로 관련성이 높은 것부터 가장 낮은 것까지 인덱싱합니다.                                                                                                                                                                                                                         |

:::tip

우리의 RAG from Scratch 비디오에서 [RAG-Fusion](https://youtu.be/77qELPbNgxA?feature=shared)을 참조하십시오. 이는 여러 쿼리에 대한 후처리 접근 방식입니다: 여러 관점에서 사용자 질문을 재작성하고, 각 재작성된 질문에 대해 문서를 검색하고, 여러 검색 결과 목록의 순위를 결합하여 단일 통합 순위를 생성합니다. [상호 순위 융합 (RRF)](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1).

:::

#### 생성

**마지막으로, RAG 시스템에 자기 수정 기능을 구축하는 방법을 고려하십시오.** RAG 시스템은 낮은 품질의 검색(예: 사용자의 질문이 인덱스의 도메인과 일치하지 않는 경우) 및/또는 생성에서의 환각으로 고통받을 수 있습니다. 단순한 검색-생성 파이프라인은 이러한 오류를 감지하거나 자기 수정할 수 있는 능력이 없습니다. ["흐름 엔지니어링"](https://x.com/karpathy/status/1748043513156272416) 개념은 [코드 생성](https://arxiv.org/abs/2401.08500) 맥락에서 도입되었습니다: 단위 테스트를 통해 코드 질문에 대한 답변을 반복적으로 구축하여 오류를 확인하고 자기 수정합니다. 여러 작업이 Self-RAG 및 Corrective-RAG와 같은 이 RAG를 적용했습니다. 두 경우 모두 RAG 답변 생성 흐름에서 문서 관련성, 환각 및/또는 답변 품질에 대한 검사가 수행됩니다.

우리는 그래프가 논리적 흐름을 신뢰성 있게 표현하는 훌륭한 방법이라는 것을 발견했으며, 여러 논문에서 아이디어를 구현했습니다 [LangGraph](https://github.com/langchain-ai/langgraph/tree/main/examples/rag)를 사용하여 아래 그림과 같이 표현했습니다(빨간색 - 라우팅, 파란색 - 폴백, 초록색 - 자기 수정):
- **라우팅:** 적응형 RAG ([논문](https://arxiv.org/abs/2403.14403)). 위에서 논의한 대로 질문을 다양한 검색 접근 방식으로 라우팅합니다.
- **폴백:** 수정 RAG ([논문](https://arxiv.org/pdf/2401.15884.pdf)). 문서가 쿼리와 관련이 없을 경우 웹 검색으로 폴백합니다.
- **자기 수정:** Self-RAG ([논문](https://arxiv.org/abs/2310.11511)). 환각이 있는 답변을 수정하거나 질문을 다루지 않습니다.

![](/img/langgraph_rag.png)

| 이름              | 사용 시기                                               | 설명 |
|-------------------|-----------------------------------------------------------|-------------|
| Self-RAG          | 환각이나 관련 없는 콘텐츠로 답변을 수정해야 할 때. | Self-RAG는 RAG 답변 생성 흐름에서 문서 관련성, 환각 및 답변 품질을 확인하여 답변을 반복적으로 구축하고 오류를 자기 수정합니다. |
| Corrective-RAG    | 낮은 관련성 문서에 대한 폴백 메커니즘이 필요할 때. | Corrective-RAG는 검색된 문서가 쿼리와 관련이 없는 경우 웹 검색으로 폴백하는 기능을 포함하여 더 높은 품질과 더 관련성 높은 검색을 보장합니다. |

:::tip

LangGraph를 사용한 RAG를 보여주는 여러 비디오와 요리책을 참조하십시오:
- [LangGraph 수정 RAG](https://www.youtube.com/watch?v=E2shqsYwxck)
- [적응형, Self-RAG 및 수정 RAG 결합하는 LangGraph](https://www.youtube.com/watch?v=-ROS6gfYIts) 
- [LangGraph를 사용한 RAG 요리책](https://github.com/langchain-ai/langgraph/tree/main/examples/rag)

파트너와 함께하는 LangGraph RAG 레시피를 참조하십시오:
- [Meta](https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/langchain)
- [Mistral](https://github.com/mistralai/cookbook/tree/main/third_party/langchain)

:::
### 텍스트 분할

LangChain은 다양한 유형의 `텍스트 분할기`를 제공합니다.  
이 모든 것은 `langchain-text-splitters` 패키지에 포함되어 있습니다.

테이블 열:

- **이름**: 텍스트 분할기의 이름
- **클래스**: 이 텍스트 분할기를 구현하는 클래스
- **분할 기준**: 이 텍스트 분할기가 텍스트를 분할하는 방법
- **메타데이터 추가**: 이 텍스트 분할기가 각 청크의 출처에 대한 메타데이터를 추가하는지 여부
- **설명**: 분할기에 대한 설명, 사용 권장 시기 포함

| 이름     | 클래스                                                                                                                                                                                                             | 분할 기준                                                   | 메타데이터 추가 | 설명                                                                                                                                                                                                                                                                  |
|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Recursive | [RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter/), [RecursiveJsonSplitter](/docs/how_to/recursive_json_splitter/) | 사용자 정의 문자 목록     |               | 텍스트를 재귀적으로 분할합니다. 이 분할은 관련된 텍스트 조각이 서로 가까이 있도록 시도합니다. 이는 텍스트 분할을 시작하는 `권장 방법`입니다.                                                                                                                    |
| HTML      | [HTMLHeaderTextSplitter](/docs/how_to/HTML_header_metadata_splitter/), [HTMLSectionSplitter](/docs/how_to/HTML_section_aware_splitter/)          | HTML 특정 문자                                                                                                 | ✅             | HTML 특정 문자를 기반으로 텍스트를 분할합니다. 특히, 이 분할은 해당 청크가 어디에서 왔는지에 대한 관련 정보를 추가합니다(HTML 기반).                                                                                                                               |
| Markdown  | [MarkdownHeaderTextSplitter](/docs/how_to/markdown_header_metadata_splitter/),                                                                                                           | Markdown 특정 문자                                                                                    | ✅             | Markdown 특정 문자를 기반으로 텍스트를 분할합니다. 특히, 이 분할은 해당 청크가 어디에서 왔는지에 대한 관련 정보를 추가합니다(Markdown 기반).                                                                                                                       |
| Code      | [many languages](/docs/how_to/code_splitter/)                                                                                                                                 | 코드 (Python, JS) 특정 문자                                                                           |               | 코딩 언어에 특정한 문자를 기반으로 텍스트를 분할합니다. 선택할 수 있는 15개의 서로 다른 언어가 있습니다.                                                                                                                                                           |
| Token    | [many classes](/docs/how_to/split_by_token/)                                                                                                                                  | 토큰                                                                                                          |               | 토큰을 기준으로 텍스트를 분할합니다. 토큰을 측정하는 몇 가지 방법이 존재합니다.                                                                                                                                                                                                   |
| Character  | [CharacterTextSplitter](/docs/how_to/character_text_splitter/)                                                                                                                | 사용자 정의 문자                                                                                        |               | 사용자 정의 문자를 기반으로 텍스트를 분할합니다. 더 간단한 방법 중 하나입니다.                                                                                                                                                                                                   |
| Semantic Chunker (Experimental) | [SemanticChunker](/docs/how_to/semantic-chunker/)                                                                                                                             | 문장                                                                                                       |               | 먼저 문장을 기준으로 분할합니다. 그런 다음 의미적으로 충분히 유사한 문장들을 서로 결합합니다. [Greg Kamradt](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)에서 가져왔습니다. |
| Integration: AI21 Semantic | [AI21SemanticTextSplitter](/docs/integrations/document_transformers/ai21_semantic_text_splitter/)                                                                                                 |                  |    ✅           | 일관된 텍스트 조각을 형성하는 뚜렷한 주제를 식별하고 그에 따라 분할합니다.                                                                                                                                                                                         |

### 평가
<span data-heading-keywords="evaluation,evaluate"></span>

평가는 LLM 기반 애플리케이션의 성능과 효과를 평가하는 과정입니다.  
이는 모델의 응답을 미리 정의된 기준이나 벤치마크에 대해 테스트하여 원하는 품질 기준을 충족하고 의도된 목적을 달성하는지 확인하는 것을 포함합니다.  
이 과정은 신뢰할 수 있는 애플리케이션을 구축하는 데 필수적입니다.

![](/img/langsmith_evaluate.png)

[LangSmith](https://docs.smith.langchain.com/)는 이 과정에서 몇 가지 방법으로 도움을 줍니다:

- 추적 및 주석 기능을 통해 데이터 세트를 생성하고 관리하는 것을 쉽게 만듭니다.
- 메트릭을 정의하고 데이터 세트에 대해 앱을 실행하는 데 도움이 되는 평가 프레임워크를 제공합니다.
- 시간 경과에 따른 결과를 추적하고 평가자를 일정에 따라 자동으로 실행하거나 CI/코드의 일부로 실행할 수 있습니다.

자세한 내용을 보려면 [이 LangSmith 가이드](https://docs.smith.langchain.com/concepts/evaluation)를 확인하세요.

### 추적
<span data-heading-keywords="trace,tracing"></span>

추적은 본질적으로 입력에서 출력으로 이동하기 위해 애플리케이션이 수행하는 일련의 단계입니다.  
추적에는 `실행`이라고 하는 개별 단계가 포함됩니다. 이는 모델, 검색기, 도구 또는 하위 체인에서의 개별 호출일 수 있습니다.  
추적은 체인과 에이전스 내부에서 관찰 가능성을 제공하며, 문제를 진단하는 데 필수적입니다.

더 깊이 알아보려면 [이 LangSmith 개념 가이드](https://docs.smith.langchain.com/concepts/tracing)를 확인하세요.